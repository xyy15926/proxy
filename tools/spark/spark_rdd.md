---
title: Resilient Distributed Dataset
tags:
  - å·¥å…·
  - Spark
categories:
  - å·¥å…·
  - Spark
date: 2019-08-02 23:17:39
updated: 2019-08-02 23:17:39
toc: true
mathjax: true
comments: true
description: Resilient Distributed Dataset
---

##	RDD

RDDï¼šå®¹é”™çš„ã€immutableã€åˆ†å¸ƒå¼ã€ç¡®å®šå¯é‡å¤è®¡ç®—çš„æ•°æ®é›†

-	RDDå¯åˆ†é…åœ¨é›†ç¾¤ä¸­çš„å¤šä¸ªèŠ‚ç‚¹ä¸­ä»¥æ”¯æŒå¹¶è¡Œå¤„ç†
	-	éš¶å±äºåŒä¸€RDDæ•°æ®ï¼Œè¢«åˆ’åˆ†ä¸ºä¸åŒçš„Partitionï¼Œä»¥æ­¤ä¸º
		å•ä½åˆ†å¸ƒåˆ°é›†ç¾¤ç¯å¢ƒä¸­å„ä¸ªèŠ‚ç‚¹ä¸Š

-	RDDæ˜¯**æ— ç»“æ„çš„æ•°æ®è¡¨**ï¼Œå¯ä»¥å­˜æ”¾ä»»ä½•æ•°æ®ç±»å‹

-	RDD immutable
	-	å¯¹RDDè¿›è¡Œè½¬æ¢ï¼Œä¸ä¼šä¿®æ”¹åŸRDDï¼Œåªæ˜¯è¿”å›æ–°RDD
	-	è¿™ä¹Ÿæ˜¯åŸºäºLineageå®¹é”™æœºåˆ¶çš„è¦æ±‚

> - æ˜¯Sparkè½¯ä»¶ç³»ç»Ÿçš„æ ¸å¿ƒæ¦‚å¿µ

###	RDDå®¹é”™æœºåˆ¶

-	RDDé‡‡ç”¨åŸºäºLineageçš„å®¹é”™æœºåˆ¶

	-	æ¯ä¸ªRDDè®°ä½ç¡®å®šæ€§æ“ä½œçš„lineageï¼Œå³ä»å…¶ä»–RDDè½¬æ¢è€Œæ¥
		çš„è·¯å¾„
	-	è‹¥æ‰€æœ‰RDDçš„è½¬æ¢æ“ä½œæ˜¯ç¡®å®šçš„ï¼Œåˆ™æœ€ç»ˆè½¬æ¢æ•°æ®ä¸€è‡´ï¼Œ
		æ— è®ºæœºå™¨å‘ç”Ÿçš„é”™è¯¯
	-	å½“æŸä¸ªRDDæŸåæ—¶ï¼ŒSparkå¯ä»¥ä»ä¸Šæ¸¸RDDé‡æ–°è®¡ç®—ã€åˆ›å»º
		å…¶æ•°æ®

-	å®¹é”™è¯­ä¹‰

	-	è¾“å…¥æºæ–‡ä»¶ï¼šSparkè¿è¡Œåœ¨HDFSã€S3ç­‰å®¹é”™æ–‡ä»¶ç³»ç»Ÿä¸Šï¼Œä»
		ä»»ä½•å®¹é”™æ•°æ®è€Œæ¥çš„RDDéƒ½æ˜¯å®¹é”™çš„
	-	receiverï¼šå¯é æ¥æ”¶è€…å‘ŠçŸ¥å¯é æ•°æ®æºï¼Œä¿è¯æ‰€æœ‰æ•°æ®æ€»æ˜¯
		ä¼šè¢«æ°å¥½å¤„ç†ä¸€æ¬¡
	-	è¾“å‡ºï¼šè¾“å‡ºæ“ä½œå¯èƒ½ä¼šä½¿å¾—æ•°æ®è¢«é‡å¤å†™å‡ºï¼Œä½†æ–‡ä»¶ä¼šè¢«
		ä¹‹åå†™å…¥è¦†ç›–

-	æ•…éšœç±»å‹

	-	workerèŠ‚ç‚¹æ•…éšœï¼šèŠ‚ç‚¹ä¸­å†…å­˜æ•°æ®ä¸¢å¤±ï¼Œå…¶ä¸Šæ¥æ”¶è€…ç¼“å†²
		æ•°æ®ä¸¢å¤±
	-	driverèŠ‚ç‚¹æ•…éšœï¼šspark contextä¸¢å¤±ï¼Œæ‰€æœ‰æ‰§è¡Œç®—å­ä¸¢å¤±

##	RDDæ“ä½œ


```scala
import org.apache.spark.rdd.RDD
```

###	Transformation

è½¬æ¢ï¼šä»å·²æœ‰æ•°æ®é›†åˆ›å»ºæ–°æ•°æ®é›†

-	è¿”å›æ–°RDDï¼ŒåŸRDDä¿æŒä¸å˜

-	è½¬æ¢æ“ä½œLazy
	-	ä»…è®°å½•è½¬æ¢æ“ä½œä½œç”¨çš„åŸºç¡€æ•°æ®é›†
	-	ä»…å½“æŸä¸ª**åŠ¨ä½œ**è¢«Driver Programï¼ˆå®¢æˆ·æ“ä½œï¼‰è°ƒç”¨DAG
		çš„åŠ¨ä½œæ“ä½œæ—¶ï¼ŒåŠ¨ä½œæ“ä½œçš„ä¸€ç³»åˆ—proceedingè½¬æ¢æ“ä½œæ‰ä¼š
		è¢«å¯åŠ¨

|Transformation|RDD|DStream|
|-----|-----|-----|
|`map(func)`|||
|`flatMap(func)`|||
|`filter(func)`|||
|`reduceByKey(func[, numTasks])`|åŒ…å«`(K, V)`é”®å€¼å¯¹ï¼Œè¿”å›æŒ‰é”®èšé›†é”®å€¼å¯¹||
|`groupByKey()`|||
|`aggregateByKey()`|||
|`pipe()`|||
|`coalesce()`|||
|`repartition(numPartitions)`|||
|`union(other)`|æ— ||

> - `XXXByKey`ï¼šRDDä¸­åº”ä¸º`(K, V)`é”®å€¼å¯¹

###	Action

åŠ¨ä½œï¼šåœ¨æ•°æ®é›†ä¸Šè¿›è¡Œè®¡ç®—åè¿”å›å€¼åˆ°é©±åŠ¨ç¨‹åº

-	æ–½åŠ äºä¸€ä¸ªRDDï¼Œé€šè¿‡å¯¹RDDæ•°æ®é›†çš„è®¡ç®—è¿”å›æ–°çš„ç»“æœ
	-	é»˜è®¤RDDä¼šåœ¨æ¯æ¬¡æ‰§è¡ŒåŠ¨ä½œæ—¶é‡æ–°è®¡ç®—ï¼Œä½†å¯ä»¥ä½¿ç”¨
		`cache`ã€`persist`æŒä¹…åŒ–RDDè‡³å†…å­˜ã€ç¡¬ç›˜ä¸­ï¼ŒåŠ é€Ÿä¸‹æ¬¡
		æŸ¥è¯¢é€Ÿåº¦

|Action|RDD|DStream|
|-----|-----|-----|
|`count()`||è¿”å›åŒ…å«å•å…ƒç´ RDDçš„DStream|
|`collect()`|å°†RDDæ•°æ®èšé›†è‡³æœ¬åœ°||
|`countByValue()`|è¿”å›`(T, long)`é”®å€¼å¯¹||
|`countByKey()`|||
|`first()`||è¿”å›åŒ…å«å•å…ƒç´ RDDdçš„DStream|
|`reduce(func)`||è¿”å›åŒ…å«å•å…ƒç´ RDDçš„DStream|
|`take(func)`|||
|`foreach(func)`|||
|`foreachPartition(func)`|||
|`join(other[, numTasks])`|åŒ…å«`(K,V)`ï¼Œä¸å¦ä¸€`(K,W)`è¿æ¥||
|`cogroup(other[, numTasks])`|åŒ…å«`(K,V)`ã€è¾“å…¥`(K,W)`ï¼Œè¿”å›`(K, Seq(V), Seq(W)`||

> - `numTasks`ï¼šé»˜è®¤ä½¿ç”¨Sparké»˜è®¤å¹¶å‘æ•°ç›®

###	DStream RDD

```scala
// RDDçº§`map`ï¼š`func`ä»¥RDDä¸ºå‚æ•°ï¼Œè‡ªå®šä¹‰è½¬æ¢æ“ä½œ
def transform(func)
// RDDçº§`foreach`
def foreachRDD(func)

// RDDçº§`reduce`
def updateStateByKey[S: ClassTag](
	// `K`ã€`Seq[V]`ï¼šå½“å‰RDDä¸­é”®`K`å¯¹åº”å€¼`V`é›†åˆ
	// `Option[S]`ï¼šä¸Šä¸ªRDDç»“æŸåé”®`K`å¯¹åº”çŠ¶æ€
	updateFunc: (Iterator[(K, Seq[V], Option[S])]) => Iterator[(K, S)],
	// åˆ†åŒºç®—æ³•

	partitioner: Partitioner,
	// æ˜¯å¦åœ¨æ¥ä¸‹æ¥Streamingæ‰§è¡Œè¿‡ç¨‹ä¸­äº§ç”Ÿçš„RDDä½¿ç”¨ç›¸åŒåˆ†åŒºç®—æ³•
	remmemberPartition: Boolean,
	// é”®å€¼å¯¹çš„åˆå§‹çŠ¶æ€
	initRDD: RDD[(K,S)]
)
```

-	RDDåˆ†å¸ƒåœ¨å¤šä¸ªworkerèŠ‚ç‚¹ä¸Šï¼Œå¯¹ä¸å¯åºåˆ—åŒ–ä¼ é€’å¯¹è±¡ï¼Œéœ€è¦åœ¨
	æ¯ä¸ªworkerèŠ‚ç‚¹ç‹¬ç«‹åˆ›å»º

	```scala
	dstream.foreachRDD(rdd => {
		rdd.foreachPartition(partitionOfRecords => {
			// ä¸ºæ¯ä¸ªpartitionåˆ›å»ºä¸å¯åºåˆ—åŒ–ç½‘ç»œè¿æ¥
			// ä¸ºæ¯ä¸ªrecordåˆ›å»ºæˆæœ¬è¿‡é«˜
			val connection = createNewConnnection()
			// è¿›ä¸€æ­¥å¯ä»¥ç»´æŠ¤é™æ€è¿æ¥å¯¹è±¡æ± 
			// val connection = ConnectionPool.getConnection()
			partitionOfRecords.foreach(record => connection.send(record))
			connection.close()
		})
	})
	```

###	DStream Window Action

|Window Action|DStream|
|-----|-----|
|`window(windowLength, slideInterval)`|åŸºäºDStreamäº§ç”Ÿçš„çª—å£åŒ–æ‰¹æ•°æ®äº§ç”ŸDStream|
|`countByWindow(windowLenght, slideInterval)`|è¿”å›æ»‘åŠ¨çª—å£æ•°|
|`reduceByWindow(func, windowLength, slideInterval)`||
|`reduceByKeyAndWindow(func, windowLength, slidenInterval[, numTasks])`||
|`reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval[, numTasks])`|é¡»æä¾›`invFunc`æ¶ˆé™¤ç¦»å¼€çª—å£RDDå¯¹reduceç»“æœå½±å“|
|`countByValueAndWindow(windowLength, slideInterval[, numTasks])`||

> - `windowLength`ï¼šçª—å£é•¿åº¦
> - `slideInterval`ï¼šæ»‘åŠ¨é—´éš”

> - ä»¥ä¸Šæ“ä½œé»˜è®¤ä¼šæŒä¹…åŒ–RDDè‡³å†…å­˜ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒç”¨`persist`ç­‰æ–¹æ³•

![spark_streaming_dstream_window_based_operation](imgs/spark_streaming_dstream_window_based_transformation.png)

###	Output

|Output Operation|RDD|DStream|
|-----|-----|-----|
|`print`|æ‰“å°å‰10æ¡å…ƒç´ |æ¯ä¸ªRDDæ‰“å°å‰10æ¡å…ƒç´ |
|`saveAsObjectFile(prefix[, suffix])`|ä¿å­˜ä¸ºåºåˆ—åŒ–æ–‡ä»¶|å‘½åä¸º`<prefix>-TIME_IN_M.<suffix>`|
|`saveAsTextFile(prefix[, suffix])`|ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶||
|`saveAsHadoopFile(prefix[, suffix])`|ä¿å­˜ä¸ºHadoopæ–‡ä»¶||

###	Persistence

|Persistence|RDD|DStream|
|-----|-----|-----|
|`persist()`|||
|`cache()`|||

##	Directed Asycled Graph

Sparkä¸­DAGï¼šå¯ä»¥çœ‹ä½œç”±RDDã€è½¬æ¢æ“ä½œã€åŠ¨ä½œæ“ä½œæ„æˆï¼Œç”¨äºè¡¨è¾¾
å¤æ‚è®¡ç®—

-	å½“éœ€è¦**æ‰§è¡Œ**æŸä¸ªæ“ä½œæ—¶ï¼Œå°†é‡æ–°ä»ä¸Šæ¸¸RDDè¿›è¡Œè®¡ç®—

-	ä¹Ÿå¯ä»¥å¯¹RDDè¿›è¡Œç¼“å­˜ã€æŒä¹…åŒ–ï¼Œä»¥ä¾¿å†æ¬¡å­˜å–ï¼Œè·å¾—æ›´é«˜æŸ¥è¯¢
	é€Ÿåº¦
	-	In-mem Storage as Deserialized Java Objects
	-	In-mem Storage as Serialized Data
	-	On-Disk Storage

###	DAGå·¥ä½œæµç¤ºä¾‹

![spark_dag_procedure](imgs/spark_dag_procedure.png)

-	ä»HDFSè£…è½½æ•°æ®è‡³ä¸¤ä¸ªRDDä¸­
-	å¯¹RDDï¼ˆå’Œä¸­é—´ç”Ÿæˆçš„RDDï¼‰æ–½åŠ ä¸€ç³»åˆ—è½¬æ¢æ“ä½œ
-	æœ€ååŠ¨ä½œæ“ä½œæ–½åŠ äºæœ€åçš„RDDç”Ÿæˆæœ€ç»ˆç»“æœã€å­˜ç›˜

###	å®½ä¾èµ–ã€çª„ä¾èµ–

![spark_dag_wide_narrow_dependencies](imgs/spark_dag_wide_narrow_dependecies.png)

-	çª„ä¾èµ–ï¼šæ¯ä¸ªçˆ¶RDDæœ€å¤šè¢«ä¸€ä¸ªå­RDDåˆ†åŒºä½¿ç”¨
	-	å³å•ä¸ªçˆ¶RDDåˆ†åŒºç»è¿‡è½¬æ¢æ“ä½œç”Ÿæˆå­RDDåˆ†åŒº
	-	çª„ä¾èµ–å¯ä»¥åœ¨ä¸€å°æœºå™¨ä¸Šå¤„ç†ï¼Œæ— éœ€Data Shufflingï¼Œ
		åœ¨ç½‘ç»œä¸Šè¿›è¡Œæ•°æ®ä¼ è¾“

-	å®½ä¾èµ–ï¼šå¤šä¸ªå­RDDåˆ†åŒºï¼Œä¾èµ–åŒä¸€ä¸ªçˆ¶RDDåˆ†åŒº
	-	æ¶‰åŠå®½ä¾èµ–æ“ä½œ
		-	`groupByKey`
		-	`reduceByKey`
		-	`sortByKey`
	-	å®½ä¾èµ–ä¸€èˆ¬æ¶‰åŠData Shuffling

###	DAG Scheduler

*DAG Scheduler*ï¼š*Stage-Oriented*çš„DAGæ‰§è¡Œè°ƒåº¦å™¨

![spark_dag_job_stage](imgs/spark_dag_job_stage.png)

-	ä½¿ç”¨Jobã€Stageæ¦‚å¿µè¿›è¡Œä½œä¸šè°ƒåº¦
	-	ä½œä¸šï¼šä¸€ä¸ªæäº¤åˆ°DAG Schedulerçš„å·¥ä½œé¡¹ç›®ï¼Œè¡¨è¾¾æˆDAGï¼Œ
		ä»¥ä¸€ä¸ªRDDç»“æŸ
	-	é˜¶æ®µï¼šä¸€ç»„å¹¶è¡Œä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡å¯¹åº”RDDä¸€ä¸ªåˆ†åŒºï¼Œæ˜¯ä½œä¸š
		çš„ä¸€éƒ¨åˆ†ã€æ•°æ®å¤„ç†åŸºæœ¬å•å…ƒï¼Œè´Ÿè´£è®¡ç®—éƒ¨åˆ†ç»“æœï¼Œ

-	DAG Scheduleræ£€æŸ¥ä¾èµ–ç±»å‹
	-	æŠŠä¸€ç³»åˆ—çª„ä¾èµ–RDDç»„ç»‡æˆä¸€ä¸ªé˜¶æ®µ
		-	æ‰€ä»¥è¯´é˜¶æ®µä¸­å¹¶è¡Œçš„æ¯ä¸ªä»»åŠ¡å¯¹åº”RDDä¸€ä¸ªåˆ†åŒº
	-	å®½ä¾èµ–éœ€è¦è·¨è¶Šè¿ç»­é˜¶æ®µ
		-	å› ä¸ºå®½ä¾èµ–å­RDDåˆ†åŒºä¾èµ–å¤šä¸ªçˆ¶RDDåˆ†åŒºï¼Œæ¶‰åŠ
			Data Shufflingï¼Œæ•°æ®å¤„ç†ä¸èƒ½åœ¨å•ç‹¬èŠ‚ç‚¹å¹¶è¡Œæ‰§è¡Œ
		-	æˆ–è€…è¯´é˜¶æ®µå°±æ˜¯æ ¹æ®å®½ä¾èµ–è¿›è¡Œåˆ’åˆ†

-	DAG Schedulerå¯¹æ•´ä¸ªDAGè¿›è¡Œåˆ†æ
	-	ä¸ºä½œä¸šäº§ç”Ÿä¸€ç³»åˆ—é˜¶æ®µã€å…¶é—´ä¾èµ–å…³ç³»
	-	ç¡®å®šéœ€è¦æŒä¹…åŒ–çš„RDDã€é˜¶æ®µçš„è¾“å‡º
	-	æ‰¾åˆ°ä½œä¸šè¿è¡Œæœ€å°ä»£ä»·çš„æ‰§è¡Œè°ƒåº¦æ–¹æ¡ˆã€æ ¹æ®Cache Status
		ç¡®å®šçš„è¿è¡Œæ¯ä¸ªtaskçš„ä¼˜é€‰ä½ç½®ï¼ŒæŠŠä¿¡æ¯æäº¤ç»™
		Task Sheduleræ‰§è¡Œ

-	å®¹é”™å¤„ç†
	-	DAG Schedulerè´Ÿè´£å¯¹shuffle output fileä¸¢å¤±æƒ…å†µè¿›è¡Œ
		å¤„ç†ï¼ŒæŠŠå·²ç»æ‰§è¡Œè¿‡çš„é˜¶æ®µé‡æ–°æäº¤ï¼Œä»¥ä¾¿é‡å»ºä¸¢å¤±çš„æ•°æ®
	-	stageå†…å…¶ä»–å¤±è´¥æƒ…å†µç”±Task Scheduleræœ¬èº«è¿›è¡Œå¤„ç†ï¼Œ
		å°†å°è¯•æ‰§è¡Œä»»åŠ¡ä¸€å®šæ¬¡æ•°ã€ç›´åˆ°å–æ¶ˆæ•´ä¸ªé˜¶æ®µ

##	DataFrame

-	*DataFrame*ï¼šç±»ä¼¼å…³ç³»å‹æ•°æ®åº“ä¸­è¡¨ï¼Œæ•°æ®è¢«ç»„ç»‡åˆ°å…·ååˆ—ä¸­
	-	ç›¸è¾ƒäºRDDæ˜¯å¯¹åˆ†å¸ƒå¼ã€ç»“æ„åŒ–æ•°æ®é›†çš„é«˜å±‚æ¬¡æŠ½è±¡ï¼Œæä¾›
		ç‰¹å®šé¢†åŸŸçš„ä¸“ç”¨APIè¿›è¡Œå¤„ç†
	-	é™æ€ç±»å‹å®‰å…¨ï¼šç›¸è¾ƒäºSQLæŸ¥è¯¢è¯­å¥ï¼Œåœ¨ç¼–è¯‘æ—¶å³å¯å‘ç°
		è¯­æ³•é”™è¯¯

-	*Dataset*ï¼šæœ‰æ˜ç¡®ç±»å‹æ•°æ®ã€æˆ–æ— æ˜ç¡®æ•°æ®ç±»å‹é›†åˆï¼Œç›¸åº”API
	ä¹Ÿåˆ†ä¸ºä¸¤ç±»
	-	ç›¸è¾ƒäºDataFrameï¼Œä¹Ÿå¯ç»„ç»‡åŠç»“æ„åŒ–æ•°æ®ï¼ŒåŒæ ·æä¾›æ–¹ä¾¿
		æ˜“ç”¨çš„ç»“æ„åŒ–API
	-	é™æ€ç±»å‹ã€è¿è¡Œæ—¶ç±»å‹å®‰å…¨ï¼šç›¸è¾ƒäºDataFrameï¼Œé›†åˆå…ƒç´ 
		æœ‰æ˜ç¡®ç±»å‹ï¼Œåœ¨ç¼–è¯‘æ—¶å³å¯å‘ç°åˆ†æé”™è¯¯

> - Spark2.0ä¸­äºŒè€…APIç»Ÿä¸€
> - DataFrameå¯è§†ä¸ºæ— æ˜ç¡®æ•°æ®ç±»å‹`Dataset[Row]`åˆ«åï¼Œæ¯è¡Œæ˜¯
	æ— ç±»å‹JVMå¯¹è±¡

###	åˆ›å»ºæ–¹å¼

-	`.toDF`

	```scala
	val sqlContext = new org.apache.spark.sql.SQLContext(sc)
	import sqlContext.implicits._

	// `.toDF` + `Seq`åˆ›å»º
	val df = Seq(
		(1, "F", java.sql.Date.valueOf("2019-08-02")),
		(2, "G", java.sql.Date.valueOf("2019-08-01"))
	).toDF("id", "level", "date")
	// ä¸æŒ‡å®šåˆ—åï¼Œåˆ™é»˜è®¤ä¸º`_1`ã€`_2`ç­‰


	// `.toDF` + `case Class`åˆ›å»º
	case class Person(name: String, age: Int)
	val people = sc.textFile("people.txt")
		.map(_.split(","))
		.map(p => Person(p(0),p(1).trim.toInt))
		.toDF()
	```

-	`.createDataFrame`

	```scala
	import org.apache.spark.sql.types._
	val schema = StrucType(List(
		StructField("id", IntegerType, nullable=False),
		StructField("level", StringType, nullable=False),
		StructField("date", DateType, nullable=False)
	))
	val rdd = sc.parallelize(Seq(
		(1, "F", java.sql.Date.valueOf("2019-08-02")),
		(2, "G", java.sql.Date.valueOf("2019-08-01"))
	))
	val df = sqlContext.createDataFrame(rdd, schema)
	```

-	è¯»å–æ–‡ä»¶åˆ›å»º

	```scala
	val df = sqlContext.read.parquet("hdfs:/peopole.parq")
	val df = spark.read.json("people.json")
	// è¯»å–csvä»…2.0ç‰ˆæœ¬`SparkSession`åå¯
	val df = spark.read.format("com.databricks.spark.csv")
		.option("header", "true")
		.option("mode", "DROPMALFORMED")
		.load("people.csv")
	```

###	ä¸‰ç§æ•°æ®é›†å¯¹æ¯”

-	ç©ºé—´ã€æ—¶é—´æ•ˆç‡ï¼šDataFrame >= Dataset > RDD
	-	DataFrameã€DatasetåŸºäºSparkSQLå¼•æ“æ„å»ºï¼Œä½¿ç”¨Catalyst
		ç”Ÿæˆä¼˜åŒ–åçš„é€»è¾‘ã€ç‰©ç†æŸ¥è¯¢è®¡åˆ’ï¼›æ— ç±»å‹DataFrameç›¸è¾ƒ
		æœ‰ç±»å‹Datasetè¿è¡Œæ›´å¿«
	-	Sparkä½œä¸ºç¼–è¯‘å™¨å¯ä»¥ç†è§£Datasetç±»å‹JVMå¯¹è±¡ï¼Œä¼šä½¿ç”¨
		ç¼–ç å™¨å°†å…¶æ˜ å°„ä¸ºTungstenå†…å­˜å†…éƒ¨è¡¨ç¤º

-	RDDé€‚åˆåœºæ™¯
	-	å¯¹æ•°æ®é›†è¿›è¡Œæœ€åŸºæœ¬çš„è½¬æ¢ã€å¤„ç†ã€æ§åˆ¶
	-	å¸Œæœ›ä»¥å‡½æ•°å¼ç¼–ç¨‹è€Œä¸æ˜¯ä»¥ç‰¹å®šé¢†åŸŸè¡¨è¾¾å¤„ç†æ•°æ®
	-	æ•°æ®ä¸ºéç»“æ„åŒ–ï¼Œå¦‚ï¼šæµåª’ä½“ã€å­—ç¬¦æµ

-	DataFrameã€Datasetä½¿ç”¨åœºæ™¯
	-	éœ€è¦è¯­ä¹‰ä¸°å¯Œã€é«˜çº§æŠ½è±¡ã€é€šç”¨å¹³å°ã€ç‰¹å®šé¢†åŸŸAPI
	-	éœ€è¦å¯¹åŠç»“æ„åŒ–æ•°æ®è¿›è¡Œé«˜çº§å¤„ç†ï¼Œå¦‚ï¼šfilterã€SQLæŸ¥è¯¢
	-	éœ€è¦ç¼–è¯‘æ—¶/è¿è¡Œæ—¶ç±»å‹å®‰å…¨ã€Catalystä¼˜åŒ–ã€å†…å­˜ä¼˜åŒ–


–


