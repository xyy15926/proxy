#	CNN

##	历史

###	生物

CNN最早可以追溯到1968Hubel和Wiesel的论文，

-	猫和猴的视觉皮层含有对视野的小区域单独反应的神经元
-	如果眼睛没有移动，则视觉刺激影响单个神经元的视觉空间区域
	被称为其感受野（Receptive Field）
-	相邻细胞具有相似和重叠的感受野
-	感受野大小和位置在皮层之间系统地变化，形成完整的视觉
	空间图

为CNN的局部感知奠定了一个基础

###	Neocognitron

1980年神经感知机的提出

-	标志第一个初始卷积神经网络的诞生，也是感受野感念在人工
	神经网络首次应用
-	神经感知机将视觉模式分解成许多子模式（特征），然后进入
	分层递阶式的特征平面处理

###	Shift-invariant neural network

时不变神经网络的提出

-	将卷积神经网络的功能进行了一次提高，使其能够在即使物体
	有位移或轻微变形的时候，也能完成识别

###	Neural abstraction pyramid

卷积神经网络的前馈架构在神经抽象金字塔中被横向和反馈连接扩展

-	所产生的复现卷积网络允许灵活地并入情景信息以迭代地解决
	局部模糊
	
-	与以前的模型相反，产生了最高分辨率的图像输出

###	GPU应用

-	2005出现了一篇GPU实现CNN的paper，标志了一种实现CNN更有效的
方式

-	之后在2012年ImageNet大赛中CNN由于其高精确度脱颖而出

-	深度学习正式进入人们的视野

##	CNN基本模块

###	输入层

CNN输入一般是3D以上张量（"channel_last"）

-	axis0一般是batch_size（或者是说样本数量）

-	对时域数据
	-	channels：每个time_step传入的features个数
	-	axis1为time_steps，axis-1为features轴

-	对空间数据（图片）
	-	channels：每个像素的色彩通道
	-	axis2-axis3为像素轴，axis-1为channels

###	卷积层

CNN核心层，用来对输入进行卷积，提取**更高层次**的特征

-	层的参数：可学习的滤波器（filter）（内核（kernels））
	-	具有小的感受野
	-	延伸到输入容积的整个深度
	
-	在前馈期间，每个滤波器对输入进行卷积
	-	计算滤波器和输入之间的点积
	-	并产生该滤波器的二维激活图（输入一般二维向量，但可能
		由深度RGB）

-	卷积作用
	-	对图像作局部变换、但保留局部特征
	-	卷积核选择和它类似信号，过滤掉其他信号、探测局部是否
		有相应模式，如：sobel算子对图像卷积可以得到图像边缘
	-	神经网络中的卷积参数是可变的，但是仍然是同样的用于
		过滤信号，只是目前还不确认何种信号效果比较好

>	CNN中的卷积更像是向量内积，求出了具体值

###	池化层

池化层又称下采样

-	用于减小数据处理量同时保留有用的信息
	-	在每个区域中选择只保留一个值
	-	保留值得选择有多种：最大值、平均值、全局最大等
-	因为相邻区域特征类似，所以池化能够选出表征特征的同时减少
	数据量
	-	直观上可以看作是模糊图像，丢掉一些不重要的细节

####	Maxpooling

最大值采样，使用池中最大值作为代表

####	Avgpooling

平均值采样，使用池中平均值作为代表

###	RELU层

修正线性单元，神经元激活函数

-	对输入值

###	全连通层

常规神经网络，对多次经过卷积层、池化层得到的高级特征进行拼接

###	Dropout层

**训练时**根据预置概率p隐藏**神经元**

-	训练时对每个batch分别进行drop，相当于对每个batch都有独特
	的网络

-	在预测时，类似bagging技术将多个模型组合
	-	只是类似，各个drop后的子网并不独立，在不同子网中相同
		神经元的权重相同
	-	多个模型组合组合可以一定程度上抵消过拟合
	-	因为在训练时子网中部分神经元被drop，剩余部分权重相较
		完全网络有$\frac 1 {1-p}$，所以在完整网络中，各部分
		权重需要$ * (1-p)$

-	讲道理应该是隐藏部分神经元而不是连接，否则会使神经元偏向
	某些输入，还不如隐藏部分神经元，这样可以让神经元随机降低
	样本权重，理论上能减弱过拟合

##	特点

###	局部感知

局部感知就是类似于生物视觉中的感受野

-	每次卷积核所覆盖的像素只是小部分、局部特征，即局部感知
-	CNN是从局部（卷积层）到整体（全联通层）的过程


###	权值共享

CNN中除全连接层之外，卷积层的参数完全取决于滤波器的设置大小

-	传统神经网络对每个输入像素的都需要独立参数
-	CNN中卷积层整张图共用一组滤波器参数
-	即使多卷积核，参数量比传统神经网络相比仍然参数量小

###	多核卷积

一种卷积核代表一种特征，可以使用多各卷积核生成不同特征

##	实现架构

以LeNet-5为例，LeNet-5主要有7层，不包括输入、输出

-	输入层
	-	输出：32 * 32
-	第一卷积层
	-	6卷积核，卷积核大小5 * 5，不填充
	-	输出：6 * 32 * 32
-	第一层池化层
	-	2 * 2池化
	-	输出：6 * 14 * 14
-	第二卷积层
	-	16卷积核，卷积核大小5 * 5，不填充
	-	输出：16 * 10 * 10
-	第二池化层
	-	2 * 2 池化
	-	输出：16 * 5 * 5
-	3 * 全连接层
	-	输入展平为一维
	-	输入：(16 * 5 * 5)
	-	最终输出：分类数







