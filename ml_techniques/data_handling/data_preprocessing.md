---
title: 数据预处理
tags:
  - 机器学习
  - 数据处理
categories:
  - 机器学习
  - 数据处理
date: 2019-07-21 00:46:35
updated: 2019-07-21 00:46:35
toc: true
mathjax: true
comments: true
description: 数据预处理
---

##	数据说明

###	数据模式

-	结构化数据：行数据，可用二维表逻辑表达数据逻辑、存储在
	数据库中
	-	可以看作是关系型数据库中一张表
	-	行：记录、元组，表示一个样本信息
	-	列：字段、属性，有清晰定义

-	非结构化数据：相对于结构化数据而言，不方便用二维逻辑表达
	的数据
	-	包含信息无法用简单数值表示
		-	没有清晰列表定义
		-	每个数据大小不相同
	-	研究方向
		-	社交网络数据
		-	文本数据
		-	图像、音视频
		-	数据流
	-	针对不同类型数据、具体研究方面有不同的具体分析方法，
		不存在普适、可以解决所有具体数据的方法


-	半结构化数据：介于完全结构化数据、完全无结构数据之间的
	数据
	-	一般是自描述的，数据结构和内容混合、没有明显区分
	-	树、图（XML、HTML文档也可以归为半结构化数据）

> - 结构化数据：先有结构、再有数据
> - 半结构化数据：先有数据、再有结构

##	数据问题

###	稀疏特征

-	产生原因
	-	数据缺失
	-	统计数据频繁0值
	-	特征工程技术，如：独热编码

###	缺失值

####	产生原因

-	信息暂时无法获取、成本高
-	信息被遗漏
-	属性不存在

####	缺失值影响

-	建模将丢失大量有用信息
-	模型不确定性更加显著、蕴含规则更难把握
-	包含空值可能使得建模陷入混乱，导致不可靠输出

####	处理方法

-	直接使用含有缺失值特征
-	删除含有缺失值特征
-	插值补全缺失值
	-	均值、中位数、众数
	-	同类均值、中位数、众数
	-	固定值
	-	建模预测：回归、决策树模型预测
		-	若其他特征和缺失特征无关，预测结果无意义
		-	若预测结果相当准确，缺失属性也没有必要纳入数据集
	-	高维映射：*one-hot*编码增加维度表示某特征缺失
		-	保留所有信息、未人为增加额外信息
		-	可能会增加数据维度、增加计算量
		-	需要样本量较大时效果才较好
	-	多重插补：认为待插补值是随机的，通常估计处待插补值，
		再加上**不同噪声**形成多组可选插补值，依据某准则，
		选取最合适的插补值
	-	压缩感知：利用信号本身具有的**稀疏性**，从部分观测
		样本中恢复原信号
		-	感知测量阶段：对原始信号进行处理以获得稀疏样本
			表示
			-	傅里叶变换
			-	小波变换
			-	字典学习
			-	稀疏编码
		-	重构恢复阶段：基于稀疏性从少量观测中恢复信号
	-	矩阵补全
	-	手动补全：根据对所在领域理解，手动对缺失值进行插补
		-	需要对问题领域有很高认识理解
		-	缺失较多时费时、费力
	-	最近邻补全：寻找与样本最接近样本相应特征补全

###	异常值

> - 异常值/离群点：样本中数值明显偏离其余观测值的个别值

异常值分析：检验数据是否有录入错误、含有不合常理的数据

####	异常值检测

-	简单统计：观察数据统计型描述、散点图
-	$3\sigma$原则：取值超过均值3倍标准差，可以视为异常值
	-	依据小概率事件发生可能性“不存在”
	-	数据最好近似正态分布
-	箱线图：利用箱线图四分位距对异常值进行检测
-	基于模型预测：构建概率分布模型，计算对象符合模型的概率，
	将低概率对象视为异常点
	-	分类模型：异常点为不属于任何类的对象
	-	回归模型：异常点为原理预测值对象
	-	特点
		-	基于统计学理论基础，有充分数据和所用的检验类型
			知识时，检验可能非常有效
		-	对多元数据，可用选择少，维度较高时，检测效果不好
-	基于近邻度的离群点检测：对象离群点得分由其距离k-NN的距离
	确定
	-	k取值会影响离群点得分，取k-NN平均距离更稳健
	-	特点
		-	简单，但时间复杂度高$\in O(m^2)$，不适合大数据集
		-	方法对参数k取值敏感
		-	使用全局阈值，无法处理具有不同密度区域的数据集
-	基于密度的离群点检测
	-	定义密度方法
		-	k-NN分类：k个最近邻的平均距离的倒数
		-	DSSCAN聚类中密度：对象指定距离d内对象个数
	-	特点
		-	给出定量度量，即使数据具有不同区域也能很好处理
		-	时间复杂度$\in O^(m^2)$，对低维数据使用特点数据
			结构可以达到$\in O(mlogm)$
		-	参数难以确定，需要确定阈值
-	基于聚类的离群点检测：不属于任何类别簇的对象为离群点
	-	特点
		-	（接近）线性的聚类技术检测离群点高度有效
		-	簇、离群点互为补集，可以同时探测
		-	聚类算法本身对离群点敏感，类结构不一定有效，可以
			考虑：对象聚类、删除离群点再聚类
		-	检测出的离群点依赖类别数量、产生簇的质量
-	*one-class SVM*
-	*isolation forest*

####	异常值处理

-	直接删除
	-	简单易行
	-	观测值很少时，可能导致样本量不足、改变分布
-	视为缺失值处理
	-	可以利用现有变量信息，对异常值进行填补
-	平均值修正：使用前后两个观测值平均值进行修正
-	不处理

> - 很多情况下，要先分析异常值出现的可能原因，判断异常值是
	**真异常值**

###	类别不平衡问题

-	扩充数据集
	-	可以使用欠采样方法放弃部分大类数据

-	尝试其他评价指标：准确度在不平衡数据中不能反映实际情况
	-	混淆矩阵
	-	精确度
	-	召回率
	-	F1得分
	-	ROC曲线
	-	Kappa

-	对数据集重采样

	> - *over-sampling*：过采样，小类数据样本增加样本数量
	> - *under-sampling*：欠采样，大类数据样本减少样本数量

	-	尝试随机采样、非随机采样
	-	对各类别尝试不同采样比例，不必保持1:1违反现实情况
	-	同时使用过采样、欠采样

-	人工生成数据样本

	-	属性值随机采样：从类中样本每个特征随机取值组成新样本
	-	基于经验对属性值随机采样
	-	类似朴素贝叶斯方法：假设各属性之间相互独立进行采样，
		但是无法保证属性之前的线性关系
	-	*synthetic minority over-sampling technique*：过采样
		算法，构造不同于已有样本小类样本
		-	基于距离度量选择小类别下相似样本
		-	选择其中一个样本、随机选择一定数据量邻居样本
		-	对选择样本某属性增加噪声，构造新数据

-	尝试不同分类算法

-	对模型进行惩罚

	-	类似AdaBoosting：对分类器小类样本数据增加权值
	-	类似Bayesian分类：增加小类样本错分代价，如：
		penalized_SVM、penalized-LDA
	-	需要根据具体任务尝试不同惩罚矩阵

-	新角度理解问题

	-	将小类样本视为异常点：问题变为异常点检测、变化趋势
		检测

-	创新：对问题进行分析，将问题划分为多个小问题

	-	大类压缩为小类
	-	使用*one-class*分类器：小类作为异常点
	-	集成模型：训练多个分类器、组合

> - 需要具体问题具体分析

###	图片数据扩充

*Data Agumentation*：根据先验知识，在保留特点信息的前提下，
对原始数据进行适当变换以达到扩充数据集的效果

-	对原始图片做变换处理
	-	一定程度内随机旋转、平移、缩放、裁剪、填充、左右翻转
		，这些变换对应目标在不同角度观察效果
	-	对图像中元素添加噪声扰动：椒盐噪声、高斯白噪声
	-	颜色变换
	-	改变图像亮度、清晰度、对比度、锐度

-	先对图像进行特征提取，在特征空间进行变换，利用通用数据
	扩充、上采样方法
	-	*SMOTE*

-	*finetuning*：微调，接用在大数据集上预训练好的模型，然后
	在小数据集上进行微调
	-	简单的迁移学习
	-	可以快速寻外效果不错针对目标类别的新模型


##	特征缩放

> - 正则化是**针对单个样本**的，将每个样本缩放到单位范数
> - 归一化针对单个属性，需要用到所有样本在该属性上值

###	*Normalizaion*

归一化/标准化：将特征/数据缩放到指定大致相同的数值区间

-	某些算法要求数据、特征数值具有零均值、单位方差
-	消除样本数据、特征之间的量纲/数量级影响
	-	量级较大属性占主导地位
	-	降低迭代收敛速度：梯度下降时，梯度方向会偏离最小值，
		学习率必须非常下，否则容易引起**宽幅震荡**
	-	依赖样本距离的算法对数据量机敏感

> - 决策树模型不需要归一化，归一化不会改变信息增益（比），
	Gini指数变化

####	*Min-Max Scaling*

线性函数归一化：对原始数据进行线性变换，映射到$[0, 1]$范围

$$
X_{norm} = \frac {X - X_{min}} {X_{max} - X_{min}}
$$

> - 训练集、验证集、测试集都使用训练集归一化参数

####	*Z-Score Scaling*

零均值归一化：将原始数据映射到均值为0，标准差为1的分布上

$$
Z = \frac {X - \mu} {\sigma}
$$

###	*Regularization*

正则化：将样本/特征**某个范数**缩放到单位1

$$\begin{align*}
\overrightarrow x_i & = (
	\frac {x_i^{(1)}} {L_p(\overrightarrow x_i)},
	\frac {x_i^{(2)}} {L_p(\overrightarrow x_i)}, \cdots,
	\frac {x_i^{(d)}} {L_p(\overrightarrow x_i)})^T \\
L_p(\overrightarrow x_i) & = (|x_i^{(1)}|^p + |x_i^{(2)}|^p + 
	\cdots + |x_i^{(d)}|^p)^{1/p}
\end{align*}$$

> - $L_p$：样本的$L_p$范数

-	使用内积、二次型、核方法计算洋房之间相似性时，正则化很
	有用

##	特征编码

###	*Ordinal Encoding*

序号编码：使用一位序号编码类别

-	一般用于处理类别间具有大小关系的数据
	-	编码后依然保留了大小关系

###	*One-hot Encoding*

独热编码：采用N位状态位对N个可能取值进行编码

-	一般用于处理类别间不具有大小关系的特征

-	独热编码后**特征表达能力变差**，特征的预测能力被人为拆分
	为多份
	-	通常只有部分维度是对分类、预测有帮助，需要借助特征
		选择降低维度

####	优点

-	能处理非数值属性
-	一定程度上扩充了特征
-	编码后向量时稀疏向量：可以使用向量的稀疏存储节省空间
-	能够处理缺失值：高维映射方法中增加维度表示缺失

####	缺点

-	k-NN算法：高维空间两点间距离难以有效衡量

-	逻辑回归模型：参数数量随维度增加而增大，增加模型复杂度，
	容易出现过拟合

-	决策树模型
	-	产生样本切分不平衡问题，切分增益非常小
		-	每个特征只有少量样本是1，大量样本是0
		-	较小的拆分样本集占总体比例太小，增益乘以所占比例
			之后几乎可以忽略
		-	较大拆分样本集的几乎就是原始样本集，增益几乎为0
	-	影响决策树的学习
		-	决策树依赖数据统计信息，独热编码将数据切分到零散
			小空间上，统计信息不准确、学习效果差
		-	独热编码后特征表达能力边人为拆分，与其他特征竞争
			最优划分点失败，最终特征重要性会比实际值低

###	*Binary Encoding*

二进制编码：先用序号编码给每个类别赋予类型ID，然后将类别ID
对应二进制编码作为结果

-	本质上利用二进制类别ID进行哈希映射，得到0/1特征向量
-	特征维度小于独热编码，更节省存储空间

###	二元化

二元化：将数值型属性转换为布尔型属性

-	通常用于假设属性取值分布为伯努利分布
-	方法：对特征指定阈值，特征值大于等于阈值取1，否则取0
	-	阈值是关键超参数，取值需要结合模型、具体任务选择

###	直方图化

直方图化：将连续的数值属性转换为离散的数值点

-	连续特征一般对预测结果影响不会突变，合理的离散化不会造成
	他特征传递信息能力丢失
	-	另外这也是线性模型的基本假设，若特征存在突变，线性
		模型同样难以拟合
	-	此时应该离散化为多个分类特征，方便引入非线性

-	一般**等频分组**而不是等距分组
	-	避免离散化后特征仍然为长尾分布、大量特征集中在少量桶
		中，对数据区分能力弱
	-	当然也可以根据经验划分区间、指定各组取值

####	优势

-	模型更稳健
	-	对异常值更稳健

-	特征取值可以指定为整形
	-	方便计算、存储
	-	尤其适合树类模型，方便节点划分

####	分桶

分桶：离散化/直方图化常用方法

-	步骤
	-	将样本在连续特征上取值从小到大排列
	-	从小到大依次选择分桶边界，其中分桶数量以及每个桶大小
		都是超参数
	-	根据样本特征取值划分为相应桶内
-	桶数量、边界超参需要人工指定
	-	根据业务领域经验指定
	-	根据模型指定：根据具体任务训练分桶之后的数据集，通过
		超参数搜索确定最优分桶数量、边界
-	分桶经验、准则
	-	桶小大必须足够小：桶内属性取值对样本标记影响在不大
		范围内
	-	桶大小必须足够大：每个桶内有足够样本，否则随机性太大
		，不具有统计意义上说服力
	-	**等频分桶**：桶内样本尽量分布均匀

###	离散化

离散化：将连续的数值属性转换为分类属性

-	一般**等频分组**而不是等距分组
	-	避免离散化后特征仍然为长尾分布、大量特征集中在少量桶
		中，对数据区分能力弱

-	模型使用离散特征、连续特征，是“海量离散特征+简单模型”、
	“少量连续特征+复杂模型”的权衡
	-	**海量离散特征+简单模型**：难点在于特征工程，成功
		经验可以推广，可以多人并行研究
	-	**少量连续特征+复杂模型**：难点在于模型调优，不需要
		复杂的特征工程

####	优势

> - 将连续特征分桶，每个桶对应不同0/1分类特征

-	方便工业应用、实现
	-	离散特征的增加、减少容易，方便模型迭代
	-	离散后特征为稀疏向量
		-	內积速度快
		-	存储方便
		-	容易扩展
-	模型更稳健
	-	对异常数据鲁棒性更好、降低模型过拟合风险
	-	模型不再拟合特征具体值，而是拟合某个概念，能够对抗
		数据扰动，更稳健
	-	需要拟合参数值更少，降低模型复杂度
-	非线性提升模型表达能力
	-	离散后得到多个特征、权重，相当于引入非线性，提升线性
		模型表达能力
	-	方便引入交叉特征，提升模型表达能力

####	适合场景

-	离散化特征更适合LR等线性模型
	-	如下离散化优势：方便引入非线性等
	-	模型中所有特征都会被考虑，考虑细节、个体
		（包括$L_1$范数也是被考虑后剔除）

-	GBDT等非线性模型则不适合
	-	特征离散化后，由于抽样误差的存在，可能存在某些离散
		特征对**样本预测能力非常强**，非线性模型容易给这些
		特征更大权重，造成过拟合
		-	如：刚好抽取的1000个样本中某离散特征取值为1者
			全为正样本
	-	树模型每次使用一个特征划分节点，特征数量较多不利于
		模型训练
		-	若单个离散化特征预测能力不强，由于树深度限制，
			只有少量特征被作为划分依据，模型可能不收敛、表达
			能力更差
		-	若单个离散化特征预测能力强，连续特征也应该也有
			较好效果

##	抽样技术

###	*Hold Out*

旁置法：将样本集随机划分为训练集、测试集

-	适合样本量较大的场合

###	*N-fold Cross Validation*

N折交叉验证：旁置法的扩展，将数据分成N份，每次将其中一份作为
测试样本集，其余N-1份作为训练样本集

-	解决了留一法计算成本高的问题：重复次数少
-	克服了旁置法中测试样本选取随机性的问题：每个样本都能作为
	测试样本
-	典型的“袋外验证”：袋内数据（训练样本）、袋外数据（测试
	样本）分开

###	*Leave-One-Out Cross Validation*

留一法：对n个观测的样本集，每次选择一个观测作为测试样本集，
剩余n-1个观测值作为训练样本集，重复n次计算模型误差

-	可以看作是N折交叉验证的特例

###	Bootstrap

重抽样自举：对样本量为n的样本集S，做k次有放回的重复抽样，
得到k个样本容量仍然未n的随机样本$S_i(i=1,2,...,k)$，称为自举
样本（模拟多组独立样本）

###	样本评价

抽样样本与整体的相似性

$$\begin{align*}
J(S, D) & = \frac {1} {D} \sum_{k=1}^{r} J_{k}(S, D) \\
J_{k}(S, D) & = \sum_{j=1}^{N_k}(P_{Sj} - P_{Dj})
	log \frac {P_{Sj}} {P_{Dj}} \\
Q(s) & = exp(-J)
\end{align*}$$

> - $D$：数据集，包含$r$个属性
> - $S$：抽样样本集
> - $J_k=J(S, D)$：*Kullblack-Laible*信息量，数据集$S$、$D$
	在属性$k$上偏差程度，越小偏差越小
> - $Q(S) \in [0, 1]$：抽样集$S$在数据集$D$中的质量，越大
	样本集质量越高

###	说明

-	若整体$D$分布稀疏，容易得到$S$在某些数据点观测值数为0，
	得到$I(S, D) \rightarrow infty$

	-	可以把该点和附近的点频率进行合并，同时调整总体频率
		分布
	-	过度合并会导致无法有效衡量数据集局部差异性

-	对于连续型变量

	-	可以把变量进行适当分组：粗糙，不利于刻画数据集直接的
		局部差异
	-	计算数据集各个取值点的非参估计，如核估计、最近邻估计
		等，再在公式中用各自的非参估计代替相应频率，计算样本
		质量

-	数据包含多个指标时
	-	可以用多个指标的平均样本质量衡量整体样本质量
	-	也可以根据指标重要程度，设置不同的权重

