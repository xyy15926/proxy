#	*Feature Engineering*

##	综述

特征工程：对原始数据进行一系列工程处理，将其提炼为特征，作为
输入供算法、模型使用

-	本质上：表示、展示数据的过程

-	目的：去除原始数据中的杂质、冗余，设计更高效的特征以刻画
	求解的问题、预测模型之间的关系
	-	把原始数据转换为可以很好描述数据特征
	-	建立在其上的模型性能接近最优

-	方式：**利用数据领域相关知识**、**人为设计输入变量**

-	特征工程重要性：特征越好

	-	模型选择灵活性越高：较好特征在简单模型上也能有较好
		效果，允许选择简单模型
	-	模型构建越简单：较好特征即使在超参不是最优时效果也
		不错，不需要花时间寻找最优参数
	-	模型性能越好
		-	排除噪声特征
		-	避免过拟合
		-	模型训练、预测更快

	> - 数据、特征决定了机器学习的上限，模型、算法只是逼近
		上限

##	*Feature Selection*

###	特征选择

特征选择：从特征集合中选择**最具统计意义**的特征子集

> - *relevant feature*：相关特征，对当前学习任务有用的属性、
	特征
> > -	特征选择最重要的是确保不丢失重要特征
> - *irrelevant feature*：无关特征，对当前学习任务无用的
	属性、特征
> - *redundant feature*：冗余特征，包含的信息可以由其他特征
	中推演出来
> > -	冗余特征通常不起作用，剔除可以减轻模型训练负担
> > -	若冗余特征恰好对应完成学习任务所需要的中间概念，则
		是有益的，可以降低学习任务的难度

-	特征选择会降低模型预测能力，因为被剔除特征中可能包含有效
	信息
	-	保留尽可能多特征，模型性能会提升，模型更复杂、计算
		复杂度同样提升
	-	剔除尽可能多特征，模型性能会下降，模型更简单、降低
		计算复杂度

-	特征选择原因
	-	维数灾难问题：仅需要选择一部分特征构建模型，可以减轻
		维数灾难问题，从此意义上特征选择和降维技术有相似动机
	-	剔除无关特征可以降低学习任务难度，简化模型、降低计算
		复杂度

-	特征选择方法可以分解为
	-	特征子集搜索
	-	特征子集评价：能判断划分之间差异的机制都能作为
		特征子集的准则

####	特征选择过程

![feature_selection_procedure](imgs/feature_selection_procedure.png)

-	*generation procedure*：产生过程，搜索特征子集
-	*evaluation function*：评价函数，评价特征子集优劣
-	*stopping criterion*：停止准则，与评价函数相关的阈值，
	评价函数达到与阈值后可以停止搜索
-	*validation procedure*：验证过程，在验证数据集上验证选择
	特征子集的有效性

####	特征子集搜索

-	遍历：从初始特征集合选择包含所有重要信息的特征子集
	-	适合没有先验（问题相关领域）知识的情况
	-	特征数量稍多会出现组合爆炸

-	迭代：产生候选子集、评价优劣，基于评价结果产生下个候选
	子集
	-	不断迭代，直至**无法找到更好的后续子集**
	-	需要评价得子集数量较少
	-	可能无法找到最优子集

######	迭代搜索

-	给定特征$A=\{A_1, A_2, \cdots, A_d\}$，将每个特征视为
	候选子集（每个子集只有一个元素），对d个候选子集进行评价

-	在上轮选定子集中加入特征，选择包含两个特征的最优候选子集

-	假定在$k+1$轮时，最优特征子集不如上轮最优的特征子集，则
	停止生成候选子集，将上轮选定特征子集作为特征选择结果


> - *Forward Feature Elimination*：前向特征选择，逐渐增加
	相关特征
> - *Backward Feature Elimination*：后向特征选择，从完整特征
	集合开始，每次尝试去掉无关特征，逐渐剔除特征
> - *Bidirectional Feature Elimination*：双向特征选择，结合
	前向、后向搜索
> > -	每轮逐渐增加选定的相关特征，特征在后续迭代中确定不会被
		去除，同时减少无关特征

####	特征子集评价

特征子集评价：能判断划分之间差异的机制都能作为特征子集的选择
准则

######	数值特征

-	方差：计算各特征方差，选择方差大于阈值的特征

-	Pearson相关系数：计算各特征对目标值的Pearson相关系数、
	相关系数P值，选择相关系数大于阈值者

-	距离指标

######	分类特征

-	卡方统计量：计算特征和目标之间卡方统计量，选择和目标变量
	显著相关者

-	Gini指数：计算特征Gini指数，选择Gini指数较大者

-	IG信息增益/互信息：计算特征信息增益，选择信息增益较大者

-	信息增益比

###	*Filter*

过滤式：对数据集进行的特征选择过程与后续学习器无关，即设计
统计量过滤特征，不考虑后续学习器问题

-	通过分析特征子集内部特点衡量特征优劣，描述自变量、目标
	变量的关联

-	特点
	-	时间效率高
	-	对过拟合问题较稳健
	-	倾向于选择**单个**、**冗余**特征，没有考虑特征之间
		相关性

####	单特征过滤

单特征过滤：直接选择合适特征子集评价标准处理各特征，选择满足
要求特征

####	*Relief: Relavant Features*

Relief方法：设置相关统计量度量特征重要性

-	特征子集对应统计量中每个分量对应一个初始特征，特征子集
	重要性由子集中每个特征对应的相关统计量分量之和决定

-	特征选择方法
	-	指定阈值k：选择比k大的相关统计量分量对应特征
	-	指定特征个数m：选择相关统计量分量最大的m个特征

> - 只适合二分类问题，扩展变体*Relief-F*可以处理多分类问题

###	*Wrapper*

包裹式：把最终要使用的**学习器性能作为特征子集评价标准**，
为给定学习器选择最有利其性能、特化的特征子集

-	优点
	-	直接针对特定学习器进行优化
	-	考虑了特征之间的关联性，通常训练效果较过滤式好
-	缺点
	-	特征选择过程中需要多次训练学习器，计算效率较低
	-	观测数据较少时容易过拟合

####	*Las Vegas Wrapper*

*LVW*：在*Las Vegas Method*框架下使用随机策略进行子集搜索，
以最终分类器误差作为特征子集评价标准

-	包含停止条件控制参数T，避免每次子集评价训练特征子集开销
	过大
-	若初始特征数量很多、T设置较大、每轮训练时间较长，算法
	执行很长时间都不会停止
	-	LVM可能无法得到解（拉斯维加斯算法本身性质）

####	递归特征消除法

递归特征消除法：使用基模型进行多轮训练，每轮训练消除若干权值
系数的特征，再基于特征集进行下一轮训练

####	子集回归

-	前向变量选择
-	后向变量选择
-	最优子集选择

###	*Embeded*

嵌入式：将特征选择、学习器训练过程融合，在同一优化过程中同时
完成，即学习器训练过程中自动进行特征选择

-	优点：兼具筛选器、封装器的优点
-	缺点：需要明确**好的选择**

####	正则化约束

$L_1$、$L_2$范数：主要用于线性回归、逻辑回归、SVM等算法

-	Ridge：$L_2$范数
-	Lasso：$L_1$范数
	-	除降低过拟合风险，还容易获得稀疏解
	-	参数$\lambda$越大，稀疏性越大，被选择特征越少
-	SVM、逻辑回归
	-	参数$c$越小，稀疏性越大，被选择特征越少

> - 参见*ml_techniques/#todo*

####	决策树

决策树思想：决策树自上而下选择分裂特征就是特征选择

-	所有树结点划分属性根据先后顺序组成的集合就是选择出来的
	特征子集

> - 参见*ml_models/unlinear_models/decision_tree*

####	神经网络

神经网络：训练时同时处理贡献度问题，不重要特征权重被剔除

##	*Feature Extraction*/*Feature Construction*

特征提取/构建：把原始数据中转换为具有物理、统计学意义特征，
构建新的人工特征

-	主观要求高
	-	对问题实际意义、相关领域有研究：思考问题形式、数据
		结构
	-	对数据敏感：需要观察原始数据
	-	分析能力强


-	目的：自动构建新特征
	-	信号表示：抽取后特征尽可能丢失较少信息
	-	信号分类：抽取后特征尽可能提高分类准确率

-	方法
	-	组合属性：混合属性创建新特征
	-	切分属性：分解、切分原有特征创建新特征，如将时间戳
		分割为日期、上下午

> - 特征选择：表示出每个特征对于模型构建的重要性
> - 特征提取：有时能发现更有意义的特征属性
> - 有时从额外划分特征构建，其相较于特征提取，需要人为的手工
	构建特征，偏经验、规则

###	通用特征提取

> - 部分详见*ml_techniques/data_handling/data_preprocessing*

####	数值型

-	幅度调整：提高SGD收敛速度
	-	归一化
	-	标准化
-	log数据域变化
-	统计值
-	数据离散化：连续值分段
	-	等距切分：各类分布不均
	-	分位数切分：各类分布均匀，但异质性不均
-	平方、开根：增加非线性化

####	分类型

-	*one-hot*编码：赋予各特征等权
-	hash技巧：针对文本类别数据，统计文本词表、倾向
-	多分类转二分类：输入变量类别合并，超类
	-	*twoing*策略：使两个超类差异足够大的合并点（分割点）
	-	*ordering*策略：对有序类型，只有两个连续基类才能合并

####	时间戳

-	视为连续型：持续时间、间隔时间
-	视为离散值：一年中某些时间段

####	统计型

-	分位线
-	比例
-	次序

####	组合特征

-	特征拼接：GBDT生成特征组合路径

###	降维

####	*Principal Component Analysis*

*PCA*：主成分分析，找到数据中主成分，用主成分来表征原始数据
，达到降维目的

-	思想：通过**坐标轴转换，寻找数据分布的最优子空间**
	-	特征向量可以理解为坐标转换中新坐标轴方向
	-	特征值表示对应特征向量方向上方差
		-	特征值越大、方差越大、信息量越大
		-	抛弃较小方差特征
-	PCA缺陷：线性降维方法
	-	*KPCA*：核主成分分析，核映射对PCA进行扩展
	-	流形映射降维方法：等距映射、局部线性嵌入、拉普拉斯
		特征映射

#####	步骤

-	对样本数据进行中心化处理（和统计中处理不同）
-	求样本协方差矩阵
-	对协方差矩阵进行特征值分解，将特征值从大至小排列
-	取前p个最大特征值对应特征向量作为新特征，实现降维

####	*Linear Discriminant Analysis*

*LDA*：线性判别分析，寻找投影方向，使得投影后样本尽可能按照
原始类别分开，即寻找可以最大化类间距离、最小化类内距离的方向

-	相较于PCA，LDA考虑数据的类别信息，不仅仅是降维，还希望
	实现“分类”

-	优点：相较于PCA
	-	LDA更适合处理带有类别信息的数据
	-	模型对噪声的稳健性更好

-	缺点
	-	对数据分布有很强假设：各类服从正太分布、协方差相等，
		实际数据可能不满足
	-	模型简单，表达能力有限，但可以通过核函数扩展LDA处理
		分布比较复杂的数据

> - Fisher判别分析

####	*Independent Component Analysis*

*ICA*：独立成分分析，寻找线性变换$z=Wx$，使得$z$各特征分量
之间独立性最大

-	思想
	-	假设随机信号$x$服从模型
		$$x = As$$
		> - $s$：未知源信号，分量相互独立
		> - $A$：未知混合矩阵
	-	ICA通过观察$x$估计混合矩阵$A$、源信号$s$，认为源信号
		携带更多信息

> - 若原信号非高斯，则分解唯一，否则可能有无穷多分解
> - 因子分析，也称*Blind Source Separation*（盲源分离）

#####	算法

-	大多数ICA算法需要进行数据预处理：先用PCA得到主成分$Y$，
	再把各个主成分各分量标准化得到$Z$满足
	-	$Z$各分量不相关
	-	$Z$各分量方差为1

-	*FastICA*算法：寻找方向$w$使得随机变量$w^T z$某种
	“非高斯性”度量最大化
	-	四阶矩

###	图像特征提取

-	提取边缘、尺度不变特征变换特征

> - 以下是传统的图像特征提取方法，现在应该都是CNN进行特征
	提取、分类
> - 详情参见*machine_learning/cv*

####	*LBP*特征

-	*Sobel Operator*
-	*Laplace Operator*
-	*Canny Edge Detector*

####	基于角点

-	*Moravec*
-	*Harris*
-	*GoodFeaturesToTrack*
-	*FAST*

####	基于尺度空间

-	*Scale-Invariant Feature Transform*
-	*Speeded Up Robust Feature*
-	*Brief*
-	*Oriented Brief*

####	*HOG*特征

方向梯度直方图特征：通过计算、统计图像局部区域梯度方向直方图
实现特征描述

#####	步骤

-	归一化处理：图像转换为灰度图像，再利用伽马校正实现
	-	提高图像特征描述对光照、环境变量稳健性
	-	降低图像局部阴影、局部曝光、纹理失真
	-	尽可能抵制噪声干扰
-	计算图像梯度
-	统计梯度方向
-	特征向量归一化（块内）
	-	克服光照不均匀变化及前景、背景对比差异
-	生成特征向量

###	文本特征提取

> - 具体参见*ml_specification/natural_language_processing/#todo*

####	词袋模型

词袋模型：将文本以词为单位切分token化

-	文章可以表示为稀疏长向量，向量每个维度代表一个单词
	-	针对有序语句，将单词两两相连
	-	维度权重反映单词在原文章中重要程度
		-	通常使用*TF-IDF*统计量表示词权重

-	*TF-IDF*

	$$\begin{align*}
	TF-IDF(t, d) & = TF(t, d) * IDF(t) \\
	IDF(t) & = log \frac {文章总数}
		{包含单词t的文章总数 + 1}
	\end{align*}$$

	> - $TF(t, d)$：单词$t$在文档$d$中出现的频率
	> - $IDF(t)$：逆文档频率，衡量单词对表达语义的重要性
	> > -	若单词在多篇文章中出现过，则可能是通用词汇，对区分
			文章贡献较小，$IDF(t)$较小、权重较小

####	*N-gram*模型

N-gram模型：将连续出现的$n, n \leq N$个词组成的词组N-gram
作为单独特征放到向量中

-	相较于词袋模型，考虑单词组合意义
-	*word stemming*：将不同词性单词统一为同一词干形式
	-	同一个词可能有多种词性变化，却拥有相同含义

####	*Word-Embedding*模型

词嵌入模型：将每个词都映射为低维空间上的稠密向量

-	*Word2Vec*：常用词嵌入模型，底层神经网络
	-	*Continuous Bag of Words*：根据上下文词语预测当前词
		生成概率
	-	*Skip-gram*：根据当前词预测上下文中各个词的生成概率

-	实际上直接使用矩阵作为源文本特征作为输入进行训练，难以
	得到好结果，往往需要提取、构造更高层特征

