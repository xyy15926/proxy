##	抽样技术

###	*Hold Out*

旁置法：将样本集随机划分为训练集、测试集

-	适合样本量较大的场合

###	*N-fold Cross Validation*

N折交叉验证：旁置法的扩展，将数据分成N份，每次将其中一份作为
测试样本集，其余N-1份作为训练样本集

-	解决了留一法计算成本高的问题：重复次数少
-	克服了旁置法中测试样本选取随机性的问题：每个样本都能作为
	测试样本
-	典型的“袋外验证”：袋内数据（训练样本）、袋外数据（测试
	样本）分开

###	*Leave-One-Out Cross Validation*

留一法：对n个观测的样本集，每次选择一个观测作为测试样本集，
剩余n-1个观测值作为训练样本集，重复n次计算模型误差

-	可以看作是N折交叉验证的特例

###	Bootstrap

重抽样自举：对样本量为n的样本集S，做k次有放回的重复抽样，
得到k个样本容量仍然未n的随机样本$S_i(i=1,2,...,k)$，称为自举
样本（模拟多组独立样本）

###	Boosting

####	AdaBoost

对加权样本做有放回的随机抽样，获得训练样本集

1.	对S做有放回的随机抽样，获得容量为n的随机样本$S_1$，建立
	模型$T_1$，计算样本i模型误差e(i)
	-	e(i)>0.5：模型无价值，差于随机分类，终止建模
	-	e(i)=0：模型完美，终止建模

2.	调整S中各个观测值权重，对$T_1$预测正确的观测给与较低权重
	，错误预测权重不变
	-	正确分类的样本权：$w_j(i+1) = w_j(i) * \beta(i)
		\beta(i) = e(i) / (1 - e(i))$
	-	错误分类样本权：$w_j(i+1) = w_j(i)$
	-	调整$w_j(i+1)$使权重和为1

3.	根据权重对S做放回随机抽样得到$S_2$，建立模型$T_2$，权重
	越大的观测进入$S_2$的可能性越高

4.	重复k次得到k个自举样本$S_1, S_2, ..., S_k$，以及K个预测
	模型$T_1, T_2, ..., T_k$

#####	缺点

-	对离群点、奇异点敏感

#####	优点

-	对过拟合不敏感

####	Grediant Boost



