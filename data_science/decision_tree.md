#	决策树

##	概述

-	分析结论的展示方式类似一棵倒置的树

-	体现了输入变量、输出变量取值的逻辑关系
	-	逻辑比较形式表述的是一种推理规则
	-	每个叶结点都对应一条推理规则

-	对新数据$X_0$的预测
	-	从决策树的树根到树叶搜索
	-	利用规则集预测：众数类、均值

-	优势
	-	能够实现非线性分割
	-	能有效处理分类型输入变量

-	问题
	-	充分生长的决策有高方差，预测不稳定
	-	剪枝可以提高预测稳健性，但是预测精度可能会下降

##	决策树生长

决策树个分支依次形成，体现对样本数据不断分组，每个分支在一定
规则下完成对p维特征空间的划分

-	目的：得到$\sum_{j=1}^J \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2$
	最小下J个区域$R_j$的划分

Classification And Regression Tree：分类树 + 回归树 + 二叉树

###	自顶向下贪心算法

原则：使区域（节点）内观测输出变量的异质性下降最大，从而确定

-	最佳分组变量$x_j$
-	分组变量取值中的最佳分割点s

考虑生成树为二叉树，则：

-	数值变量：离散化处理，分成两组
	-	适用于有序输入变量，只有连续的类别才可以合并
-	多分类：输入变量类别合并，超类
	-	Twoing策略：找到使两个超类差异足够大的合并点（分割点）

###	树剪枝

最小化RSS、最大化置信目标下，会导致庞大的树（复杂模型）

-	推广能力差
-	比较难理解、解释

通过剪枝得到恰当的树，具备一定的预测精度、复杂程度恰当、代价
（误差）和复杂度之间的权衡是必要的

-	pre-pruning：限制决策树的充分生长
	-	事先指定决策树生长最大深度
	-	事先指定决策树叶结点最大值
	-	事先指定树节点样本量最小值
	-	异质性下降必须大于阈值

-	post-pruning：决策树生长完毕后，根据一定规则，剪去不具备
	普遍性的子树

####	Minimal Cost Complexity Pruning

后剪枝策略

#####	代价复杂度

定义代价复杂度为

$$
R_{\alpha}(T)=R(T) + \alpha|\tilde T|
$$

其中：
-	$\alpha$：complexity parameter，每增加一个叶结点带来的
	复杂度
-	$R(T)$：T在测试样本集上的预测误差（分类树中为误判率）
-	$|\tilde T|$：叶结点个数（所以剪枝应该一层层往上剪）

基于训练样本集，对于中间节点$t$及其下的子树$T_t$

-	定义$t$的代价复杂度：$R_{\alpha}(t) = R(t) + \alpha$
-	则$T_t$代价复杂度为：
	$R_{\alpha}(T_t) = R(T_t) + \alpha|\tilde T_t|$

则：

$$
R_{\alpha}(T_t) < R_{\alpha}(t) \rightarrow
	保留子树：\alpha < \frac {R(t) - R(T_t)} {|\tilde T_t| - 1} \\

R_{\alpha}(T_t) \geqslant R_{\alpha}(t) \rightarrow
	剪掉子树：\alpha \geqslant \frac {R(t) - R(T_t)}
		{|\tilde T_t| - 1}
$$

#####	剪枝算法

-	$\alpha$逐渐增大，逐一剪枝产生子树序列$T_1,T_2,...T_k$，
	$T_1$为最大树，$T_k$只包含根节点，分别计算代价复杂度

	-	$\alpha$可以设置为大于0的值，以提高剪枝效率
	-	初始值过大可能会漏掉最优解，太大速度慢

-	选择k个子树中代价复杂度最低的子树$min R_{\alpha}(T_k)$，
	此时对应$\alpha$即为最优的CP值

	-	应该使用N折交叉验证确定CP值，取N个测试误差和最小情况
		下的CP值

#####	N折交叉验证剪枝

1.	对**训练**样本集建树、剪枝，得到
	$\beta_1, \beta_2, ..., \beta_k$

2.	设复杂度CP参数为$\alpha$，其**典型代表值**为：
	$\alpha_1 = \sqrt {\beta_1 \beta_2},...,\alpha_{k-1} = \sqrt{\beta_{k-1} \beta_k}, \alpha_k = \infty$

3.	N折交叉验证：对第i次迭代
	-	得到当前训练样本集上各$\beta$对应的若干子树
	-	在测试样本集上计算预测误差

4.	对每个$\beta_j(j=1,2,...,k)$计算误差和，找到误差最小的
	$\beta_{opt}$为最优

5.	$\beta_{opt}$在样本全体上对应的子树为最终决策树

##	组合预测模型

-	提高模型预测精度、稳健性
-	源于样本均值抽样分布思路；$var(\bar{X}) = \sigma^2 / n$
-	基于独立样本，建立一组具有相同形式的模型，单个模型称为
	Base Learner，这里是未剪枝的决策树
-	预测由这组模型共同参与：
	$\hat{f}_{avg}(x) = \frac 1 B \sum_{b=1}^B \hat{f}^b(x)$
-	关键在于如何获得多个独立样本、组合多个模型

###	Bagging技术

Bagging：Bootstrap Aggregating，核心为Bootstrap（重抽样自举）

-	建模阶段：通过Bootstrap技术获得k个自举样本
	$S_1, S_2,..., S_k$，建立充分生长（不剪枝）分类回归树
	$T_1, T_2,..., T_k$

-	预测阶段
	-	分类问题：k个预测模型“投票”
	-	回归问题：K个预测模型平均值

-	模型评估阶段
	-	总有部分观测参与建模，预测误差估计偏乐观
	-	采用基于袋外（Out of Bag，OOB）观测的预测误差
		（即对每个样本，使用其**没有参与**建树的生成树进行
		预测，计算错判比率）
	-	OOB观测比例大约为36.8%，n较大时n次均未被抽中的概率
		为$(1 - \frac 1 n)^n \approx \frac 1 e = 0.367$，
	-	与10折交叉验证相比，每个模型训练样本小于10时的90%，
		单个模型为若模型，组合模型为强模型
	-	Bagging中的变量性度量：输入变量在k棵树的中异质性下降
		总和

###	Boosting技术


-	建模阶段：通过AdaBoost技术获得k个自举样本
	$S_1, S_2,..., S_k$，建立充分生长（不剪枝）分类回归树
	$T_1, T_2,..., T_k$

-	预测阶段
	-	不同的模型有不同权重，与模型预测误差成反比
		-	Breiman策略：
			$W_i(e)=\frac 1 2 log(\frac {1-e(i)} {e(i)})$
		-	Freund策略：$W_i(e)=log(\frac {1-e(i)} {e(i)})$
	-	分类问题：k个预测模型加权“投票”
	-	回归问题：K个预测模型加权平均值

###	Random Forest

随机建立多个有较高预测精度、弱相关（甚至不相关）的决策树
（基础学习器），多棵决策树共同对新观测做预测

-	样本随机：Bootstrap自举样本

-	输入变量随机：对第i课决策树通过随机方式选取K个输入
	变量构成候选变量子集$\Theta_I$，只有这些变量才有可能
	能参与分组，称为最佳分组变量
	-	Forest-RI（random input）：随机选择
		$K=log_2P+1, k=\sqrt P$个变量
	-	Forest-RC（random combination）
		-	随机选择L个输入变量x
		-	生成L个服从均匀分布的随机数$\alpha$
		-	做线性组合
			$v_j = \sum_{i=1}^L \alpha_i x_i, \alpha_i \in [-1, 1]$
		-	得到k个由新变量v组成的输入变量子集$\Theta_i$

因为选择最优分组变量、最优合并点都是贪心策略，所以使用所有
变量的子集能够一定程度上避免贪心算法带来的局部最优局限

#todo

-	启发式操作
-	优化操作

###	GBDT

Gradient Boosted Decision Tree：采用加法模型，即基函数的
线性组合，不断减小训练过程产生的残差达到数据分类、回归算法

-	多轮迭代，每轮产生一个弱分类器，每个分类器在上一轮分类器
	的**残差**基础上进行训练
	-	弱分类器要求足够简单，低方差、高偏差，因而每个树不会
		很深
	-	因为训练过程就是通过不断降低偏差不断提高最终分类器
		精度
	-	弱分类器一般会选择为Cart Tree（分类回归树）

-	GBDT通过**经验风险最小化**（即添加惩罚项）确定下一个
	弱分类器参数，即损失函数l选择

-	为了让损失函数不断、尽可能快的减小，选择让损失函数沿着
	负梯度方差下降


-	总分类器将每轮训练的弱分类器加权求和得到（加法模型）
	（完成后共K棵树）
	$$
	\hat y_K(x) = F_K(x) = \sum_{i=1}^K f_i(x; l_i) \\
	$$

-	模型损失函数（预测误差）

	$$\begin{align*}
	L = L^t & = \sum_i^n l(y_i, \hat y_i^t) \\
		& = \sum_i^K l(y_i, \hat y_i^{t-1} + f_t(x_i)) \\
	& 在t时刻，正在生成第t棵树 \\
	& n为样本量 \\
	& Square Loss：l(y_i,\hat y_i) = (y_i - \hat y_i)^2即残差 \\
	& Logistic Loss：l(y_i, \hat y_i) =
		y_i ln(1+e^{-\hat y_i}) + (1-y_i)ln(1+e^{\hat y_i}) \\
	& 0-1 Loss \\
	\end{align*}
	$$

####	极小化目标函数

同时考虑模型复杂度（惩罚项）

$$
\begin{align*}
obj^t & = L^t + \sum_{i=1}^t \Omega(f_k)						\\
	& = \sum_i^K l(y_i, \hat y_i^{t-1} + f_t(x_i)) +
		\Omega(f_t) + constant									\\
	& = \sum_{i=1}^n [l(y_i, \hat y^{t-1}) + g_i f_t(x_i) +
		\frac 1 2 h_i f_t^2(x_i)] +
		\Omega(f_t) + constant									\\
	& = \sum_{i=1}^n [l(y_i, \hat y^{t-1}) + g_i w_t(x_i) +
		\frac 1 2 h_i w_t^2(x_i)] +
		\gamma T + \frac 1 2 \lambda \sum_{j=1}^T w_j^2			\\
	& = \sum_{j=1}^T [(\sum_{i \in I_j} g_i) w_j +
		\frac 1 2 (\sum_{i \in I_j} h_i + \lambda) w_j^2] +
		\lambda T												\\
	& = \sum_{j=1}^T[G_iw_j + \frac 1 2 (H_j + \lambda)w_j^2] +
		\lambda T												\\
\end{align*}
$$

说明：

-	对第t步，之前生成树已经确定，不影响目标函数取极值，可以
	认为是常数

-	对损失函数在$\hat y_i^{t-1}$泰勒展开
	-	$g_i = \partial_{\hat y^{t-1}} l(y_i, \hat y^{t-1})$
	-	$h_i = \partial^2_{\hat y^{t-1}} l(y_i, \hat y^{t-1})$

-	$w$是预测得分
	-	$w_t(x_i)$是t时刻树对样本$x_i$的预测得分
	-	$w_j$则是**当前t时刻树**第j叶子节点预测得分，对归于
		同一个叶子节点所有样本点，其预测得分相同，即此叶子
		节点的预测得分

-	惩罚项$\Omega(f_t)$被定义为如此
	-	T是**当前t时刻树**叶子节点数
	-	$\gamma, \lambda$权重参数
	-	这个惩罚项定义的非常巧妙，添加了$w_j^2$作为惩罚，
		要求树能解释的残差越大越好，同时能够和前面泰勒展开
		的二次项合并，需要注意的是，惩罚项是针对树的，即定义
		在的是叶子节点，而误差是定义在样本上，所以合并时是
		将样本误差改写成针对叶子节点

-	合并部分
	-	$I_j$为叶结点集合
	-	$G_j = \sum_{i \in I_j} g_i, H_j=\sum_{i \in I_j} h_i$

####	最优预测得分

极小化目标函数对$w_i$求导，令其为0即可得最优得分$w_j^*$，
对应的目标函数值称为树的结构分数

$$
$w_j^* = -\frac {G_j} {H_j + \lambda} \\
$obj =  -\frac 1 2 \sum_{j=1}^T \frac {G_j^2} {H_j + \lambda}
	+ \lambda T$
$$

-	结构分数最小时的树为所求的树
-	结构分数可以用于评价树结构的合理性，越小越好

####	损失函数确定

### XGBoost

一般模型构建准则通常由两部分构成

$$
Obj(\Theta) = L(\Theat) + \Omega(\Theta)
$$

-	其中：$L(\Theta)$表示模型拟合能力
-	$\Omega(\Theta)$描述模型复杂度

在XGBoost中

$$
Obj(\Theta)
$$

##	树算法

###	ID3

###	C4.5

###	CART

classification adn regression tree

-	包括分类树、回归树
-	是AID、CHAID算法的发展提高
-	克服了经典统计

####	优点

-	对自变量、因变量不做任何形式的分布假定
-	自变量可以是分类变量、连续变量的分布假定
-	具有对付一个观测中具有缺失值的功能，但用变量的线性组合
	分割节点时除外（斜/协决策树，变量组合）

	# todo

###	CHAID

-	对于定性变量，各个水平将观测事先分组，形成一些小块的子集
-	通过统计检验：卡方检验、F检验确定分割变量、合并子集，
	得到新的子节点
-	重复前两步直到每组只有一个类别

