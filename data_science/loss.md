#	Loss Function

##	Singluar Loss

单个样本点损失：度量模型“一次”预测的好坏

-	最基本损失，之和的损失都建立在此基础之上
-	模型在单点上的性质

###	0-1 Loss

0-1损失函数

$$
L(Y, f(x)) = \left \{ \begin{align*}
	1, & Y \neq f(X) \\
	0, & Y = f(X)
\end{align*} \right.
$$

###	Quadratic Loss

平方损失函数

$$
L(Y, f(X)) = (Y - f(X))^2
$$

###	Absolute Loss

绝对损失函数

$$
L(Y, f(X)) = |Y-f(X)|
$$

###	Logarithmic Loss

对数损失函数（对数似然损失函数）

$$
L(Y, P(Y|X)) = -logP(Y|X)
$$

##	Total Loss

模型（目标函数）在样本整体的损失：度量模型整体预测效果

-	代表模型在整体上的性质

-	可以用于**设计学习策略、评价模型**
	-	风险函数
	-	评价函数

-	有时在算法中也会使用整体损失

###	*Risk Function*

（期望）风险函数：*expected loss*，是损失函数$L(Y, f(X))$
（随机变量）期望

$$
R_exp(f) = E_p[L(Y, f(X))] = \int_{x*y} L(y,f(x))P(x,y) dxdy
$$

> - $P(X, Y)$：随机变量$(X, Y)$遵循的联合分布，未知


-	风险函数值度量模型预测错误程度

-	评价标准（**监督学习目标**）就应该是选择期望风险最小

-	联合分布未知，所以才需要学习，否则可以直接计算条件分布
	概率，而计算期望损失需要知道联合分布，因此监督学习是一个
	病态问题

###	*Empirical Risk*

经验风险：*empirical loss*模型关于给定训练数据集的平均损失

####	判别方法

$$
R_{emp}(f) = \frac 1 N \sum_{i=1}^N L(y_i, f(x_i))
$$

-	根据大数定律，样本量容量N趋于无穷时，$R_{emp}(f)$趋于
	$R_{exp}(f)$

-	但是现实中训练样本数目有限、很小，利用经验风险估计期望
	常常并不理想，需要对经验风险进行矫正

####	生成方法

对*maximum probability estimation*

-	损失函数：对数损失函数

###	*Structual Risk*

*结构风险*：在经验风险上加上表示**模型复杂度**的
*regularizer*（*penalty term*）

####	判别方法

$$
R_{srm} = \frac 1 N \sum_{i=1}^N L(y_i, f(x_i)) +
	\lambda J(f)
$$

> - $J(f)$：模型复杂度，定义在假设空间$F$上的泛函
> - $\lambda$：权衡经验风险、模型复杂度的系数

-	结构风险最小化通过添加*regularization*（正则化）实现

-	模型复杂度$J(f)$表示对复杂模型的惩罚：模型$f$越复杂，
	复杂项$J(f)$越大

####	生成方法

对贝叶斯估计*maximum posterior probability estimation*

-	损失函数：对数损失函数
-	模型复杂度：模型先验概率对数后取负
-	先验概率对应模型复杂度，先验概率越小，复杂度越大

##	Batch Loss

模型（目标函数）在某个batch上的损失

-	是模型在batch上的特征，对整体的代表性取决于batch大小

	-	batch越大对整体代表性越好
	-	batch大小为1时，就是某个样本点个体损失
	-	batch大小为整个训练集时，就是经验（结构）风险

-	这个loss是学习算法中最常用的loss

	-	虽然策略往往是风险最小化，但在实际操作中往往是使用
		batch loss替代风险（参见*algorithms*）
	-	所以和风险一样可能会带有正则化项

	-	损失极值：SVM（几何间隔最小）

##	*regularization*

正则化：（向目标函数）添加额外信息以求解病态问题、避免过拟合

-	常应用在机器学习、逆问题求解

	-	对模型（目标函数）复杂度惩罚
	-	提高学习模型的泛化能力、避免过拟合
	-	学习简单模型：稀疏模型、引入组结构

-	有多种用途

	-	最小二乘也可以看作是简单的正则化
	-	岭回归中的$\mathcal{l_2}$范数

###	模型复杂度

模型复杂度：经常作为正则化项添加作为额外信息添加的，衡量模型
复杂度方式有很多种

-	函数光滑限制

	-	多项式最高次数

-	向量空间范数

	-	$\mathcal{L_0}$ norm：参数个数
	-	$\mathcal{L_1}$ norm：参数绝对值和
	-	$\mathcal{L_2}$ norm：参数平方和

####	$\mathcal{L_0}$ norm

-	稀疏化约束

-	解$\mathcal{L_0}$范数正则化是NP-hard问题

####	$\mathcal{L_1}$ norm

-	$\mathcal{L_1}$范数可以通过凸松弛得到$\mathcal{L_0}$的
	近似解

-	有时候出现解不唯一的情况

-	$\mathcal{L_1}$范数凸但不严格可导，可以使用依赖次梯度的
	方法求解极小化问题

-	应用
	-	*LASSO*

-	求解
	-	*Proximal Method*
	-	*LARS*

#####	稀疏解产生

######	step1

-	稀疏解：待估参数系数在某些分量上为0

-	$\mathcal{L_1}$范数在参数满足**一定条件**情况下，能对
	**平方损失**产生稀疏效果，是因为在$[-1,1]$内$y=|x|$导数
	大于$y=x^2$（除0）

-	所以特征在一定范围内变动时，为了取到极小值，参数必须始终
	为0

-	而这个一定条件就是特征满足在0附近、$y=x$导数更大的范围内

-	而大部分更高阶的多项式损失，其在0附近导数更小，所以
	$\mathcal{L_1}$能产生稀疏解是很广泛的

-	而且这个特征其实很特殊，在0附近导数不小，导数中必须有
	常数项，所以这样稀疏特征只有**带有**$\mathcal{L_1}$的
	正则化项才能具有，并且其起决定性作用

-	而其他高阶范数无法保证，因为虽然看起来标准幂函数$[0,1]$
	中的导数低阶都大于高阶，但是原始损失函数中往往会有平移，
	#todo?可以吗

######	step2

-	$f(x) = ln(|x|+1), f(x) = e^{|x|}-1$

######	step3

-	在0点处不光滑

	-	对>0、<0分别压缩

######	step4

-	在不光滑点附近会被压缩向不光滑点处----整数规划
	$f(x) = |sin \pi x|$、$f(x) = -x^2+1$

######	step5

-	不光滑（尖锐）的部分有聚集、疏远解的的倾向
-	聚集、疏远解的倾向和梯度有关，这个就和SGD一样了，解会
	向极小点靠近
-	关键是不光滑部分会极大的抑制解变动，和一般的最低点性质
	差得有点多

####	$\mathcal{L_2}$ norm

-	$\mathcal{L_2}$范数凸且严格可导，极小化问题有解析解

-	求解

####	$\mathcal{L_1 + L_2}$

-	有组效应，相关变量权重倾向于相同

-	应用
	-	*Elastic Net*

###	Earlty Stopping

*Early Stopping*也可以被视为是*regularizing on time*

-	迭代式训练随着迭代次数增加，往往会有学习复杂模型的倾向
-	对时间施加正则化，可以减小模型复杂度、提高泛化能力

-	





