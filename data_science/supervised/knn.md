#	K近邻

##	近邻分析

-	基本思想：为预测新观测$X_0$输出变量$y_0$的取值
	1.	在已有数据中找到与$X_0$相似的若干个观测
		$(X_1, X_2, ..., X_k)$，称为$X_0$的近邻
	2.	对近邻$(X_1, X_2, ..., X_k)$的输出变量
		$(y_1, y_2, ..., y_k)$，计算诸如算术平均值
		（加权平均值、中位数、众数），作为新观测$X_0$输出
		变量取值$y_0$的预测值
-	特点
	-	不需要假设$y=f(X)$函数体形式
	-	只需假设$\hat{y}_0$是$(y_1, y_2, ..., y_k)$的函数：
		$\hat{y}_)=F(y_1, y_2, ..., y_k)$

##	KNN

###	出发点

-	将样本包含的n个观测数据看成p维特征空间的点
-	找到$X_0$的K个近邻
-	根据$X_0$的k个近邻依函数计算预测值
	-	回归（数值型）
		$$
		\hat{y}_0 = \frac {1} {K} \sum_{X_i \in N_K(X_0)} y_i
		$$
	-	分类预测：众数

###	距离测度

$$
\begin{align*}
MINKOSKI(x, y) & = \sqrt {\sum_{i=1}^p |x_i - y_i|^k} \\
EUCLID(x, y) & = \sqrt {\sum_{i=1}^p(x_i - y_i)^2} \\
BLOCK(x, y) & = \sum_{i=1}^p|x_i - y_i| \\
CHEBYCHEV(x, y) & = Max(|x_i - y_i|), i=1,2,...,p \\
ALL(x, y) & = \sqrt[1/q] \sum_{i=1}^p {|x_i - y_i|^q} \\
\end{align*}
$$

###	近邻数目

####	k=1

只使用一个近邻做预测，找到距离$X_0$最近的近邻$X_i$，用其取值
作为预测值

-	简单、效果较理想：尤其适合特征空间维度较低，且类别边界
	不规则的情况
-	预测错误的概率不高于普通贝叶斯方法的两倍
	$$
	P_e & = (1-p(y=1|X=X_0))P(y=1|X=X_0) +
			(1-p(y=0|X=X_0))P(y=0|X=X_0)
		& = 2P(y=1|X=X_0)(1-P(y=1|X_0))
		& \leqslant 2(1-P(y=1|X=X_0))
	$$
-	只根据单个近邻预测，预测结果受近邻差异影响极大，预测波动
	（方差）大，稳健性低

####	K-近邻

-	随着K的增加，决策边界平直，预测偏差增大，方差减小
	-	找到恰当的K值平衡预测偏差、方差
	-	即使测试误差最小的K值：旁置法、留一法、N折交叉验证

-	K-近邻是一种“局部”方法，仅适合特征空间维度较低的情况，
	-	对于高维空间，确定确定了K时，需要到更远的区域寻找
		近邻，局部性逐渐上市，预测偏差加大
	-	n个观测均匀分布在超立方体中，确定K后即确定$X_0$近邻
		个数占总观测的比率r，则考虑$X_0$为原点时，这些近邻
		分布在小立方体个边期望长度为
		$$
			Ed_p(r) = r^{1/p} \\
			Ed_3(0.1)=0.1^{1/3}=0.46 \\
			Ed_{10}(0.1)=0.1^{1/10}=0.79 \\
			Ed_{10}(0.01)=0.1^{1/10}=0.63 \\
		$$
		-	可以看出，减小r并没有帮助，且会使预测方差加大
		-	特征选择有必要

	-	只有增大样本量才能得到既定的K个近邻

####	特征选择

-	变量本身考察（low variance filter，missing values ratio）
	-	剔除标准差小于阈值的数值型变量
	-	剔除众数比率大于阈值的分类型变量
	-	剔除缺失值大于阈值的变量

-	变量与输出变量相关性角度考察（high correlation filter）

-	对预测误差影响角度考察
	-	Wrapper方法：逐个选择使错误率、均方误差下降最快变量
	-	Forward Feature Elimination
		1.	当前初始变量集合$S_j$、剩余变量集合$S_j^c$
		2.	选择$S_j^c$中使$e_j$下降最快的变量进入$S_j$，
			$j=j+1$
		3.	判断是否满足终止条件，否则重复
			-	$j$达到限制
			-	错误率（均方误差）变化量改变量小于阈值
			$$
			\frac {|e_j-e_{j+1}|} {e_j} \leqslant \delta_{min}
			$$

####	基于变量重要性的加权KNN

-	加权距离：$EUCLID(x, y)=\sqrt {\sum_{i=1}^{p} w_{(i)}(x_i - y_i)^2}$
	，给重要变量赋予较高权重

-	计算变量重要性：Backward Feature Elimination
	$$
	FI_{(i)} = e_i + \frac {1} {p} \quad
		e_i为剔除变量i之后的均方误差（错误率）
	w_{(i)} = \frac FI_{(i)} {\sum_{j=1}^p FI_{(j)}}
	$$

####	基于观测相似性的加权KNN

-	与$X_0$越相似的观测，预测时重要性（权重）越大，改进前述
	方法，K个近邻对预测结果有“同等力度”的影响

-	权重：采用函数$K(d)$将距离d转换相似性，$K(d)$应该有如下
	特性
	-	非负：$K(d) \geqslant 0, d \in R^n$
	-	$max K(d) = K(0)$
	-	$K(d)$是单调减函数，距离越远，相似性越小
	核函数符合上述特征，且研究表明除均匀核外，其他核函数预测
	误差差异均不明显

#####	步骤

1.	依据函数距离函数$d(Z_{(i)}, Z_0}$找到$X_0$的k+1个近邻

	1.	变量预处理后得到Z，$z_{ij}=\frac x_{ij} {\sigma_{x_i}}$
	2.	计算距离d
		-	数值型
			$$
			d(Z_{(i)}, Z_0) = \sqrt [k]
				{\sum_{j=1}^p |Z_{(i)j} - Z_{0j}|^k}
			$$

		-	分类型：其对应各虚拟变量值$V_{j1}, V_{j2},...$
			相减的k次方之和，处以虚拟变量个数m
	3.	调整距离d的取值范围
		$$
		D(Z_{(i)}, Z_0) = \frac {d(Z_{(i)}, Z_0}
			{d(Z_{(k+1)}, Z_0)}, \quad i=1,2,...,k
		$$

2.	通过函数$w_i=K(d)$转换为相似性，确定k各近邻的权重

3.	预测
	
	-	回归预测
		$$
		\hat{y}_0 = \frac 1 k (\sum_{i=1}^k w_iy_i)
		$$
	-	分类预测
		$$
		\hat{y}_0 = max_r (\sum_{i=1}^k w_iI(y_i=r)) \\
		P(\hat{y}_0=r|X_0)= \frac
			{\sum_{i=1}^k w_iI(y_i=r)} {\sum_{i=1}^k w_i}
		$$

