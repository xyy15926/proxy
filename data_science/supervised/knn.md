#	K近邻

##	K-NN


-	基本思想：为预测新观测$X_0$输出变量$y_0$的取值

	-	输入为实例特征向量，对应于特征空间的中点，输出为实例
		类别，可以取多类

	-	在已有数据中找到与$X_0$相似的若干个观测
		$(X_1, X_2, ..., X_k)$，称为$X_0$的近邻

	-	对近邻$(X_1, X_2, ..., X_k)$的输出变量
		$(y_1, y_2, ..., y_k)$，计算诸如算术平均值
		（加权平均值、中位数、众数），作为新观测$X_0$输出
		变量取值$y_0$的预测值

-	特点

	-	k近邻不具有显式学习过程、简单、直观
	
	-	不需要假设$y=f(X)$函数体形式，实际上是利用训练数据集
		对特征空间进行划分

	-	只需假设$\hat{y}_0$是$(y_1, y_2, ..., y_k)$的函数：
		$\hat{y}_)=F(y_1, y_2, ..., y_k)$

###	出发点

-	将样本包含的n个观测数据看成p维特征空间的点
-	找到$X_0$的K个近邻
-	根据$X_0$的k个近邻依函数计算预测值
	-	回归（数值型）
		$$
		\hat{y}_0 = \frac {1} {K} \sum_{X_i \in N_K(X_0)} y_i
		$$
	-	分类预测：众数

###	特征选择

-	变量本身考察
	-	*low variance filter*：剔除标准差小于阈值数值型变量
	-	*missing values ratio*：剔除缺失值大于阈值的变量
	-	剔除众数比率大于阈值的分类型变量

-	变量与输出变量相关性角度考察
	-	*high correlation filter*

-	对预测误差影响角度考察
	-	Wrapper方法：逐个选择使错误率、均方误差下降最快变量
		，可使用*Forward Feature Elimination*

##	k-NN模型

K-NN是使用模型：实际上对应于特征空间的划分

-	模型包括3个基本要素，据此划分特征空间，确定特征空间中
	每个点所属类
	-	k值选择
	-	距离度量：参见*data_science/ref/functions*
	-	分类决策规则

###	k值选择

k值选择对k-NN方法有重大影响

-	较小k值：相当于使用较小邻域中训练实例进行预测

	-	复杂模型，容易发生过拟合
	-	*approximation error*较小：只有于输入实例近、相似的
		训练实例才会对预测结果有影响
	-	*estimation error*较大：预测结果对近邻实例点非常敏感

-	较大k值：相当于使用较大邻域中训练实例进行预测

	-	简单模型
	-	估计误差较小
	-	近似误差较大：同输如实例远、相似程度差的训练实例也会
		对预测结果有影响

####	k=1

只使用一个近邻做预测

-	找到距离$X_0$最近的近邻$X_i$，用其取值作为预测值

-	模型简单、效果较理想

	-	尤其适合特征空间维度较低、类别边界不规则情况
	-	只根据单个近邻预测，预测结果受近邻差异影响极大，预测
		波动（方差）大，稳健性低

-	预测错误的概率不高于普通贝叶斯方法的两倍

	$$\begin{align*}
	P_e & = (1-p(y=1|X=X_0))P(y=1|X=X_0) +
			(1-p(y=0|X=X_0))P(y=0|X=X_0) \\
		& = 2P(y=1|X=X_0)(1-P(y=1|X=X_0)) \\
		& \leq 2(1-P(y=1|X=X_0)) \\
	\end{align*}$$

	> - $P(y=1|X=X_0)$：普通贝叶斯方法做分类预测，预测结果
		为1的概率
	> - 1-NN方法犯错的概率就是$X_0$、$X_i$二者实际值不同的
		概率？？？？

####	k=N

使用训练样本整体做预测

-	无论输入实例，预测结果完全相同
	-	对分类预测，预测结果为“众数”
	-	对回归预测，预测结果为“平均数”

-	模型过于简单、效果不好
	-	忽略训练实例中大量信息
	-	“稳健性”极好：预测值基本不受近邻影响，无波动

###	决策规则

####	分类决策规则

#####	*Majority Voting Rule*

多数表决规则：等价于经验风险最小化

-	分类损失函数为0-1损失函数，分类函数为
	$f: \mathcal{R^n} \leftarrow \{c_1, c_2, \cdots\}$

-	误分类概率$P(Y \neq f(X)) = 1 - P(Y=f(X))$

-	给定实例$x \in \mathcal{X}$的误分率为

	$$
	\frac 1 k \sum_{x \in N_k(x)} I(y_i \neq c_j) = 
	1 - \frac 1 k \sum_{x \in N_k(x)} I(y_i = c_j)
	$$

	> - $N_k(x)$：最近邻k个实例构成集合
	> - $c_j$：涵盖$N_k(x)$区域的类别
	> - $I$：指示函数

-	为使误分率（经验风险）最小，应选择众数

> - 经验风险的构造中，前提是近邻被认为属于相同类别$c_j$，
> - 当然这个假设是合理的，因为k-NN方法就是认为近邻类别相同，
	并使用近邻信息预测
> - $c_j$的选择、选择方法是模型选择的一部分，不同的$c_j$会
	有不同的经验风险

##	算法

-	实现k近邻法时，主要问题是对训练数据进行快速k近邻搜索，
	尤在特征空间维数大、训练数据量大

-	考虑使用特殊的结构存储训练数据，减少计算距离次数，提高
	k近邻搜索效率

###	*linear scan*

线性扫描：最简单的实现方法

-	需要计算输入实例与每个训练实例的距离，训练集较大时计算
	非常耗时

###	*kd tree*

kd树：对k维空间中实例点进行存储，以便对其进行**快速检索**的
树形数据结构

-	kd树是搜索二叉树，表示对k维空间的一个划分
-	kd树中每个节点对应一个k维超矩形区域
-	构造kd树相当于不断用垂直于坐标轴的超平面将k维空间划分，
	构成一系列超矩形区域

> - kd树中k是维度，不是最近邻数

####	构造平衡kd树

平衡kd树：以为坐标轴中位数为切分点

-	平衡kd树搜索效率未必最优

> - 输入：k维空间数据集$T=\{x_1,x_2,...,x_N\}$
> - 输出：kd树

-	开始：构造根节点，对应于包含T的k维空间超矩形区域

	-	选择`x^{(1)}`为坐标轴，以T中所有实例的`x^{(1)}`坐标
		中位数为切分点

	-	以通过切分点、垂直坐标轴的超平面，将根节点对应
		超矩形区域切分为两个子区域，生成两个深度为1的左、右
		子节点

	-	将落在切分超平面上的实例点保存在根节点

-	重复：对深度为j的节点

	-	选择$x^{(l)}, l=j(modk)+1$为切分坐标轴，以该节点的
		区域中，所有实例$x^{(l)}$坐标中位数为切分

	-	将落在切分超平面上的实例点保存在当前节点中

-	直到两个子区域没有实例存在时停止，形成kd树划分

> - 坐标轴按次序划分是为方便搜索kd树

####	kd树最近邻搜索

> - 输入：已构造kd树
> - 输出：x的最近邻

-	在kd树种找出包含目标点x的叶节点的

	-	从根节点出发，比较对应坐标，递归进行访问，直到叶节点
		为止

	-	目标点在训练样本中不存在，必然能够访问到叶节点

-	以此叶节点为“当前最近点”

	-	目标点在此叶节点中点所在的区域内，且区域内只有该
		叶节点中点

-	回溯，并在每个节点上检查

	-	如果当前节点保存实例点比当前最近点距离目标的更近，
		更新该实例点为“当前最近点”

	-	检查该节点另一子区域是否可能具有更近距离的点

		-	即其是否同以目标点为圆心、当前最短距离为半径圆
			相交
		-	只需要比较目标点和相应坐标轴距离和最短距离即可

	-	若二者相交，则将目标节点视为**属于**该子区域中点，
		进行最近邻搜索，**递归向下**查找到相应叶节点，重新
		开始回退

	-	若二者不相交，则继续回退

-	退回到根节点时，搜索结束，最后“当前最近点”即为最近邻点

> - 这里涉及到回溯过程中，另一侧子域是否访问过问题，可以通过
	标记、比较相应轴坐标等方式判断
> - k>1的情况类似，不过检测时使用最远近邻，新近邻需要和所有
	原近邻依次比较

##	加权k-NN

###	变量重要性

计算变量的加权距离，重要变量赋予较高权重

-	变量重要性：*Backward Feature Elimination*得到各变量
	重要性排序

	$$
	FI_{(i)} = e_i + \frac {1} {p} \quad \\
	w_{(i)} = \frac {FI_{(i)}} {\sum_{j=1}^p FI_{(j)}}
	$$

	> - $e_i$：剔除变量i之后的均方误差（错误率）

-	加权距离：$d_w(x,y)=\sqrt {\sum_{i=1}^{p} w^{(i)}(x_i - y_i)^2}$


###	观测相似性

目标点的k个近邻对预测结果不应有“同等力度”的影响，与$X_0$越
相似的观测，预测时重要性（权重）越大

-	权重：用函数$K(d)$将距离d转换相似性，$K(d)$应该有特性

	> - 非负：$K(d) \geqslant 0, d \in R^n$
	> - 0处取极大：$max K(d) = K(0)$
	> - 单调减函数，距离越远，相似性越小

	-	核函数符合上述特征
	-	且研究表明除均匀核外，其他核函数预测误差差异均不明显

####	步骤

-	依据函数距离函数$d(Z_{(i)}, Z_0)$找到$X_0$的k+1个近邻

	-	使用第k+1个近邻距离作为最大值，调整距离在0-1之间
		$$
		D(Z_{(i)}, Z_0) = \frac {d(Z_{(i)}, Z_0)}
			{d(Z_{(k+1)}, Z_0)}, \quad i=1,2,...,k
		$$

-	依据函数$w_i=K(d)$确定k各近邻的权重

-	预测
	-	回归预测
		$$\hat{y}_0 = \frac 1 k (\sum_{i=1}^k w_iy_i)$$
	-	分类预测：多数表决原则
		$$
		\hat{y}_0 = max_r (\sum_{i=1}^k w_iI(y_i=r)) \\
		P(\hat{y}_0=r|X_0)= \frac
			{\sum_{i=1}^k w_iI(y_i=r)} {\sum_{i=1}^k w_i}
		$$

