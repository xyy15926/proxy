#	*Support Vector Machine*

##	总述

支持向量机是二分类模型

###	学习要素

-	基本模型：定义在特征空间上的间隔最大线性分类器

-	学习策略：间隔最大化

	-	可形式化为求解凸二次规划问题，也等价于正则化的合页
		损失函数最小化问题

	-	间隔最大使其有别于感知机，训练数据线性可分时分离
		超平面唯一

		-	误分类最小策略（0-1损失）得到分离超平面的解无穷
			多个
		-	距离和最小策略（平方损失）得到分离超平面唯一，
			单与此不同

-	学习算法：求解凸二次规划的最优化算法

###	数据

> - 输入空间：欧氏空间或离散集合
> - 特征空间：欧式空间或希尔伯特空间
> - 输出空间：离散集合？？？

-	SVM假设输入空间、特征空间为两个不同空间，SVM在特征空间
	上进行学习

-	线性[可分]支持向量机假设两个空间元素**一一对应**，并将
	输入空间中的输入映射为特征空间中特征向量

-	非线性支持向量机利用，输入空间到特征空间的非线性映射
	（核函数）将输入映射为特征向量

###	概念

-	*soft-margin maximization*：软间隔最大化

##	*Linear Support Vector Machine in Linearly Separable Case*

线性可分支持向量机：硬间隔支持向量机

-	训练数据线性可分时，通过硬间隔最大化策略学习

###	*hard-margin maximization*

硬间隔最大化：最大化超平面$(w,b)$关于线性可分训练数据集的
两类样本集几何间隔

-	直观含义：不仅将正负实例分开，而且对于最难分的实例点
	（离超平面最近的点），也有足够大的确信度将其分开，这样的
	超平面对位置新实例预测能力应该不错

-	约束最优化问题表述：
	$$
	\max_{w,b} \gamma \\
	s.t. \frac {y_i} {\|w\|} (wx + b) \geq \gamma,
		i=1,2,\cdots,N
	$$

-	考虑函数间隔、几何间隔关系得到问题
	$$
	\max_{w,b} \frac {\hat{\gamma}} {\|w\|} \\
	s.t. y_i(wx_i + b) \geq \hat{\gamma}, i=1,2,\cdots,N
	$$

-	而函数间隔$\hat{\gamma}$大小会随着超平面参数变化
	成比例变化，其取值对问题求解无影响，所以可取
	$\hat{\gamma}=1$带入，得到最优化问题

	$$
	\min_{w,b} \frac 1 2 {\|w\|}^2
	s.t. y_i(wx_i + b) - 1 \geq 0, i=1,2,\cdots,N
	$$

	> - $\max \frac 1 {\|w\|}$和
		$\min \frac 1 2 {\|w\|}^2$等价

###	最大间隔分离平面存在惟一性

定理：若训练数据集T线性可分，则可将训练数据集中样本点完全
正确分开的最大间隔分离超平面存在且唯一

####	存在性

-	训练数据集线性可分，所以最优化问题一定存在可行解
-	又目标函数有下界，
-	#todo

###	*Linear Support Vector Machine*

线性支持向量机

-	训练数据进行可分时，通过软间隔最大化策略学习

###	*Non-Linear Support Vector Machine*

非线性支持向量机

-	训练数据线性不可分时，使用*kernel trick*、软间隔最大化
	策略学习

####


