#	Feature Engineering

##	特征工程

特征工程：利用数据领域相关知识，创建能使机器学习算法达到最佳
性能的特征的过程

-	即人为设计输入变量，把原始数据转换为可以很好描述数据、
	建立在其上的模型性能接近最优的特征

-	特征工程重要性，特征越好
	-	模型选择灵活性越高：较好特征在简单模型上也能有较好
		效果，允许选择简单模型
	-	模型构建越简单：较好特征即使在超参不是最优时效果也
		不错，不需要花时间寻找最优参数
	-	模型性能越好

	> - 数据、特征决定了机器学习的上限，模型、算法只是逼近
		上限

##	*Feature Selection*

特征选择：从特征集合中选择**最具统计意义**的特征子集，达到
降维效果

###	*Filter*

筛选器：描述自变量、目标变量的关联，通过分析特征子集内部特点
衡量优劣，选择排名考虑特征

-	侧重于筛选单个特征
-	优点
	-	效率高
	-	对过拟合问题稳健性较好
-	缺点
	-	未考虑特征之间相关性，倾向选择冗余特征、忽略特征联合
		能力
-	评价函数
	-	Pearson相关系数
	-	Gini指数
	-	IG信息增益/互信息
	-	卡方统计量
	-	距离指标

###	*Wrapper*

封装器：利用特定分类器，用选取的特征子集对样本集进行分类，
以分类精度作为衡量特征子集优劣的标准，选择最好的特征子集

-	侧重于选择特征子集
-	优点
	-	考虑了特征之间的关联性
-	缺点
	-	观测数据较少时容易过拟合
	-	计算效率随特征数量增加迅速下降
-	方法/评价方案
	-	前向搜索
	-	后向搜索

###	*Embeded*

集成方法：由学习器自身自动选择特征

-	优点：兼具筛选器、封装器的优点
-	缺点：需要明确**好的选择**
-	典例
	-	$\mathcal{L}$范数正则化：Lasso、Ridge
	-	决策树算法：决策树自上而下选择分裂特征就是特征选择
	-	神经网络

###	构成

![feature_selection_procedure](imgs/feature_selection_procedure.png)

-	*generation procedure*：产生过程，搜索特征子集
-	*evaluation function*：评价函数，评价特征子集优劣
-	*stopping criterion*：停止准则，与评价函数相关的阈值，
	评价函数达到与阈值后可以停止搜索
-	*validation procedure*：验证过程，在验证数据集上验证选择
	特征子集的有效性

##	*Feature Construction*

特征构建：从原始数据中人工构建新特征

-	方法
	-	组合属性
	-	切分属性：如将时间戳分割为日期、上下午
-	主观要求高
	-	对问题实际意义有研究
	-	对数据敏感
	-	分析能力强

###	数值型

-	幅度调整：提高SGD收敛速度
	-	归一化
	-	标准化
-	log数据域变化
-	统计值
-	数据离散化：连续值分段
	-	等距切分
	-	分位数切分

###	分类型

-	*one-hot*编码：赋予各特征等权
-	hash技巧：针对文本类别数据，统计文本词表、倾向

###	时间戳

-	视为连续型：持续时间、间隔时间
-	视为离散值：一年中某些时间段

###	文本型

词带模型

-	将文本数据映射为稀疏向量
-	针对有序语句，将单词两两相连
-	TF-IDF统计量：反映词对在文档中重要程度

###	统计型

-	分位线
-	比例
-	次序

###	组合特征

-	特征拼接：GBDT生成特征组合路径

##	*Feature Extraction*

特征提取：将原始特征转换为具有物理、统计学意义特征，实现降维

-	目的
	-	信号表示：抽取后特征尽可能丢失较少信息
	-	信号分类：抽取后特征尽可能提高分类准确率
-	典例
	-	PCA：主成分分析
	-	ICA：独立成分分析
	-	LDA：线性判别分析






