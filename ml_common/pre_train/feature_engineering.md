#	*Feature Engineering*

特征工程：对原始数据进行一系列工程处理，将其提炼为特征，作为
输入供算法、模型使用

-	本质上：表示、展示数据的过程

-	目的：去除原始数据中的杂质、冗余，设计更高效的特征以刻画
	求解的问题、预测模型之间的关系
	-	把原始数据转换为可以很好描述数据特征
	-	建立在其上的模型性能接近最优

-	方式：**利用数据领域相关知识**、**人为设计输入变量**

-	特征工程重要性：特征越好
	-	模型选择灵活性越高：较好特征在简单模型上也能有较好
		效果，允许选择简单模型
	-	模型构建越简单：较好特征即使在超参不是最优时效果也
		不错，不需要花时间寻找最优参数
	-	模型性能越好
		-	排除噪声特征
		-	避免过拟合
		-	模型训练、预测更快

	> - 数据、特征决定了机器学习的上限，模型、算法只是逼近
		上限

##	数据预处理

-	结构化数据：可以看作是关系型数据库中一张表
	-	列有清晰定义
	-	行数据表示一个样本信息

-	非结构化数据：包含信息无法用简单数值表示
	-	没有清晰列表定义
	-	每个数据大小不相同
	-	文本、图像、音频、视频数据

###	缺失值

####	产生原因

-	信息暂时无法获取、成本高
-	信息被遗漏
-	属性不存在

####	缺失值影响

-	建模将丢失大量有用信息
-	模型不确定性更加显著、蕴含规则更难把握
-	包含空值可能使得建模陷入混乱，导致不可靠输出

####	处理方法

-	直接使用含有缺失值特征
-	删除含有缺失值特征
-	插值补全缺失值
	-	均值、中位数、众数
	-	同类均值、中位数、众数
	-	固定值
	-	建模预测：回归、决策树模型预测
		-	若其他特征和缺失特征无关，预测结果无意义
		-	若预测结果相当准确，缺失属性也没有必要纳入数据集
	-	高维映射：*one-hot*编码增加维度表示某特征缺失
		-	保留所有信息、未人为增加额外信息
		-	可能会增加数据维度、增加计算量
		-	需要样本量较大时效果才较好
	-	多重插补：认为待插补值是随机的，通常估计处待插补值，
		再加上**不同噪声**形成多组可选插补值，依据某准则，
		选取最合适的插补值
	-	压缩感知：利用信号本身具有的**稀疏性**，从部分观测
		样本中恢复原信号
		-	感知测量阶段：对原始信号进行处理以获得稀疏样本
			表示
			-	傅里叶变换
			-	小波变换
			-	字典学习
			-	稀疏编码
		-	重构恢复阶段：基于稀疏性从少量观测中恢复信号
	-	矩阵补全
	-	手动补全：根据对所在领域理解，手动对缺失值进行插补
		-	需要对问题领域有很高认识理解
		-	缺失较多时费时、费力
	-	最近邻补全：寻找与样本最接近样本相应特征补全

###	图片数据扩充

*Data Agumentation*：根据先验知识，在保留特点信息的前提下，
对原始数据进行适当变换以达到扩充数据集的效果

-	对原始图片做变换处理
	-	一定程度内随机旋转、平移、缩放、裁剪、填充、左右翻转
		，这些变换对应目标在不同角度观察效果
	-	对图像中元素添加噪声扰动：椒盐噪声、高斯白噪声
	-	颜色变换
	-	改变图像亮度、清晰度、对比度、锐度

-	先对图像进行特征提取，在特征空间进行变换，利用通用数据
	扩充、上采样方法
	-	*SMOTE*

-	*finetuning*：微调，接用在大数据集上预训练好的模型，然后
	在小数据集上进行微调
	-	简单的迁移学习
	-	可以快速寻外效果不错针对目标类别的新模型

###	异常值

> - 异常值/离群点：样本中数值明显偏离其余观测值的个别值

异常值分析：检验数据是否有录入错误、含有不合常理的数据

####	异常值检测

-	简单统计：观察数据统计型描述、散点图
-	$3\sigma$原则：取值超过均值3倍标准差，可以视为异常值
	-	依据小概率事件发生可能性“不存在”
	-	数据最好近似正态分布
-	箱线图：利用箱线图四分位距对异常值进行检测
-	基于模型预测：构建概率分布模型，计算对象符合模型的概率，
	将低概率对象视为异常点
	-	分类模型：异常点为不属于任何类的对象
	-	回归模型：异常点为原理预测值对象
	-	特点
		-	基于统计学理论基础，有充分数据和所用的检验类型
			知识时，检验可能非常有效
		-	对多元数据，可用选择少，维度较高时，检测效果不好
-	基于近邻度的离群点检测：对象离群点得分由其距离k-NN的距离
	确定
	-	k取值会影响离群点得分，取k-NN平均距离更稳健
	-	特点
		-	简单，但时间复杂度高$\in O(m^2)$，不适合大数据集
		-	方法对参数k取值敏感
		-	使用全局阈值，无法处理具有不同密度区域的数据集
-	基于密度的离群点检测
	-	定义密度方法
		-	k-NN分类：k个最近邻的平均距离的倒数
		-	DSSCAN聚类中密度：对象指定距离d内对象个数
	-	特点
		-	给出定量度量，即使数据具有不同区域也能很好处理
		-	时间复杂度$\in O^(m^2)$，对低维数据使用特点数据
			结构可以达到$\in O(mlogm)$
		-	参数难以确定，需要确定阈值
-	基于聚类的离群点检测：不属于任何类别簇的对象为离群点
	-	特点
		-	（接近）线性的聚类技术检测离群点高度有效
		-	簇、离群点互为补集，可以同时探测
		-	聚类算法本身对离群点敏感，类结构不一定有效，可以
			考虑：对象聚类、删除离群点再聚类
		-	检测出的离群点依赖类别数量、产生簇的质量
-	*one-class SVM*
-	*isolation forest*

####	异常值处理

-	直接删除
	-	简单易行
	-	观测值很少时，可能导致样本量不足、改变分布
-	视为缺失值处理
	-	可以利用现有变量信息，对异常值进行填补
-	平均值修正：使用前后两个观测值平均值进行修正
-	不处理

> - 很多情况下，要先分析异常值出现的可能原因，判断异常值是
	**真异常值**

###	类别不平衡问题

-	扩充数据集
	-	可以使用欠采样方法放弃部分大类数据

-	尝试其他评价指标：准确度在不平衡数据中不能反映实际情况
	-	混淆矩阵
	-	精确度
	-	召回率
	-	F1得分
	-	ROC曲线
	-	Kappa

-	对数据集重采样

	> - *over-sampling*：过采样，小类数据样本增加样本数量
	> - *under-sampling*：欠采样，大类数据样本减少样本数量

	-	尝试随机采样、非随机采样
	-	对各类别尝试不同采样比例，不必保持1:1违反现实情况
	-	同时使用过采样、欠采样

-	人工生成数据样本
	-	属性值随机采样：从类中样本每个特征随机取值组成新样本
	-	基于经验对属性值随机采样
	-	类似朴素贝叶斯方法：假设各属性之间相互独立进行采样，
		但是无法保证属性之前的线性关系
	-	*synthetic minority over-sampling technique*：过采样
		算法，构造不同于已有样本小类样本
		-	基于距离度量选择小类别下相似样本
		-	选择其中一个样本、随机选择一定数据量邻居样本
		-	对选择样本某属性增加噪声，构造新数据
-	尝试不同分类算法
-	对模型进行惩罚
	-	类似AdaBoosting：对分类器小类样本数据增加权值
	-	类似Bayesian分类：增加小类样本错分代价，如：
		penalized_SVM、penalized-LDA
	-	需要根据具体任务尝试不同惩罚矩阵
-	新角度理解问题
	-	将小类样本视为异常点：问题变为异常点检测、变化趋势
		检测
-	创新：对问题进行分析，将问题划分为多个小问题
	-	大类压缩为小类
	-	使用*one-class*分类器：小类作为异常点
	-	集成模型：训练多个分类器、组合

> - 需要具体问题具体分析

##	特征缩放

> - 正则化是**针对单个样本**的，将每个样本缩放到单位范数
> - 归一化针对单个属性，需要用到所有样本在该属性上值

###	*Normalizaion*

归一化/标准化：将特征/数据缩放到指定大致相同的数值区间

-	某些算法要求数据、特征数值具有零均值、单位方差
-	消除样本数据、特征之间的量纲/数量级影响
	-	量级较大属性占主导地位
	-	降低迭代收敛速度：梯度下降时，梯度方向会偏离最小值，
		学习率必须非常下，否则容易引起**宽幅震荡**
	-	依赖样本距离的算法对数据量机敏感

> - 决策树模型不需要归一化，归一化不会改变信息增益（比），
	Gini指数变化

####	*Min-Max Scaling*

线性函数归一化：对原始数据进行线性变换，映射到$[0, 1]$范围

$$
X_{norm} = \frac {X - X_{min}} {X_{max} - X_{min}}
$$

> - 训练集、验证集、测试集都使用训练集归一化参数

####	*Z-Score Scaling*

零均值归一化：将原始数据映射到均值为0，标准差为1的分布上

$$
Z = \frac {X - \mu} {\sigma}
$$

###	*Regularization*

正则化：将样本/特征**某个范数**缩放到单位1

$$\begin{align*}
\overrightarrow x_i & = (
	\frac {x_i^{(1)}} {L_p(\overrightarrow x_i)},
	\frac {x_i^{(2)}} {L_p(\overrightarrow x_i)}, \cdots,
	\frac {x_i^{(d)}} {L_p(\overrightarrow x_i)})^T \\
L_p(\overrightarrow x_i) & = (|x_i^{(1)}|^p + |x_i^{(2)}|^p + 
	\cdots + |x_i^{(d)}|^p)^{1/p}
\end{align*}$$

> - $L_p$：样本的$L_p$范数

-	使用内积、二次型、核方法计算洋房之间相似性时，正则化很
	有用

##	特征编码

###	*Ordinal Encoding*

序号编码：使用一位序号编码类别

-	一般用于处理类别间具有大小关系的数据
	-	编码后依然保留了大小关系

###	*One-hot Encoding*

独热编码：采用N位状态位对N个可能取值进行编码

-	一般用于处理类别间不具有大小关系的特征

-	独热编码后**特征表达能力变差**，特征的预测能力被人为拆分
	为多份
	-	通常只有部分维度是对分类、预测有帮助，需要借助特征
		选择降低维度

####	优点

-	能处理非数值属性
-	一定程度上扩充了特征
-	编码后向量时稀疏向量：可以使用向量的稀疏存储节省空间
-	能够处理缺失值：高维映射方法中增加维度表示缺失

####	缺点

-	k-NN算法：高维空间两点间距离难以有效衡量

-	逻辑回归模型：参数数量随维度增加而增大，增加模型复杂度，
	容易出现过拟合

-	决策树模型
	-	产生样本切分不平衡问题，切分增益非常小
		-	每个特征只有少量样本是1，大量样本是0
		-	较小的拆分样本集占总体比例太小，增益乘以所占比例
			之后几乎可以忽略
		-	较大拆分样本集的几乎就是原始样本集，增益几乎为0
	-	影响决策树的学习
		-	决策树依赖数据统计信息，独热编码将数据切分到零散
			小空间上，统计信息不准确、学习效果差
		-	独热编码后特征表达能力边人为拆分，与其他特征竞争
			最优划分点失败，最终特征重要性会比实际值低

###	*Binary Encoding*

二进制编码：先用序号编码给每个类别赋予类型ID，然后将类别ID
对应二进制编码作为结果

-	本质上利用二进制类别ID进行哈希映射，得到0/1特征向量
-	特征维度小于独热编码，更节省存储空间

###	二元化

二元化：将数值型属性转换为布尔型属性

-	通常用于假设属性取值分布为伯努利分布
-	方法：对特征指定阈值，特征值大于等于阈值取1，否则取0
	-	阈值是关键超参数，取值需要结合模型、具体任务选择

###	离散化

离散化：将连续的数值属性转换为离散的数值属性

-	**海量离散特征+简单模型**：适合线性模型
	-	模型简单
	-	特征工程比较困难，成功经验可以推广，可以多人并行研究
-	**少量连续特征+复杂模型**：适合非线性模型
	-	不需要复杂的特征工程
	-	模型复杂
-	连续特征离散化为0/1离散特征
	-	离散化得到稀疏向量内积乘法运算速度更快，计算结果方便
		存储
	-	模型会更稳定：对异常数据鲁棒性更好、降低模型过拟合
		风险
		-	模型不再拟合特征具体值，而是拟合某个概念，能够
			对抗数据扰动，更稳健
		-	需要拟合参数值更少，降低模型复杂度
	-	特征离散化相当于引入非线性，提升模型表达能力，增强
		拟合能力
		-	进行特征交叉离散化，可以进一步引入非线性，提高
			模型表达能力

####	分桶

分桶：离散化的常用方法

-	步骤
	-	将样本在连续特征上取值从小到大排列
	-	从小到大依次选择分桶边界，其中分桶数量以及每个桶大小
		都是超参数
	-	根据样本特征取值划分为相应桶内
-	桶数量、边界超参需要人工指定
	-	根据业务领域经验指定
	-	根据模型指定：根据具体任务训练分桶之后的数据集，通过
		超参数搜索确定最优分桶数量、边界
-	分桶经验、准则
	-	桶小大必须足够小：桶内属性取值对样本标记影响在不大
		范围内
	-	桶大小必须足够大：每个桶内有足够样本，否则随机性太大
		，不具有统计意义上说服力
	-	桶内样本尽量分布均匀

##	*Feature Selection*

特征选择：从特征集合中选择**最具统计意义**的特征子集

> - *relevant feature*：相关特征，对当前学习任务有用的属性、
	特征
> > -	特征选择最重要的是确保不丢失重要特征
> - *irrelevant feature*：无关特征，对当前学习任务无用的
	属性、特征
> - *redundant feature*：冗余特征，包含的信息可以由其他特征
	中推演出来
> > -	冗余特征通常不起作用，剔除可以减轻模型训练负担
> > -	若冗余特征恰好对应完成学习任务所需要的中间概念，则
		是有益的，可以降低学习任务的难度

-	特征选择会降低模型预测能力，因为被剔除特征中可能包含有效
	信息
	-	保留尽可能多特征，模型性能会提升，模型更复杂、计算
		复杂度同样提升
	-	剔除尽可能多特征，模型性能会下降，模型更简单、降低
		计算复杂度

-	特征选择原因
	-	维数灾难问题：仅需要选择一部分特征构建模型，可以减轻
		维数灾难问题，从此意义上特征选择和降维技术有相似动机
	-	剔除无关特征可以降低学习任务难度，简化模型、降低计算
		复杂度

###	特征选择方法

-	遍历：没有先验（问题相关领域）知识的情况下，从初始特征
	集合选择包含所有重要信息的特征子集
	-	特征数量稍多会出现组合爆炸

-	迭代
	-	产生候选子集，评价优劣
	-	基于评价结果产生下个候选子集，评价优劣
	-	不断迭代，直至**无法找到更好的后续子集**

-	特征子集搜索+子集评价即可得到特征选择方法

####	子集搜索

-	给定特征$A=\{A_1, A_2, \cdots, A_d\}$，将每个特征视为
	候选子集（每个子集只有一个元素），对d个候选子集进行评价

-	在上轮选定子集中加入特征，选择包含两个特征的最优候选子集

-	假定在$k+1$轮时，最优特征子集不如上轮最优的特征子集，则
	停止生成候选子集，将上轮选定特征子集作为特征选择结果


> - *Forward Feature Elimination*：前向特征选择，逐渐增加
	相关特征
> - *Backward Feature Elimination*：后向特征选择，从完整特征
	集合开始，每次尝试去掉无关特征，逐渐剔除特征
> - *Bidirectional Feature Elimination*：双向特征选择，结合
	前向、后向搜索
> > -	每轮逐渐增加选定的相关特征，特征在后续迭代中确定不会被
		去除，同时减少无关特征

####	子集评价

-	根据特征子集A取值可以将数据集D划分为多个子集
	-	每个划分区域对应A上的取值，样本标记y对应D的真实划分
	-	能判断划分之间差异的机制都能作为特征子集的评价

-	信息增益：信息增益越大，表明特征子集A包含有助于分类信息
	越多
	-	事实上，决策树可以用于特征选择，所有树结点划分属性
		组成的集合就是选择出来的特征子集

####	特征选择组件

![feature_selection_procedure](imgs/feature_selection_procedure.png)

-	*generation procedure*：产生过程，搜索特征子集
-	*evaluation function*：评价函数，评价特征子集优劣
-	*stopping criterion*：停止准则，与评价函数相关的阈值，
	评价函数达到与阈值后可以停止搜索
-	*validation procedure*：验证过程，在验证数据集上验证选择
	特征子集的有效性

###	*Filter*

过滤式：对数据集进行的特征选择过程与后续学习器无关，即设计
统计量过滤特征，不考虑后续学习器问题

-	通过分析特征子集内部特点衡量特征优劣，描述自变量、目标
	变量的关联

-	特点
	-	时间效率高
	-	对过拟合问题较稳健
	-	倾向于选择**单个**、**冗余**特征，没有考虑特征之间
		相关性

-	评价函数
	-	Pearson相关系数
	-	Gini指数
	-	IG信息增益/互信息
	-	卡方统计量
	-	距离指标

####	*Relief: Relevant Features*

Relief方法：设置相关统计量度量特征重要性

-	统计量为向量
	-	每个分量对应一个初始特征
	-	特征子集重要性由子集中每个特征对应的相关统计量分量
		之和决定
-	特征选择方法
	-	指定阈值k：选择比k大的相关统计量分量对应特征
	-	指定特征个数m：选择相关统计量分量最大的m个特征

> - 只适合二分类问题，扩展变体*Relief-F*可以处理多分类问题

####	方差选择法

方差选择法：计算各特征方差，根据阈值选择方差大于阈值的特征

> - 类似有启发式选择方法：剔除**取值变化的特征**

####	相关稀疏法

相关系数法：计算各特征对目标值的Pearson相关系数、相关系数
P值

####	卡方检验

卡方检验：检验定性自变量对定性因变量的相关性

-	观察实际值与理论值得偏差程度
	-	若偏差足够小，任务误差是自然样本误差，变量之间独立
	-	若偏差大到某个程度，不可能是偶然因素引起，认为变量
		直接相关

-	构建卡方统计量：体现自变量对因变量的相关性

	$$
	\mathcal{X}^2 = \sum \frac {(A - E)^2} E
	$$

	> - $A$：自变量、因变量组合对应频数观察值
	> - $E$：自变量、因变量组合对应频数期望值

#####	特点

-	由于随机误差存在，卡方统计量容易夸大频数较小的特征影响
-	只存在少数类别中特征的卡方统计量值可能很小，容易被排除，
	而往往这类词对分类贡献很大

####	信息增益/互信息法

信息增益/互信息法：同样衡量定性自变量对定性因变量相关性

$$
I(X;Y) = \sum_{x \in X} \sum_{y \in Y} p(x,y)
	log \frac {p(x,y)} {p(x)p(y)}
$$

#####	特点

-	信息量与变量的可能变化有关，与变量的具体取值无关
-	信息增益方法总是倾向于选择由取值较多的特征
-	只能考察特征对整个系统的贡献
	-	无法具体到某个类别上
	-	只适合作全局特征选择，即所有类使用相同的特征集合

###	*Wrapper*

包裹式：直接把最终要使用的**学习器性能**作为特征子集评价
原则，为给定学习器选择最有利其性能、特化的特征子集

-	优点
	-	直接针对特定学习器进行优化
	-	考虑了特征之间的关联性，通常训练效果较过滤式好
-	缺点
	-	特征选择过程中需要多次训练学习器，计算效率较低
	-	观测数据较少时容易过拟合

####	*Las Vegas Wrapper*

*LVW*：在*Las Vegas Method*框架下使用随机策略进行子集搜索，
以最终分类器误差作为特征子集评价标准

-	包含停止条件控制参数T，避免每次子集评价训练特征子集开销
	过大
-	若初始特征数量很多、T设置较大、每轮训练时间较长，算法
	执行很长时间都不会停止
	-	LVM可能无法得到解（拉斯维加斯算法本身性质）

####	递归特征消除法

递归特征消除法：使用基模型进行多轮训练，每轮训练消除若干权值
系数的特征，再基于特征集进行下一轮训练

####	子集回归

-	前向变量选择
-	后向变量选择
-	最优子集选择

###	*Embeded*
#todo

嵌入式：将特征选择和学习器训练过程融为以提，在同一优化过程中
完成，即学习器训练过程中自动进行特征选择

-	优点：兼具筛选器、封装器的优点
-	缺点：需要明确**好的选择**

####	正则化

-	$L_1$、$L_2$范数：主要用于线性回归、逻辑回归、SVM等算法
	-	Ridge：$L_2$范数
	-	Lasso：$L_1$范数
		-	除降低过拟合风险，还容易获得稀疏解
		-	参数$\lambda$越大，稀疏性越大，被选择特征越少
	-	SVM、逻辑回归
		-	参数$c$越小，稀疏性越大，被选择特征越少

####	决策树

利用决策树思想：决策树自上而下选择分裂特征就是特征选择

-	决策树
-	随机森林
-	Gradient Boosting

####	神经网络

##	*Feature Extraction*

特征提取：从原始数据中转换为具有物理、统计学意义特征，自动
构建新的特征

-	目的：自动构建新特征
	-	信号表示：抽取后特征尽可能丢失较少信息
	-	信号分类：抽取后特征尽可能提高分类准确率

> - 特征选择：表示出每个特征对于模型构建的重要性
> - 特征提取：有时能发现更有意义的特征属性
> - 特征构建：相较于特征提取，需要人为的手工构建特征

###	降维

####	*Principal Component Analysis*

*PCA*：主成分分析，找到数据中主成分，用主成分来表征原始数据
，达到降维目的

-	思想：通过**坐标轴转换，寻找数据分布的最优子空间**
	-	特征向量可以理解为坐标转换中新坐标轴方向
	-	特征值表示对应特征向量方向上方差
		-	特征值越大、方差越大、信息量越大
		-	抛弃较小方差特征
-	PCA缺陷：线性降维方法
	-	*KPCA*：核主成分分析，核映射对PCA进行扩展
	-	流形映射降维方法：等距映射、局部线性嵌入、拉普拉斯
		特征映射

#####	步骤

-	对样本数据进行中心化处理（和统计中处理不同）
-	求样本协方差矩阵
-	对协方差矩阵进行特征值分解，将特征值从大至小排列
-	取前p个最大特征值对应特征向量作为新特征，实现降维

####	*Linear Discriminant Analysis*

*LDA*：线性判别分析，寻找投影方向，使得投影后样本尽可能按照
原始类别分开，即寻找可以最大化类间距离、最小化类内距离的方向

-	相较于PCA，LDA考虑数据的类别信息，不仅仅是降维，还希望
	实现“分类”

-	优点：相较于PCA
	-	LDA更适合处理带有类别信息的数据
	-	模型对噪声的稳健性更好

-	缺点
	-	对数据分布有很强假设：各类服从正太分布、协方差相等，
		实际数据可能不满足
	-	模型简单，表达能力有限，但可以通过核函数扩展LDA处理
		分布比较复杂的数据

> - Fisher判别分析

####	*Independent Component Analysis*

*ICA*：独立成分分析，寻找线性变换$z=Wx$，使得$z$各特征分量
之间独立性最大

-	思想
	-	假设随机信号$x$服从模型
		$$x = As$$
		> - $s$：未知源信号，分量相互独立
		> - $A$：未知混合矩阵
	-	ICA通过观察$x$估计混合矩阵$A$、源信号$s$，认为源信号
		携带更多信息

> - 若原信号非高斯，则分解唯一，否则可能有无穷多分解
> - 因子分析，也称*Blind Source Separation*（盲源分离）

#####	算法

-	大多数ICA算法需要进行数据预处理：先用PCA得到主成分$Y$，
	再把各个主成分各分量标准化得到$Z$满足
	-	$Z$各分量不相关
	-	$Z$各分量方差为1

-	*FastICA*算法：寻找方向$w$使得随机变量$w^T z$某种
	“非高斯性”度量最大化
	-	四阶矩

###	图像特征提取

> - 以下是传统的图像特征提取方法，现在应该都是CNN进行特征
	提取、分类
> - 详情参见*machine_learning/cv*

####	*LBP*特征

-	*Sobel Operator*
-	*Laplace Operator*
-	*Canny Edge Detector*

####	基于角点

-	*Moravec*
-	*Harris*
-	*GoodFeaturesToTrack*
-	*FAST*

####	基于尺度空间

-	*Scale-Invariant Feature Transform*
-	*Speeded Up Robust Feature*
-	*Brief*
-	*Oriented Brief*

####	*HOG*特征

方向梯度直方图特征：通过计算、统计图像局部区域梯度方向直方图
实现特征描述

#####	步骤

-	归一化处理：图像转换为灰度图像，再利用伽马校正实现
	-	提高图像特征描述对光照、环境变量稳健性
	-	降低图像局部阴影、局部曝光、纹理失真
	-	尽可能抵制噪声干扰
-	计算图像梯度
-	统计梯度方向
-	特征向量归一化（块内）
	-	克服光照不均匀变化及前景、背景对比差异
-	生成特征向量

###	文本特征提取

####	词袋模型

词袋模型：将争端文本以词为单位切分，则每篇文章可以表示为
长向量

-	向量每个维度代表一个单词
-	维度权重反映单词在原文章中重要程度
	-	通常使用*TF-IDF*计算权重

#####	*TF-IDF*

$$\begin{align*}
TF-IDF(t, d) & = TF(t, d) * IDF(t) \\
IDF(t) & = log \frac {文章总数}
	{包含单词t的文章总数 + 1}
\end{align*}$$

> - $TF(t, d)$：单词$t$在文档$d$中出现的频率
> - $IDF(t)$：逆文档频率，衡量单词对表达语义的重要性
> > -	若单词在多篇文章中出现过，则可能是通用词汇，对区分
		文章贡献较小，$IDF(t)$较小、权重较小

####	*N-gram*模型

N-gram模型：将连续出现的$n, n \leq N$个词组成的词组N-gram
作为单独特征放到向量中

-	相较于词袋模型，考虑单词组合意义
-	*word stemming*：将不同词性单词统一为同一词干形式
	-	同一个词可能有多种词性变化，却拥有相同含义

####	*Word-Embedding*模型

词嵌入模型：将每个词都映射为低维空间上的稠密向量

-	*Word2Vec*：常用词嵌入模型，底层神经网络
	-	*Continuous Bag of Words*：根据上下文词语预测当前词
		生成概率
	-	*Skip-gram*：根据当前词预测上下文中各个词的生成概率

-	实际上直接使用矩阵作为源文本特征作为输入进行训练，难以
	得到好结果，往往需要提取、构造更高层特征

##	*Feature Construction*

特征构建：从原始数据中人工构建新特征

-	方法
	-	组合属性：混合属性创建新特征
	-	切分属性：分解、切分原有特征创建新特征，如将时间戳
		分割为日期、上下午
-	主观要求高
	-	对问题实际意义、相关领域有研究：思考问题形式、数据
		结构
	-	对数据敏感：需要观察原始数据
	-	分析能力强

####	数值型

-	幅度调整：提高SGD收敛速度
	-	归一化
	-	标准化
-	log数据域变化
-	统计值
-	数据离散化：连续值分段
	-	等距切分：各类分布不均
	-	分位数切分：各类分布均匀，但异质性不均
-	平方、开根：增加非线性化

####	分类型

-	*one-hot*编码：赋予各特征等权
-	hash技巧：针对文本类别数据，统计文本词表、倾向
-	多分类转二分类：输入变量类别合并，超类
	-	*twoing*策略：使两个超类差异足够大的合并点（分割点）
	-	*ordering*策略：对有序类型，只有两个连续基类才能合并

####	时间戳

-	视为连续型：持续时间、间隔时间
-	视为离散值：一年中某些时间段

####	文本型

-	词袋模型
	-	将文本数据映射为稀疏向量
	-	针对有序语句，将单词两两相连
	-	TF-IDF统计量：反映词对在文档中重要程度

-	Token化
	-	将语句分成token次

####	统计型

-	分位线
-	比例
-	次序

####	组合特征

-	特征拼接：GBDT生成特征组合路径

####	图像

-	提取边缘、尺度不变特征变换特征

