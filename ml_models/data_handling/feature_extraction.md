---
title: 特征提取
categories:
  - ML Techs
  - Data Handling
tags:
  - Machine Learning
  - Data Handling
  - Feature Engineering
date: 2020-11-03 19:56:42
updated: 2020-11-03 19:56:42
toc: true
mathjax: true
description: 特征提取
---

##	*Feature Extraction*/*Feature Construction*

特征提取/构建：把原始数据中转换为具有物理、统计学意义特征，
构建新的人工特征

-	主观要求高
	-	对问题实际意义、相关领域有研究：思考问题形式、数据
		结构
	-	对数据敏感：需要观察原始数据
	-	分析能力强

-	目的：自动构建新特征
	-	信号表示：抽取后特征尽可能丢失较少信息
	-	信号分类：抽取后特征尽可能提高分类准确率

-	方法
	-	组合属性：混合属性创建新特征
	-	切分属性：分解、切分原有特征创建新特征，如将时间戳
		分割为日期、上下午

> - 特征选择：表示出每个特征对于模型构建的重要性
> - 特征提取：有时能发现更有意义的特征属性
> - 有时从额外划分特征构建，其相较于特征提取，需要人为的手工
	构建特征，偏经验、规则

##	通用特征提取

> - 部分详见*ml_techniques/data_handling/data_preprocessing*

###	数值型

-	幅度调整：提高SGD收敛速度
	-	归一化
	-	标准化
-	log数据域变化
-	统计值
-	数据离散化：连续值分段
	-	等距切分：各类分布不均
	-	分位数切分：各类分布均匀，但异质性不均
-	平方、开根：增加非线性化

###	分类型

-	*one-hot*编码：赋予各特征等权
-	hash技巧：针对文本类别数据，统计文本词表、倾向
-	多分类转二分类：输入变量类别合并，超类
	-	*twoing*策略：使两个超类差异足够大的合并点（分割点）
	-	*ordering*策略：对有序类型，只有两个连续基类才能合并

###	时间戳

-	视为连续型：持续时间、间隔时间
-	视为离散值：一年中某些时间段

###	统计型

-	分位线
-	比例
-	次序

###	组合特征

-	特征拼接：GBDT生成特征组合路径

##	降维

###	*Principal Component Analysis*

*PCA*：主成分分析，找到数据中主成分，用主成分来表征原始数据
，达到降维目的

-	思想：通过**坐标轴转换，寻找数据分布的最优子空间**
	-	特征向量可以理解为坐标转换中新坐标轴方向
	-	特征值表示对应特征向量方向上方差
		-	特征值越大、方差越大、信息量越大
		-	抛弃较小方差特征
-	PCA缺陷：线性降维方法
	-	*KPCA*：核主成分分析，核映射对PCA进行扩展
	-	流形映射降维方法：等距映射、局部线性嵌入、拉普拉斯
		特征映射

####	步骤

-	对样本数据进行中心化处理（和统计中处理不同）
-	求样本协方差矩阵
-	对协方差矩阵进行特征值分解，将特征值从大至小排列
-	取前p个最大特征值对应特征向量作为新特征，实现降维

###	*Linear Discriminant Analysis*

*LDA*：线性判别分析，寻找投影方向，使得投影后样本尽可能按照
原始类别分开，即寻找可以最大化类间距离、最小化类内距离的方向

-	相较于PCA，LDA考虑数据的类别信息，不仅仅是降维，还希望
	实现“分类”

-	优点：相较于PCA
	-	LDA更适合处理带有类别信息的数据
	-	模型对噪声的稳健性更好

-	缺点
	-	对数据分布有很强假设：各类服从正太分布、协方差相等，
		实际数据可能不满足
	-	模型简单，表达能力有限，但可以通过核函数扩展LDA处理
		分布比较复杂的数据

> - Fisher判别分析

###	*Independent Component Analysis*

*ICA*：独立成分分析，寻找线性变换$z=Wx$，使得$z$各特征分量
之间独立性最大

-	思想
	-	假设随机信号$x$服从模型
		$$x = As$$
		> - $s$：未知源信号，分量相互独立
		> - $A$：未知混合矩阵
	-	ICA通过观察$x$估计混合矩阵$A$、源信号$s$，认为源信号
		携带更多信息

> - 若原信号非高斯，则分解唯一，否则可能有无穷多分解
> - 因子分析，也称*Blind Source Separation*（盲源分离）

####	算法

-	大多数ICA算法需要进行数据预处理：先用PCA得到主成分$Y$，
	再把各个主成分各分量标准化得到$Z$满足
	-	$Z$各分量不相关
	-	$Z$各分量方差为1

-	*FastICA*算法：寻找方向$w$使得随机变量$w^T z$某种
	“非高斯性”度量最大化
	-	四阶矩

##	图像特征提取

-	提取边缘、尺度不变特征变换特征

> - 以下是传统的图像特征提取方法，现在应该都是CNN进行特征
	提取、分类
> - 详情参见*machine_learning/cv*

###	*LBP*特征

-	*Sobel Operator*
-	*Laplace Operator*
-	*Canny Edge Detector*

###	基于角点

-	*Moravec*
-	*Harris*
-	*GoodFeaturesToTrack*
-	*FAST*

###	基于尺度空间

-	*Scale-Invariant Feature Transform*
-	*Speeded Up Robust Feature*
-	*Brief*
-	*Oriented Brief*

###	*HOG*特征

方向梯度直方图特征：通过计算、统计图像局部区域梯度方向直方图
实现特征描述

####	步骤

-	归一化处理：图像转换为灰度图像，再利用伽马校正实现
	-	提高图像特征描述对光照、环境变量稳健性
	-	降低图像局部阴影、局部曝光、纹理失真
	-	尽可能抵制噪声干扰
-	计算图像梯度
-	统计梯度方向
-	特征向量归一化（块内）
	-	克服光照不均匀变化及前景、背景对比差异
-	生成特征向量

##	文本特征提取

> - 具体参见*ml_specification/natural_language_processing/#todo*

###	词袋模型

词袋模型：将文本以词为单位切分token化

-	文章可以表示为稀疏长向量，向量每个维度代表一个单词
	-	针对有序语句，将单词两两相连
	-	维度权重反映单词在原文章中重要程度
		-	通常使用*TF-IDF*统计量表示词权重

-	*TF-IDF*

	$$\begin{align*}
	TF-IDF(t, d) & = TF(t, d) * IDF(t) \\
	IDF(t) & = log \frac {文章总数}
		{包含单词t的文章总数 + 1}
	\end{align*}$$

	> - $TF(t, d)$：单词$t$在文档$d$中出现的频率
	> - $IDF(t)$：逆文档频率，衡量单词对表达语义的重要性
	> > -	若单词在多篇文章中出现过，则可能是通用词汇，对区分
			文章贡献较小，$IDF(t)$较小、权重较小

###	*N-gram*模型

N-gram模型：将连续出现的$n, n \leq N$个词组成的词组N-gram
作为单独特征放到向量中

-	相较于词袋模型，考虑单词组合意义
-	*word stemming*：将不同词性单词统一为同一词干形式
	-	同一个词可能有多种词性变化，却拥有相同含义

###	*Word-Embedding*模型

词嵌入模型：将每个词都映射为低维空间上的稠密向量

-	*Word2Vec*：常用词嵌入模型，底层神经网络
	-	*Continuous Bag of Words*：根据上下文词语预测当前词
		生成概率
	-	*Skip-gram*：根据当前词预测上下文中各个词的生成概率

-	实际上直接使用矩阵作为源文本特征作为输入进行训练，难以
	得到好结果，往往需要提取、构造更高层特征


