---
title: Decision Tree
tags:
  - Model
  - Decision Tree
  - ID3
  - CART
categories:
  - 模型
  - 非线性模型
date: 2019-07-13 23:25:01
updated: 2019-07-13 12:03:11
toc: true
mathjax: true
comments: true
description: Deision Tree
---

##	概述

###	本质

决策树：本质上是从训练数据中归纳出一组分类规则

-	与训练数据不矛盾的分类规则（即能对训练数据正确分类）可能
	有多个、没有，需要找到矛盾较小、泛化能力较好的
-	决策树学习也是由训练数据集估计条件概率模型，需要寻找对
	训练数据有很好拟合、对未知数据有很好预测的模型

####	分类规则集合

决策树可以看作是*if-then*规则的集合：体现输入、输出变量
逻辑关系

-	决策树根节点到叶节点每条路径构成一条规则

-	路径上内部节点的特征对应规则的条件，叶节点对应规则结论

-	决策树的路径或其对应的*if-then*规则集合**互斥且完备**，
	即每个实例有且仅有一条路径覆盖

####	条件概率分布

决策树可以表示定义在特征空间、类空间上的条件概率分布

-	此条件概率分布定义在特征空间的一个划分上

	-	其中每个单元定义一个类的概率分布就构成一个条件概率
		分布
	-	决策树中一条路径（叶节点）对应划分中一个单元

-	条件概率分布由**各单元的给定条件下**，各类的条件概率分布
	组成

	-	$P(Y|X)$：$X$为表示特征的随机变量（取值各个单元），
		$Y$表示类的随机变量
	-	各叶节点上的条件概率往往偏向于某类，决策树分类时将
		属于该节点实例分为该类

###	结构

分析结论、展示方式类似一棵倒置的树

-	决策树由*node*、*directed edge*组成
	-	*interal node*：内部节点，表示特征、属性
	-	*leaf node*：一个类

-	对训练数据进行分类
	-	从根节点开始，对实例某特征进行测试，根据测试结果将
		实例分配到其子节点，对应该特征一个取值
	-	递归地对实例进行分配，直至到达叶子节点，将实例分到
		叶节点地类中

-	对新数据$X_0$的预测
	-	从决策树的树根到树叶搜索，确定数所的叶子节点
	-	利用叶子节点中训练数据集预测：众数类、均值

###	特点

-	优势
	-	能有效处理分类型输入变量
	-	能够实现非线性分割
	-	模型具有可读性，分类速度块

-	问题
	-	充分生长的决策有高方差，预测不稳定
	-	剪枝可以提高预测稳健性，但是预测精度可能会下降

###	分类

##	特征选择

选取对训练数据具有**分类能力**的特征，提高决策树学习效率

###	*Infomation Gain*特征选择

$$
g(D, A) = H(D) - H(D|A)
$$

-	经验熵$H(D)$：对数据集D进行分类的不确定性

-	经验条件熵$H(D|A)$：在特征A给定条件下，对数据集D进行分类
	的不确定性

-	信息增益：由于特征A而使得数据集D的分类的不确定性减少程度
	-	依赖于特征，不同特征有不同的信息增益
	-	信息增益大的特征具有更强的分类能力

-	*infomation gain ratio*：信息增益比

	$$\begin{align*}
	g_R(D,A) & = \frac {g(D,A)} {H_A(D)} \\
	H_A(D) & = -\sum_{m=1}^M \frac {|D_i|} {|D|}
		log_2 \frac {|D_i|} {|D|}
	\end{align*}$$

	> - $H_A(D)$：*split information*，类似于以A作为随机变量
		的熵
	> - $D_i$：指标A第i个取值样本集合

> - 熵参见*data_science/reference/model_evaluation*

####	步骤

对训练数据集（或子集）D，计算其每个特征的信息增益，选择其中
信息增益最大的特征

> - 输入：训练数据集D、特征A，其中训练集D包含K个类别，特征A
	有M种取值
> - 输出：特征A对训练数据集D的信息增益$g(D,A)$

-	计算数据集D经验熵
	$$
	H(D) = \sum_{k=1}^K \frac {|C_k|} {|D|}
		log_2 \frac {|C_k|} {|D|}
	$$

-	计算特征A对数据集D的条件经验熵
	$$\begin{align*}
	H(D|A) & = \sum_{m=1}^M \frac {|D_m|} {|D|} H(D_m) \\
	& = -\sum_{m=1}^M \frac {|D_m|} {|D|}
		\sum_{k=1}^K \frac {|D_{m,k}|} {|D_m|}
		log_2 \frac {|D_{m,k}|} {|D_m|}
	\end{align*}$$

-	计算信息增益$g(D,A)=H(D) - H(D|A)$

##	决策树生成

决策树的生成就是递归的构造决策树的过程

-	使用自顶向下贪心算法

	-	从所有可能决策树中选取最优决策树是NP完全问题

	-	所以实际决策树算法通常采用**启发式**算法，近似求解
		这一最优化问题，得到*sub-optimal*决策树

	-	从包含所有数据的根节点开始，递归的选择**当前**最优
		特征对训练数据进行分割，使得各子数据集有当前最好分类

	-	此样本不断分组过程对应特征空间的划分、决策树的构建

-	原则：使区域（节点）内观测输出变量的异质性下降最大，从而
	确定

	-	最佳分组变量$x_j$
	-	分组变量取值中的最佳分割点s

###	ID3算法

在决策树各个节点上应用信息增益准则选择特征，递归的构建决策树

####	步骤

> - 输入：训练数据集D，特征集A，阈值$\epsilon$
> - 输出：决策树T

-	若D中所有实例属于同一类$C_k$，则T为单节点树，并将类$C_k$
	作为该节点的类标记，返回T

-	若$A = \varnothing$，则T为单节点树，将D中实例数最大的类
	$C_k$作为该节点的类标记，返回T

-	否则，计算A各个特征对D的信息增益，选择信息增益最大的特征
	$A_g$

-	若$A_g$的信息增益小于阈值$\epsilon$，则置T为单节点数树，
	并将D中实例数最大的类$C_k$作为该节点的类标记，返回T

-	否则，对$A_g$**每个**可能值$a_m$，将D分割为若干非空子集
	$D_i$，将$D_i$中**实例数最大**的类作为标记，构建子节点，
	由节点极其子节点构成树T并返回

-	对第i个子节点，以$D_i$为训练集，以$A-{A_g}$为特征集，
	递归的构造子树$T_i$并返回

####	特点

-	每个特征只会被用于进行分割（建立子节点）一次，每次所有
	取值都会被用于建立子节点
	-	因此ID3树是多叉树，各个节点的分叉个数取决于使用特征

-	只有树的生成，生成树容易产生过拟合

-	以信息增益作为划分训练数据集的特征，倾向于选择取值较多
	的特征进行划分

	-	理论上特征取值严格遵循比例，取值多则其各个值对应样本
		数量较小，对信息增益没有影响

	-	由于各种误差的存在，样本不可能严格符合总体比例，某个
		取值总体数量较小时，误差会使得条件经验熵倾向于偏小
		（假设误差随机，可以大概证明）

-	相当于用**极大似然法**进行概率模型的选择

###	C4.5算法

C4.5算法类似于ID3算法，使用信息增益比代替信息增益用于选择
特征、判断是否需要继续生成子树

-	修正ID3倾向于使用取值较多的特征值分裂结点的问题

###	CHAID

*Chi-squared Automatic Interaction Detector*：卡方自动交叉
检验法

-	通过卡方检验统计量p值选择合适特征变量
	-	p值越小，说明使用该特征变量分类效果越好

-	在构建决策树有一定优势，是从统计显著性角度确定特征变量、
	分割数值，对决策树分支优化明显

> - 应该是检测和均匀分布的差距，原假设为均匀分步

####	步骤

-	将各个水平将观测事先分组，形成一些小块的子集
-	通过统计检验：卡方检验、F检验确定分割变量、合并子集，
	得到新的子节点
-	重复前两步直到无法继续分割

###	CART算法

*classification and regression tree*：分类与回归树（二叉树）

-	CART树是二叉树，左侧分支取“是”，右侧取“否”，等价于递归
	的二分每个特征，将输入空间（特征空间）划分为有限个单元

-	在每个单元上确定预测的概率分布，即给定输入、输出下的条件		概率分布

-	特点
	-	可以用于分类、回归
	-	对自变量、因变量不做任何形式的分布假定
	-	能较好的处理缺失值（协决策树，属性/变量组合除外）

####	回归树

回归树

-	模型

	$$
	f(x) = \sum_{m=1} \hat c_m I(x \in R_m)
	$$

	> - $R_m$：空间划分出的第m单元
	> - $\hat c_m=avg(y_i|x_i \in R_m)$：第m个单元上所有实例输出
		变量均值，此时平方误差最小

-	策略：平方误差最小化准则

#####	步骤

最小二乘回归树生成算法

> - 输入：训练数据集D
> - 输出：回归树$f(x)$

-	选择最优切变量j、切分点s，即求解
	$$
	\arg\min_{j,s} [\min_{c_1} \sum_{x_i \in R_1(j,s)}
		(y_i - c_1)^2 + \min_{c_2}
		\sum_{x_i \in R_2(j,s)} (y_i - c_2)^2
	]
	$$

	> - $R_1(j,s) = \{x|x^{(j)} \leq s\}$
	> - $R_2(j,s) = \{x|x^{(j)} \geq s\}$
	> - $c_m = avg(y_i|x_i \in R_m)$：使得区域$R_m$中平方
		误差最小，即其中样本点$y_i$均值
	> - 这里通过**遍历**得到
	
-	对两个子区域$R_1(j,s), R_2(j,s)$继续重复以上步骤，直至
	满足停止条件

-	将输入空间划分为M个区域$R_1, R_2, \cdots, R_M$，生成决策
	树
	$$
	f(x) = \sum_{m=1} \hat c_m I(x \in R_m)
	$$

####	分类树

#####	Gini指数

-	给定样本集合D的**基尼指数**为

	$$
	Gini(D) = 1 - \sum_{k=1}^K (\frac {|C_k|} {|D|})^2
	$$

	> - $C_k$：样本集D中属于第k类的子集
	> - $K$：类数目

-	在特征A是否取值a条件下，集合D的**条件基尼指数**

	$$
	Gini(D,A) = \frac {|D_1|} {|D|} Gini(D_1) +
		\frac {|D_2|} {|D|} Gini(D_2)
	$$

	> - $D_1 = \{(x,y) \in D| A(x) = a\}$
	> - $D_2 = D - D_1$

> - Gini指数参见*data_science/reference/model_evaluation*

#####	步骤

以最小化基尼指数作为每步目标

> - 输入：训练数据集D，停止计算条件
> - 输出：CART决策树

-	设节点训练数据集为D，计算现有特征对该数据集的基尼指数，
	对每个特征A，对其所有取值a计算条件基尼指数

	-	2个可能取值只有一个切分点

-	在所有可能特征A、所有可能的切分点a中，选择基尼指数最小
	的特征极其对应的切分点作为最优特征、最优切分点，将训练
	数据依特征分配到两个子结点中

-	对两个子节点递归分裂，直至满足停止条件

	-	样本个数小于阈值
	-	样本基尼指数小于阈值
	-	没有更多特征

-	生成CART决策树

###	QUEST

*Quick Unbiased Efficient Statical Tree*：二分类决策树算法

-	以不同策略处理特征变量、切分点（类似CHAID，选择p值最小、
	且显著者）
	-	定性特征：卡方检验
	-	数量特征：F检验
-	运行速度快于CART树

##	Pruning

树剪枝：在决策树的学习过程中，将已生成的树进行简化的过程

###	说明

####	剪枝意义

-	最小化RSS、最大化置信目标下，会导致庞大的树

	-	对训练数据拟合越好
	-	模型复杂度越高
	-	推广能力差
	-	比较难理解、解释

-	通过剪枝得到恰当的树，具备一定的预测精度、复杂程度恰当、
	代价（误差）和复杂度之间的权衡是必要的

####	剪枝策略

-	*pre-pruning*：限制决策树的充分生长

	-	事先指定决策树生长最大深度
	-	事先指定决策树叶结点最大值
	-	事先指定树节点样本量最小值
	-	异质性下降必须大于阈值

-	*post-pruning*：决策树生长完毕后，根据一定规则，剪去不
	具备普遍性的子树

	-	极小化损失复杂度剪枝

###	*Minimal Cost Complexity Pruning*

极小化损失复杂度剪枝

$$\begin{align*}
C_\alpha(T) & = C(T) + \alpha |T| \\
	& = \sum_{t=1}^{|T|} N_t H_t(T) + \alpha |T| \\
	& = -\sum_{t=1}^{|T|} \sum_{k=1}^K \frac {N_{t,k}} {N_t}
		log \frac {N_{t,k}} {N_t}  + \alpha|T| \\
H_t(T) & = -\sum_k (\frac {N_{t,k}} {N_t}
	log \frac {N_{t,k}} {N_t})
\end{align*}$$

> - 损失函数：正则化的极大似然函数
> - $N_t$：树T的第t个叶子节点中样本点数量
> - $N_{t,k}$：树T的第t个叶子节点第k类样本点数量
> - $H_t(T)$：树T的第t个叶子节点熵
> - $C(T)$：模型对训练数据的预测误差
> - $|T|$：模型复杂度，这里就是叶节点数量
> - $\alpha \geq 0$：控制模型复杂度对模型总损失影响，每个
	叶节点带来的复杂度

-	剪枝即在给定$\alpha$的情况下，选择损失函数最小的模型
-	决策树仅仅考虑通过提高信息增益（比）更好的拟合数据，而
	剪枝还考虑模型复杂度
-	**决策树生成局部模型，决策树剪枝学习整体模型**

####	步骤

> - 输入：生成算法产生的整个树T，参数$\alpha$
> - 输出：修剪后的子数$T_\alpha$

-	计算每个节点的经验熵
-	递归的从树的叶节点向上回缩
	-	若$C_\alpha(T_{before}) \geq C_\alpha(T_{after})$，
		则剪枝
	-	或不断回缩直到根节点，选取损失函数最小的子树
		$T_\alpha$

> - 算法只需要比较节点、节点子树之间损失函数之差即可，计算
	可以在局部进行
> - 算法可以由一种动态规划算法实现

###	CART剪枝

####	生成子树序列

从算法生成决策树$T_0$底端开始不断剪枝，直到根节点，形成
子树序列$\{T_0, T_1, \cdots, T_n\}$

-	对给定$\alpha$，存在使损失函数$C_\alpha(T)$最小的子树
	$T_\alpha$，且此最优子树唯一

	-	$\alpha$偏大时，最优子树$T_\alpha$偏小
	-	$\alpha=0$时完整树最优，
		$\alpha \rightarrow \infty$时单节点树最优

-	可以证明：可以用递归的方法对树进行剪枝，即以从0逐渐增大
	$\alpha_1, \alpha_2, \cdots, \alpha_N$进行剪枝，最优子树
	序列$T_1, T_2, \cdots, T_N$嵌套，二者一一对应
	
	-	$C_{\alpha}(t) = C(t) + \alpha$：以t为单节点树损失

	-	$C_{\alpha}(T_t) = C(T_t) + \alpha|T_t|$：以t为
		根节点子树$T_t$损失

	-	则$\alpha = \frac {C(t) - C(T_t)} {|T_t|-1}$时，t和
		$T_t$有相同的损失函数值

	-	对$T_0$中每个内部节点t，
		$g(t) = \frac {C(t) - C(T_t)} {|T_t|-1}$表示剪枝后
		整体损失函数值减少程度

####	选取最优子树

通过交叉验证法在独立的验证数据集上对子树进行测试，选择
最优子树

-	平方误差、基尼指数最小的决策树最优
-	子树序列$T_1, T_2, \cdots, T_N$和参数序列
	$\alpha_1, \alpha_2, \cdots, \alpha_n$一一对应，最优子树
	确定，则最优参数也确定

####	步骤

> - 输入：CART算法生成决策树$T_0$
> - 输出：最优决策树$T_\alpha$

-	置：$k=0, T=T_0, \alpha \rightarrow +\infty$

-	自下而上地计算各内部节点t对应$C(T_t), |T_t|, g(t)$，
	取$\alpha = min(\alpha, min(g(t)))$

-	**自上而下**访问内部节点，若有$g(t)=\alpha$，进行剪枝，
	并对的叶节点t以多数表决发决定其类，得到树T

-	置：$k+=1, \alpha_k = \alpha, T_k = T$

-	若T不是单节点树，则重复以上（从重新计算$g(t)$开始）

-	采用交叉验证法在子树序列中选取最优子树$T_\alpha$

> - 也可以从接近0的$\alpha$值开始，逐渐增加，剪枝得到一系列
	子树序列$T_1, T_2, \cdots, T_K$

