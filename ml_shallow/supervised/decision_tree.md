#	Deision Tree

##	概述

###	本质

决策树：本质上是从训练数据中归纳出一组分类规则

-	与训练数据不矛盾的分类规则（即能对训练数据正确分类）可能
	有多个、没有，需要找到矛盾较小、泛化能力较好的
-	决策树学习也是由训练数据集估计条件概率模型，需要寻找对
	训练数据有很好拟合、对未知数据有很好预测的模型

####	分类规则集合

决策树可以看作是*if-then*规则的集合，体现输入、输出变量
逻辑关系

-	决策树根节点到叶节点每条路径构成一条规则

-	路径上内部节点的特征对应规则的条件，叶节点对应规则结论

-	决策树的路径或其对应的*if-then*规则集合**互斥且完备**，
	即每个实例有且仅有一条路径覆盖

####	条件概率分布

决策树可以表示定义在特征空间、类空间上的条件概率分布

-	此条件概率分布定义在特征空间的一个划分上

	-	其中每个单元定义一个类的概率分布就构成一个条件概率
		分布
	-	决策树中一条路径（叶节点）对应划分中一个单元

-	条件概率分布由**各单元的给定条件下**，各类的条件概率分布
	组成

	-	$P(Y|X)$：$X$为表示特征的随机变量（取值各个单元），
		$Y$表示类的随机变量
	-	各叶节点上的条件概率往往偏向于某类，决策树分类时将
		属于该节点实例分为该类

###	结构

分析结论、展示方式类似一棵倒置的树

-	决策树由*node*、*directed edge*组成
	-	*interal node*：内部节点，表示特征、属性
	-	*leaf node*：一个类

-	对训练数据进行分类
	-	从根节点开始，对实例某特征进行测试，根据测试结果将
		实例分配到其子节点，对应该特征一个取值
	-	递归地对实例进行分配，直至到达叶子节点，将实例分到
		叶节点地类中

-	对新数据$X_0$的预测
	-	从决策树的树根到树叶搜索，确定数所的叶子节点
	-	利用叶子节点中训练数据集预测：众数类、均值

###	特点

-	优势
	-	能有效处理分类型输入变量
	-	能够实现非线性分割
	-	模型具有可读性，分类速度块

-	问题
	-	充分生长的决策有高方差，预测不稳定
	-	剪枝可以提高预测稳健性，但是预测精度可能会下降

###	分类

##	特征选择

选取对训练数据具有**分类能力**的特征，提高决策树学习效率

###	*Infomation Gain*特征选择

$$
g(D, A) = H(D) - H(D|A)
$$

-	经验熵$H(D)$：对数据集D进行分类的不确定性

-	经验条件熵$H(D|A)$：在特征A给定条件下，对数据集D进行分类
	的不确定性

-	信息增益：由于特征A而使得数据集D的分类的不确定性减少程度
	-	依赖于特征，不同特征有不同的信息增益
	-	信息增益大的特征具有更强的分类能力

-	*infomation gain ratio*：信息增益比

	$$\begin{align*}
	g_R(D,A) & = \frac {g(D,A)} {H_A(D)} \\
	H_A(D) & = -\sum_{m=1}^M \frac {|D_i|} {|D|}
		log_2 \frac {|D_i|} {|D|}
	\end{align*}$$

	> - $H_A(D)$：*split information*，类似于以A作为随机变量
		的熵
	> - $D_i$：指标A第i个取值样本集合

> - 熵参见*data_science/reference/model_evaluation*

####	步骤

对训练数据集（或子集）D，计算其每个特征的信息增益，选择其中
信息增益最大的特征

> - 输入：训练数据集D、特征A，其中训练集D包含K个类别，特征A
	有M种取值
> - 输出：特征A对训练数据集D的信息增益$g(D,A)$

-	计算数据集D经验熵
	$$
	H(D) = \sum_{k=1}^K \frac {|C_k|} {|D|}
		log_2 \frac {|C_k|} {|D|}
	$$

-	计算特征A对数据集D的条件经验熵
	$$\begin{align*}
	H(D|A) & = \sum_{m=1}^M \frac {|D_m|} {|D|} H(D_m) \\
	& = -\sum_{m=1}^M \frac {|D_m|} {|D|}
		\sum_{k=1}^K \frac {|D_{m,k}|} {|D_m|}
		log_2 \frac {|D_{m,k}|} {|D_m|}
	\end{align*}$$

-	计算信息增益$g(D,A)=H(D) - H(D|A)$

##	决策树生成

决策树的生成就是递归的构造决策树的过程

-	使用自顶向下贪心算法

	-	从所有可能决策树中选取最优决策树是NP完全问题

	-	所以实际决策树算法通常采用**启发式**算法，近似求解
		这一最优化问题，得到*sub-optimal*决策树

	-	从包含所有数据的根节点开始，递归的选择**当前**最优
		特征对训练数据进行分割，使得各子数据集有当前最好分类

	-	此样本不断分组过程对应特征空间的划分、决策树的构建

-	原则：使区域（节点）内观测输出变量的异质性下降最大，从而
	确定

	-	最佳分组变量$x_j$
	-	分组变量取值中的最佳分割点s

###	ID3算法

在决策树各个节点上应用信息增益准则选择特征，递归的构建决策树

####	步骤

> - 输入：训练数据集D，特征集A，阈值$\epsilon$
> - 输出：决策树T

-	若D中所有实例属于同一类$C_k$，则T为单节点树，并将类$C_k$
	作为该节点的类标记，返回T

-	若$A = \varnothing$，则T为单节点树，将D中实例数最大的类
	$C_k$作为该节点的类标记，返回T

-	否则，计算A各个特征对D的信息增益，选择信息增益最大的特征
	$A_g$

-	若$A_g$的信息增益小于阈值$\epsilon$，则置T为单节点数树，
	并将D中实例数最大的类$C_k$作为该节点的类标记，返回T

-	否则，对$A_g$**每个**可能值$a_m$，将D分割为若干非空子集
	$D_i$，将$D_i$中**实例数最大**的类作为标记，构建子节点，
	由节点极其子节点构成树T并返回

-	对第i个子节点，以$D_i$为训练集，以$A-{A_g}$为特征集，
	递归的构造子树$T_i$并返回

####	特点

-	每个特征只会被用于进行分割（建立子节点）一次，每次所有
	取值都会被用于建立子节点
	-	因此ID3树是多叉树，各个节点的分叉个数取决于使用特征

-	只有树的生成，生成树容易产生过拟合

-	以信息增益作为划分训练数据集的特征，倾向于选择取值较多
	的特征进行划分

	-	理论上特征取值严格遵循比例，取值多则其各个值对应样本
		数量较小，对信息增益没有影响

	-	由于各种误差的存在，样本不可能严格符合总体比例，某个
		取值总体数量较小时，误差会使得条件经验熵倾向于偏小
		（假设误差随机，可以大概证明）

-	相当于用**极大似然法**进行概率模型的选择

###	C4.5算法

C4.5算法类似于ID3算法，使用信息增益比代替信息增益用于选择
特征、判断是否需要继续生成子树

-	修正ID3倾向于使用取值较多的特征值分裂结点的问题

###	CHAID

*Chi-squared Automatic Interaction Detector*：卡方自动交叉
检验法

-	通过卡方检验统计量p值选择合适特征变量
	-	p值越小，说明使用该特征变量分类效果越好

-	在构建决策树有一定优势，是从统计显著性角度确定特征变量、
	分割数值，对决策树分支优化明显

> - 应该是检测和均匀分布的差距，原假设为均匀分步

####	步骤

-	将各个水平将观测事先分组，形成一些小块的子集
-	通过统计检验：卡方检验、F检验确定分割变量、合并子集，
	得到新的子节点
-	重复前两步直到无法继续分割

###	CART算法

*classification and regression tree*：分类与回归树（二叉树）

-	CART树是二叉树，左侧分支取“是”，右侧取“否”，等价于递归
	的二分每个特征，将输入空间（特征空间）划分为有限个单元

-	在每个单元上确定预测的概率分布，即给定输入、输出下的条件		概率分布

-	特点
	-	可以用于分类、回归
	-	对自变量、因变量不做任何形式的分布假定
	-	能较好的处理缺失值（协决策树，属性/变量组合除外）

####	回归树

回归树

-	模型

	$$
	f(x) = \sum_{m=1} \hat c_m I(x \in R_m)
	$$

	> - $R_m$：空间划分出的第m单元
	> - $\hat c_m=avg(y_i|x_i \in R_m)$：第m个单元上所有实例输出
		变量均值，此时平方误差最小

-	策略：平方误差最小化准则

#####	步骤

最小二乘回归树生成算法

> - 输入：训练数据集D
> - 输出：回归树$f(x)$

-	选择最优切变量j、切分点s，即求解
	$$
	\arg\min_{j,s} [\min_{c_1} \sum_{x_i \in R_1(j,s)}
		(y_i - c_1)^2 + \min_{c_2}
		\sum_{x_i \in R_2(j,s)} (y_i - c_2)^2
	]
	$$

	> - $R_1(j,s) = \{x|x^{(j)} \leq s\}$
	> - $R_2(j,s) = \{x|x^{(j)} \geq s\}$
	> - $c_m = avg(y_i|x_i \in R_m)$：使得区域$R_m$中平方
		误差最小，即其中样本点$y_i$均值
	> - 这里通过**遍历**得到
	
-	对两个子区域$R_1(j,s), R_2(j,s)$继续重复以上步骤，直至
	满足停止条件

-	将输入空间划分为M个区域$R_1, R_2, \cdots, R_M$，生成决策
	树
	$$
	f(x) = \sum_{m=1} \hat c_m I(x \in R_m)
	$$

####	分类树

#####	Gini指数

-	给定样本集合D的**基尼指数**为

	$$
	Gini(D) = 1 - \sum_{k=1}^K (\frac {|C_k|} {|D|})^2
	$$

	> - $C_k$：样本集D中属于第k类的子集
	> - $K$：类数目

-	在特征A是否取值a条件下，集合D的**条件基尼指数**

	$$
	Gini(D,A) = \frac {|D_1|} {|D|} Gini(D_1) +
		\frac {|D_2|} {|D|} Gini(D_2)
	$$

	> - $D_1 = \{(x,y) \in D| A(x) = a\}$
	> - $D_2 = D - D_1$

> - Gini指数参见*data_science/reference/model_evaluation*

#####	步骤

以最小化基尼指数作为每步目标

> - 输入：训练数据集D，停止计算条件
> - 输出：CART决策树

-	设节点训练数据集为D，计算现有特征对该数据集的基尼指数，
	对每个特征A，对其所有取值a计算条件基尼指数

	-	2个可能取值只有一个切分点

-	在所有可能特征A、所有可能的切分点a中，选择基尼指数最小
	的特征极其对应的切分点作为最优特征、最优切分点，将训练
	数据依特征分配到两个子结点中

-	对两个子节点递归分裂，直至满足停止条件

	-	样本个数小于阈值
	-	样本基尼指数小于阈值
	-	没有更多特征

-	生成CART决策树

###	QUEST

*Quick Unbiased Efficient Statical Tree*：二分类决策树算法

-	以不同策略处理特征变量、切分点（类似CHAID，选择p值最小、
	且显著者）
	-	定性特征：卡方检验
	-	数量特征：F检验
-	运行速度快于CART树

##	*Pruning*

树剪枝：在决策树的学习过程中，将已生成的树进行简化的过程

###	说明

####	剪枝意义

-	最小化RSS、最大化置信目标下，会导致庞大的树

	-	对训练数据拟合越好
	-	模型复杂度越高
	-	推广能力差
	-	比较难理解、解释

-	通过剪枝得到恰当的树，具备一定的预测精度、复杂程度恰当、
	代价（误差）和复杂度之间的权衡是必要的

####	剪枝策略

-	*pre-pruning*：限制决策树的充分生长

	-	事先指定决策树生长最大深度
	-	事先指定决策树叶结点最大值
	-	事先指定树节点样本量最小值
	-	异质性下降必须大于阈值

-	*post-pruning*：决策树生长完毕后，根据一定规则，剪去不
	具备普遍性的子树

	-	极小化损失复杂度剪枝

###	*Minimal Cost Complexity Pruning*

极小化损失复杂度剪枝

$$\begin{align*}
C_\alpha(T) & = C(T) + \alpha |T| \\
	& = \sum_{t=1}^{|T|} N_t H_t(T) + \alpha |T| \\
	& = -\sum_{t=1}^{|T|} \sum_{k=1}^K \frac {N_{t,k}} {N_t}
		log \frac {N_{t,k}} {N_t}  + \alpha|T| \\
H_t(T) & = -\sum_k (\frac {N_{t,k}} {N_t}
	log \frac {N_{t,k}} {N_t})
\end{align*}$$

> - 损失函数：正则化的极大似然函数
> - $N_t$：树T的第t个叶子节点中样本点数量
> - $N_{t,k}$：树T的第t个叶子节点第k类样本点数量
> - $H_t(T)$：树T的第t个叶子节点熵
> - $C(T)$：模型对训练数据的预测误差
> - $|T|$：模型复杂度，这里就是叶节点数量
> - $\alpha \geq 0$：控制模型复杂度对模型总损失影响，每个
	叶节点带来的复杂度

-	剪枝即在给定$\alpha$的情况下，选择损失函数最小的模型
-	决策树仅仅考虑通过提高信息增益（比）更好的拟合数据，而
	剪枝还考虑模型复杂度
-	**决策树生成局部模型，决策树剪枝学习整体模型**

####	步骤

> - 输入：生成算法产生的整个树T，参数$\alpha$
> - 输出：修剪后的子数$T_\alpha$

-	计算每个节点的经验熵
-	递归的从树的叶节点向上回缩
	-	若$C_\alpha(T_{before}) \geq C_\alpha(T_{after})$，
		则剪枝
	-	或不断回缩直到根节点，选取损失函数最小的子树
		$T_\alpha$

> - 算法只需要比较节点、节点子树之间损失函数之差即可，计算
	可以在局部进行
> - 算法可以由一种动态规划算法实现

###	CART剪枝

####	生成子树序列

从算法生成决策树$T_0$底端开始不断剪枝，直到根节点，形成
子树序列$\{T_0, T_1, \cdots, T_n\}$

-	对给定$\alpha$，存在使损失函数$C_\alpha(T)$最小的子树
	$T_\alpha$，且此最优子树唯一

	-	$\alpha$偏大时，最优子树$T_\alpha$偏小
	-	$\alpha=0$时完整树最优，
		$\alpha \rightarrow \infty$时单节点树最优

-	可以证明：可以用递归的方法对树进行剪枝，即以从0逐渐增大
	$\alpha_1, \alpha_2, \cdots, \alpha_N$进行剪枝，最优子树
	序列$T_1, T_2, \cdots, T_N$嵌套，二者一一对应
	
	-	$C_{\alpha}(t) = C(t) + \alpha$：以t为单节点树损失

	-	$C_{\alpha}(T_t) = C(T_t) + \alpha|T_t|$：以t为
		根节点子树$T_t$损失

	-	则$\alpha = \frac {C(t) - C(T_t)} {|T_t|-1}$时，t和
		$T_t$有相同的损失函数值

	-	对$T_0$中每个内部节点t，
		$g(t) = \frac {C(t) - C(T_t)} {|T_t|-1}$表示剪枝后
		整体损失函数值减少程度

####	选取最优子树

通过交叉验证法在独立的验证数据集上对子树进行测试，选择
最优子树

-	平方误差、基尼指数最小的决策树最优
-	子树序列$T_1, T_2, \cdots, T_N$和参数序列
	$\alpha_1, \alpha_2, \cdots, \alpha_n$一一对应，最优子树
	确定，则最优参数也确定

####	步骤

> - 输入：CART算法生成决策树$T_0$
> - 输出：最优决策树$T_\alpha$

-	置：$k=0, T=T_0, \alpha \rightarrow +\infty$

-	自下而上地计算各内部节点t对应$C(T_t), |T_t|, g(t)$，
	取$\alpha = min(\alpha, min(g(t)))$

-	**自上而下**访问内部节点，若有$g(t)=\alpha$，进行剪枝，
	并对的叶节点t以多数表决发决定其类，得到树T

-	置：$k+=1, \alpha_k = \alpha, T_k = T$

-	若T不是单节点树，则重复以上（从重新计算$g(t)$开始）

-	采用交叉验证法在子树序列中选取最优子树$T_\alpha$

> - 也可以从接近0的$\alpha$值开始，逐渐增加，剪枝得到一系列
	子树序列$T_1, T_2, \cdots, T_K$

##	组合预测模型

-	Bagging：参见*data_science/ensemble/bagging*
-	AdaBoost：参见*data_science/ensemble/boosting*

###	*Random Forest*

随机森林：随机建立多个有较高预测精度、弱相关（甚至不相关）
的决策树（基础学习器），多棵决策树共同对新观测做预测

-	RF是Bagging的扩展变体，在以决策树为基学习器构建Bagging
	集成模型的基础上，在训练过程中引入了**随机特征选择**

####	步骤

-	样本随机：Bootstrap自举样本

-	输入属性随机：对第i棵决策树通过随机方式选取K个输入变量
	构成候选变量子集$\Theta_I$

	-	Forest-Random Input：随机选择$k=log_2P+1或k=\sqrt P$
		个变量

	-	Forest-Random Combination
		-	随机选择L个输入变量x
		-	生成L个服从均匀分布的随机数$\alpha$
		-	做线性组合
			$v_j = \sum_{i=1}^L \alpha_i x_i, \alpha_i \in [-1, 1]$
		-	得到k个由新变量v组成的输入变量子集$\Theta_i$

-	在候选变量子集中选择最优变量构建决策树
	-	生成决策树时不需要剪枝

-	重复以上步骤构建k棵决策树，用一定集成策略组合多个决策树
	-	简单平均/随机森林投票

####	优点

-	样本抽样、属性抽样引入随机性
	-	基学习器估计误差较大，但是组合模型偏差被修正
	-	不容易发生过拟合、对随机波动稳健性较好
	-	一定程度上避免贪心算法带来的局部最优局限

-	数据兼容性
	-	能够方便处理高维数据，“不用做特征选择”
	-	能处理分类型、连续型数据

-	训练速度快、容易实现并行

-	其他
	-	可以得到变量重要性排序
	-	启发式操作
	-	优化操作

####	缺点

-	决策树数量过多时，训练需要资源多
-	模型解释能力差，有点黑盒模型

###	*Gradient Boosted Desicion Tree*

*GBDT*：梯度提升树，以回归树为基学习器的梯度提升方法

-	GBDT会累加所有树的结果，本质上是回归模型（毕竟梯度）
	-	所以一般使用CART回归树做基学习器
	-	当然可以实现分类效果

-	损失函数为平方损失（毕竟回归），则相应伪损失/残差

	$$
	r_{t,i} = y_i - f_{t-1}(x_i), i=1,2,\cdots,N
	$$

> - 具体步骤参考*data_science/ensemble/boosting*

####	优势

-	准确率、效率相较于RF有一定提升
-	能够灵活的处理多类型数据

####	劣势

-	Boosting算法，基学习器之间存在依赖，难以并行训练数据
	-	比较可行的并行方案是在每轮选取最优特征切分时，并行
		处理特征

###	*XGBoost Tree*

XGBoost Tree：以回归树为基学习器的XGBoost模型

####	损失函数

-	正则化项定义为

	$$
	\Omega(f) = \gamma T + \frac 1 2 \lambda
		\sum_{j=1}^T w_j^2
	$$

	> - $f$：**CART回归树**模型
	> - $\gamma$：模型复杂度惩罚系数
	> - $T$：树复杂度，一般使用叶子结点数量
	> - $\lambda$：**模型贡献惩罚系数**
	> - $w_j$：第j个叶子结点取值，预测得分
	
	-	此正则化项与$w_j^2$正相关，即不希望单个基学习器贡献
		过大，给之后树留下学习空间

-	其中第t棵树损失函数为

	$$\begin{align*}
	obj^{(t)} & = \sum_{i=1}^N l(y_i, \hat y_i^{(t)}) +
		\Omega(f_t) \\

	& \approx \sum_{i=1}^N [l(y_i, \hat y^{(t-1)}) + g_i
		f_t(x_i) + \frac 1 2 h_i f_t^2(x_i)] + \gamma T_t
		+ \frac 1 2 \lambda \sum_{j=1}^T {w_j^{(t)}}^2 \\

	& = \sum_{j=1}^T [(\sum_{i \in I_j} g_i) w_j +
		\frac 1 2 (\sum_{i \in I_j} h_i + \lambda)
		{w_j^{(t)}}^2] + \gamma T_t + \sum_{i=1}^N
		l(y_i, \hat y^{(t)}) \\

	& = \sum_{j=1}^T [G_i w_j + \frac 1 2
		(H_j + \lambda){w_j^{(t)}}^2] + \gamma T_t +
		\sum_{i=1}^N l(y_i, \hat y^{(t)}) \\

	& = \sum_{j=1}^T [G_i w_j + \frac 1 2
		(H_j + \lambda)w_j^2] + \gamma T_t +
		\sum_{i=1}^N l(y_i, \hat y^{(t)}) \\

	\end{align*}$$

	> - $f_t$：第t棵回归树
	> - $f_t(x_i)$：第t棵回归树对样本$x_i$的预测得分
	> - $w_j^{(t)} = f_t(x)$：第t棵树中第j叶子节点预测得分
	> - $g_i = \partial_{\hat y} l(y_i, \hat y^{t-1})$
	> - $h_i = \partial^2_{\hat y} l(y_i, \hat y^{t-1})$
	> - $I_j$：第j个叶结点集合
	> - $G_j = \sum_{i \in I_j} g_i$
	> - $H_j = \sum_{i \in I_j} h_i$
	> - 之前推导参见*machine_learning/ensemble/boosting*

	-	对回归树，正则项中含有$(w_j^{(t)})^2$作为惩罚，能够
		和损失函数二阶导合并，不影响计算

	-	模型复杂度惩罚项惩罚项是针对树的，定义在叶子节点上，
		而平方损失是定义在样本上，合并时将其改写

-	第t棵树的整体损失等于**其各叶子结点损失加和**，且
	各叶子结点取值之间独立

	-	则第t棵树各叶子结点最优取值如下（损失最小）

		$$
		w_j^{(*)} = -\frac {\sum_{i \in I_j} g_i}
			{\sum_{i \in I_j} h_i + \lambda}
		$$

	-	整棵树结构分数（最小损失）带入即可得

		$$
		obj^{(t)} = -\frac 1 2 \sum_{j=i}^M \frac {G_j^2}
			{H_j + \lambda} + \lambda T
		$$

	-	则在结点分裂为新节点时，树损失变化量为

		$$
		l_{split} = \frac 1 2 \left [
		\frac {(\sum_{i \in I_L} g_i)^2} {\sum_{i \in I_L h_i}
			+ \lambda} +
		\frac {(\sum_{i \in I_R} g_i)^2} {\sum_{i \in I_R h_i}
			+ \lambda} -
		\frac {(\sum_{i \in I} g_i)^2} {\sum_{i \in I h_i} +
			\lambda}
		\right ] - \gamma
		$$

		> - $I_L, I_R$：结点分裂出的左、右结点

####	实现细节

-	列/属性抽样：借鉴随机森林，防止过拟合，减少计算量

-	稀疏值/缺失值处理：为缺失值或者指定的值指定分支的默认
	分裂方向，提升算法的效率

-	划分选取：根据百分位法列举几个可能成为分割点的候选者，
	从中计算找出最佳的分割点

-	Cache：特征列排序后以块的形式存储在内存中

	-	在迭代中可以重复使用

	-	但是当以行计算梯度数据时会导致内存的不连续访问，严重
		时会导致cache miss，降低算法效率

-	并行：boosting算法迭代必须串行，但在处理每个特征列时可以
	做到并行

-	数据量大：考虑了当数据量比较大，内存不够时有效的使用磁盘

	-	主要是结合多线程、数据压缩、分片的方法，尽可能的提高
		算法的效率


