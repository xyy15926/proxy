---
title: 文本预处理
tags:
  - 机器学习
  - NLP
categories:
  - 机器学习
  - NLP
date: 2019-07-14 20:58:18
updated: 2019-07-13 12:03:12
toc: true
mathjax: true
comments: true
description: 文本预处理
---

##	综述

-	去除噪声文档、文档中垃圾数据
-	停用词去除
-	词根还原（英文）
-	分词（中文）
-	词性标注
-	短语识别
-	词频统计

##	汉语分词

分词：添加合适的显性词语边界标志，使所形成的词串反映句子本意

-	分词是正确处理中文信息的基础
	-	文本基于单字
	-	书面表达方式以汉字作为最小单位
	-	词之间没有显性界限标志

-	用单个汉字作特征，不考虑词语含义，直接利用汉字在文本中
	出现的**统计特性**对文本进行划分

	-	直观明了
	-	操作简单
	-	对西语文本划分非常容易（使用空格划分）

-	使用词作为特征
	-	词是中文语义的最小信息单位，可以更好的反映句子中信息
	-	分析难度更高，中文文本中词之间没有分隔标记，正确分词
		是关键

###	分词方法

-	基于词典
	-	*FMM*：正向最大匹配分词
	-	*BMM*：逆向最大匹配分词
	-	BM法：双向扫描法
	-	逐词遍历

-	基于统计模型
	-	N-最短路径
	-	HMM
	-	N元语法
	-	由字构词的汉语分词方法

###	分词难点

####	歧义切分

-	分词规范
	-	分词单位
		-	二字、三字以及结合紧密、使用稳定的
		-	四字成语
		-	四字词或结合紧密、使用稳定的四字词组
	-	五字、五字以上谚语、格言等，分开后如不违背原有组合
		意义，应切分

-	歧义切分
	-	交集型切分歧义
	-	组合型切分歧义

####	未登录词识别

> - 词表词：记录在词表中的词
> - 未登录词：词表中没有的词、或已有训练语料中未曾出现词
	（此时也称为*out of vocabulary*）

-	真实文本切分中，未登录词总数大约9成是专有名词，其余为
	新词

-	未登录词对分词精度影响是歧义词的10倍

-	命名实体识别：实体名词、专业名词
	-	界定规则不存在太大分歧、构成形式有一定规律
	-	在文本中只占8.7%，引起分词错误率59.2%

####	词性标注

词性标注：在给定句子中判定每个词的语法范畴，确定词性并加以
标注的过程

-	*POS*作为特征可以更好的识别词语之间关系

	-	词性标注计数为*phrase chunking*词组组块的界定、
		*entities and relationship*实体与关系的识别打下良好
		基础，有利于深入探索文本语义信息

	-	词组的形式提高了特征向量的语义含量，使得向量更稀疏

-	难点

	-	汉语缺乏词形态变化
	-	常用词兼类现象严重：占11%
	-	研究者主观原因：不同语料库有不同规定、划分方法

> - *part of speech*：*POS*，词性

###	*Forward Maximum Matching Method*

*FMM*：正向最大匹配分词

-	步骤
	-	记词典中最长此表包含汉字数量为M
	-	从材料中选取前$m = M$个汉字去作为匹配字段，查找分词
		词典
		-	若存在匹配词，则将其切分出
		-	否则$m = m - 1$，重复
	-	重复直至材料分词完毕

-	特点
	-	对交叉歧义、组合歧义没有解决办法
	-	错误切分率为$\frac 1 {169}$

###	*Backward Maximum Matching Method*

*BMM*：逆向最大匹配分词

-	步骤：类似FMM，仅从材料/句子末尾开始处理

-	特点
	-	错误切分率$\frac 1 {245}$，较FMM更有效

###	*Bi-direction Matching Method*

BM法：双向扫描法

-	步骤：比较FMM、BMM法切分结果，决定正确切分

-	特点
	-	可以识别分词中交叉语义

###	N-最短路径

-	思想

	-	考虑待切分字串$S=c_1 c_2 \cdots c_n$，其中$c_i$为
		单个字、$n$为串长

	-	建立节点数为$n+1$的切分有向无环图，各节点编号为
		$V_0, V_1, \cdots, V_n$

		-	相邻节点间存在边
		-	若$w=c_i c_{i+1} \cdots c_j$是一个词，则节点
			$v_{i-1}, v_j$直接存在边
		-	所有边距离均为1

	-	求有图无环图中最短路径

####	特点

-	算法时间复杂度为$O(n*N*K)$

	> - $n$：字串长度
	> - $N$：最短路径数目
	> - $k$：某个字作为词末端字的平均次数

####	改进--考虑噪声

基于统计信息的粗分模型

-	考虑词串$W$经过信道传输，由于噪声干扰丢失词界切分标志，
	到输出端为字串$C$

-	N-最短路径词语粗分模型可以改进为：求N个候选切分$W$，使得
	概率$P(W|C)$为前N个最大值

	$$
	P(W|C) = \frac {P(W) P(C|W)} {P(C)}
	$$

	> - $P(C)$：字串概率，常数
	> - $P(C|W)$：仅有

-	采用一元统计模型，设$W=w_1w_2\cdots W_m$是字串
	$S=c_1c_2\cdots c_n$的切分结果，则其切分概率为

	$$\begin{align*}
	P(W) & = \prod_{i=1}^m P(w_i) \\
	P^{*}(W) = -ln P(w) = \sum_{i=1}^m (-ln P(W_i))
	\end{align*}$$

	> - $P(w_i)$：词$w_i$出现概率，在大规模预料训练的基础上
		通过极大似然方法得到

-	则$-lnP(w_i)$可看作是词$w_i$在切分有向无环图中对应距离，
	改进N-最短路径方法

###	由字构词

####	假设、背景

> - 思想：将分词过程看作字分类问题，认为每个字在构造特定词语
	时，**占据确定的位置**

-	中文词一般不超过4个字，字位数量很小
	-	首部B
	-	词中M
	-	词尾E
	-	单独成词S
-	部分汉字按一定方式分布，有规律
-	利用相对固定的字推断相对不定的字的位置问题
-	虽然无法将所有词列入词典，但字基本稳定

####	步骤

-	对所有字根据预定义的特征进行**词位特征学习**，获得概率
	模型
-	在带待分字串上根据字与字之间的结合紧密程度得到词位的分类
	结果
-	根据词位定义直接获得最终分词结果

####	*Productivity*

能产度：词$c_i$在词位$t_j$的能产度定义为

$$
P_{c_i}(t_j) = \frac {count(c_i, t_j)}
	\sum_{t_j \in T} count(c_i, t_j)
$$

> - $T = {B, B_2, B_3, M, E, S}$

-	主词位：给定字在其上能产度高于0.5的词位

	|标记|B|B2|B3|M|E|S|总字量|
	|-----|-----|-----|-----|-----|-----|-----|-----|
	|字量|1634|156|27|33|1438|632|3920|
	|百分比|31.74|3.03|0.52|0.64|27.94|12.28|76.16|

	> - MSRA2005语料库中有主词位的字量分布

-	自由字：没有主词位的字
	-	自由字是基于词位分类的分词操作得以有效进行的的基础
		之一

> - 字：不仅限于汉字，包括标点、外文字母、注音符号、数字等
	任何可能文字符号

####	优势

-	能平衡词表词、未登录词
-	简化分词系统设计
	-	无需强调词表词信息
	-	无需设置特定未登录词识别模块

###	分词评价指标

-	正确率
-	召回率
-	F-测度值

##	*Vector Space Model*

向量空间模型：自然语言处理常用模型

> - *document*：文档，句子、段落、整篇文章
> - *term/feature*：词根、词、短语、其他
> - *weight*：项的权重，每个特征项在文档中重要程度

###	相似度比较

-	内积

	$$
	sim(D_1, D_2) = \sum_{k=1}^n w_{1,k} w_{2,k}
	$$

-	Cosine相似度

	$$
	cos(D_1, D_2) = cos \theta = \frac
		{\sum_{k=1}^n w_{1,k} w_{2,k}}
		{\sqrt{\sum_{k=1}^n w_{1,k}^2 \sum_{k=1}^n w_{2,k}^2}}
	$$

###	权重

-	布尔权重：$bw_{t,d} = \{0, 1\}$
-	*TF*：绝对词频，$TF_{t,d} = \frac {n_{t,d}} {n_d}$
-	*IDF*：倒排文档频度，$IDF_{t,d} = log \frac M {m_t}$
-	*TF-IDF*：$TF-IDF_{t,d} = TF_{t,d} * IDF_{t,d}$
-	*TF-IWF*：$TF_IWF_{t,d}= TF_{t,d} log \frac {\sum_{t=1}^T \sum_{d=1}^N n_{t,d}} {\sum_{t=1} n_{t,d}}$

> - $t_{t,d}$：文档$d$中出现特征$t$的次数
> - $t_d$：文档$d$中出现总词数
> - $m_t$：训练集中出现特征$t$文档数
> - $M$：训练集中文档总数
> - $K$：特征总数量

####	特征加权

-	特征加权主要包括三个部分（层次）
	-	局部加权：使用词语在文档中的统计量
	-	全局加权：词语在整个数据集中的统计量
	-	标准化

-	一般化特征加权表达式

	$$
	L_d(w) G(w) N_d
	$$

	> - $L_d(w)$：词$w$在文档$d$中的局部权重
	> - $G(w)$：词$w$在文档集合中的全局权重
	> - $N_d$：文档d的标准化因子

###	*Document Frequency*

*DF*：文档频率，文本数据中包含某词条的文档数目

-	通过文档频率进行特征选择：按文档频率大小对词条进行排序

	-	将DF小于某阈值的词删除
		-	稀有词项全局影响力不大
		-	文档若有稀有词向，通常也会有常见词项

		> - 和通常信息获取观念抵触：稀有更有代表性

	-	将DF大于某阈值的词删除
		-	太频繁词词项没有区分度

-	容易实现、可扩展性好

###	其他指标

-	信息增益/互信息

-	卡方统计量

##	*Latent Semantic Analysis*

*LSA*：潜在语义分析

-	文本分析中常用的降维技术
	-	特征重构方法
	-	很好解决了同义词、一词多义等现象给文本分析造成的困难

-	理论依据、假设
	-	认为有潜在语义结构隐含在文档中词语的上下文使用模式中
	-	而文档词频共现矩阵在一定程度可以反映词和不同主题之间
		关系

-	以文档词频矩阵为基础进行分析
	-	得到向量空间模型中文档、词的高维表示
	-	并通过投影形成文档、词在潜在语义空间中的相对稠密的
		低维表示，缩小问题规模
	-	通过这种低维表示解释出“文档-语义-词语”之间的联系

-	数学描述
	-	LSA将每个文本视为以词语/特征为维度的空间的点，包含
		语义的文本出现在空间中分布服从某种语义结构
	-	LSA将每个词视为以文档为维度的空间中点
	-	文档由词语构成，词语需要放在文档中理解，体现词语和
		文档之间的双重概率关系

###	应用SVD分解

-	词频共现矩阵$X=(x_{d,t})$：文档、词语的共现频率矩阵
	-	其中每行代表文档向量
	-	每列代表词语向量
	-	元素$x_{d,t}$表示文档$d$中词$t$出现的频率

-	对词频共现矩阵$X$进行SVD分解得到$X=U\SigmaV^T$

-	仅保留$\Sigma$中满足阈值要求的较大的前$r$特征值，
	其余置为0，得到
	$\tilde X = \tilde U \tilde \Sigma \tilde V^T$，达到信息
	过滤、去除噪声的目的

	-	$A = \tilde X$：矩阵特征分解后的文档词频矩阵近似
	-	$T = \tilde U$：文档和潜在语义的关系矩阵近似
	-	$S = \tilde V$：词语和潜在语义的关系矩阵近似
	-	$D = \tilde \Sigma$：各潜在语义的重要程度

####	说明

-	从数据压缩角度：近似矩阵是秩为$K$的前提下，矩阵$X$的最小
	二乘意义下最佳近似

-	r值过大会增加运算量，一般选择K使得贡献率满足

	$$
	\sum_{i=1}^r d_i / \sum_{i=1}^K d_i \geq \theta
	$$

	> - $\theta$：阈值
	> - $K$：原始词频共现矩阵秩

-	LSA缺点
	-	SVD的向量元素有正、有负，性质难以解释
	-	SVD的实际意义不够明确，难以控制词义据类的效果
	-	涉及高维矩阵运算

###	相似关系计算

-	潜在语义空间中存在：词-词、文本-文本、词-文本3种关系，
	可以通过近似矩阵$T, S, D$计算

-	比较词汇两两相似度：“正向乘法”

	$$ A A^T = T S D^T D S^T T^T = T S^2 T^T $$

-	比较文本两两相似度：“逆向乘法”

	$$ A^T A = T^T S^T D D^T S T = T^T S^2 T $$

-	词汇、文本两两相似度：就是原始矩阵$X$的近似矩阵本身$A$

	$$ A = T * S * D^T $$

