---
title: CTR Stacking Models
tags:
  - æœºå™¨å­¦ä¹ 
  - æ¨èç³»ç»Ÿ
categories:
  - æœºå™¨å­¦ä¹ 
  - æ¨èç³»ç»Ÿ
date: 2019-07-29 21:16:01
updated: 2019-07-29 21:16:01
toc: true
mathjax: true
comments: true
description: CTR Stacking Models
---

##	æ·±åº¦å­¦ä¹ CTR

![stacking_nn_models_envolution_network](imgs/stacking_nn_models_envolution_network.png)

##	Deep Crossing

*Deep Crossing*ï¼šæ·±åº¦å­¦ä¹ CTRæ¨¡å‹æœ€å…¸å‹ã€åŸºç¡€æ€§æ¨¡å‹

![deep_crossing_structure](imgs/deep_crossing_structure.png)

> - *multiple residual units*ï¼šæ®‹å·®ç½‘ç»œ

##	Factorization Machine based Neural Network

*FNN*ï¼šä½¿ç”¨FMéšå±‚ä½œä¸ºembeddingå‘é‡ï¼Œé¿å…å®Œå…¨ä»éšæœºçŠ¶æ€è®­ç»ƒ
embedding

![fnn_structure](imgs/fnn_structure.png)

-	è¾“å…¥ç‰¹å¾ä¸ºé«˜ç»´ç¨€ç–ç‰¹å¾ï¼Œembeddingdå±‚ä¸è¾“å…¥å±‚è¿æ¥æ•°é‡å¤§ã€
	è®­ç»ƒæ•ˆç‡ä½ã€ä¸ç¨³å®š

-	æå‰è®­ç»ƒembeddingæé«˜æ¨¡å‹å¤æ‚åº¦ã€ä¸ç¨³å®šæ€§

##	Product-based Neural Network

*PNN*ï¼šåœ¨embeddingå±‚ã€å…¨è¿æ¥å±‚é—´åŠ å…¥*product layer*ï¼Œå®Œæˆ
é’ˆå¯¹æ€§ç‰¹å¾äº¤å‰

![pnn_structure](imgs/pnn_structure.png)

> - *product layer*ï¼šåœ¨ä¸åŒç‰¹å¾åŸŸé—´è¿›è¡Œç‰¹å¾ç»„åˆï¼Œå®šä¹‰æœ‰
	innerã€outer productä»¥æ•æ‰ä¸åŒçš„äº¤å‰ä¿¡æ¯ï¼Œæé«˜è¡¨ç¤ºèƒ½åŠ›

> - ä¼ ç»ŸDNNä¸­é€šè¿‡å¤šå±‚å…¨è¿æ¥å±‚å®Œæˆç‰¹å¾äº¤å‰ç»„åˆï¼Œç¼ºä¹é’ˆå¯¹æ€§
> > -	æ²¡æœ‰é’ˆå¯¹ä¸åŒç‰¹å¾åŸŸè¿›è¡Œäº¤å‰
> > -	ä¸æ˜¯ç›´æ¥é’ˆå¯¹äº¤å‰ç‰¹å¾è®¾è®¡

##	Wide&Deep

*Wide&Deep*ï¼šç»“åˆæ·±å±‚ç½‘ç»œã€å¹¿åº¦ç½‘ç»œå¹³è¡¡è®°å¿†ã€æ³›åŒ–

![wide_and_deep_structure](imgs/wide_and_deep_structure.png)

> - *deep models*ï¼šåŸºäºç¨ å¯†embeddingå‰é¦ˆç¥ç»ç½‘ç»œ
> - *wide models*ï¼šåŸºäºç¨€ç–ç‰¹å¾ã€ç‰¹å¾è½¬æ¢çš„çº¿æ€§æ¨¡å‹

-	åŸºäºè®°å¿†çš„æ¨èé€šå¸¸å’Œç”¨æˆ·å·²ç»æ‰§è¡Œç›´æ¥ç›¸å…³ï¼›åŸºäºæ³›åŒ–çš„æ¨è
	æ›´æœ‰å¯èƒ½æä¾›å¤šæ ·æ€§çš„æ¨è

> - *memorization*ï¼šè®°å¿†ï¼Œå­¦ä¹ é¢‘ç¹å‡ºç°çš„ç‰©å“ã€ç‰¹å¾ï¼Œä»å†å²
	æ•°æ®ä¸­æ¢ç´¢ç›¸å…³æ€§
> - *generalization*ï¼šæ³›åŒ–ï¼ŒåŸºäºç›¸å…³æ€§çš„transitivityï¼Œæ¢ç´¢
	è¾ƒå°‘å‡ºç°çš„æ–°ç‰¹å¾ç»„åˆ

> - <https://arxiv.org/pdf/1606.07792.pdf>
> - wide&deepç³»æ¨¡å‹åº”è¯¥éƒ½å±äºstackingé›†æˆ

###	é€»è¾‘å›å½’ç¤ºä¾‹

![wide_and_deep_logit_structure](imgs/wide_and_deep_logit_structure.png)

$$
P(Y=1|x) = \sigma(w_{wide}^T[x, \phi(x)] + w_{deep}^T
	\alpha^{l_f} + b)
$$

##	DeepFM

*DeepFM*ï¼šç”¨FMæ›¿ä»£*wide&deep*ä¸­wideéƒ¨åˆ†ï¼Œæå‡å…¶è¡¨è¾¾èƒ½åŠ›

![deepfm_structure](imgs/deepfm_structure.png)

> - *Dense Embeddings*ï¼šFMä¸­å„ç‰¹å¾éšå‘é‡ï¼ŒFMã€DNNå…¬ç”¨
> - *FM Layer*ï¼šFMå…§ç§¯ã€æ±‚å’Œå±‚

-	ç›¸å½“äºåŒæ—¶ç»„åˆwideã€äºŒé˜¶äº¤å‰ã€deepä¸‰éƒ¨åˆ†ç»“æ„ï¼Œå¢å¼ºæ¨¡å‹
	è¡¨è¾¾èƒ½åŠ›

##	Deep&Cross

*Deep&Cross*ï¼šç”¨crossç½‘ç»œæ›¿ä»£*wide&deep*ä¸­wideéƒ¨åˆ†ï¼Œæå‡å…¶
è¡¨è¾¾èƒ½åŠ›

![deep_and_cross_structure](imgs/deep_and_cross_structure.png)

##	Nueral Factorization Machine

*NFM*ï¼šç”¨å¸¦äºŒé˜¶äº¤äº’æ± åŒ–å±‚çš„DNNæ›¿æ¢FMä¸­äºŒé˜¶äº¤å‰é¡¹ï¼Œæå‡FMçš„
éçº¿æ€§è¡¨è¾¾èƒ½åŠ›

$$\begin{align*}
\hat y_{NFM}(x) & = w_0 + \sum_{i=1}^m w_i x_i + f_{DNN}(x) \\
& = w_0 + \sum_{i=1}^m + h^T f_{\sigma}(f_{BI}(\varepsilon_x))
\end{align*}$$

> - $f_{DNN}(x)$ï¼šå¤šå±‚å‰é¦ˆç¥ç»ç½‘ç»œï¼ŒåŒ…æ‹¬*Embedding Layer*ã€
	*Bi-Interaction Layer*ã€*Hidden Layer*ã€
	*Prediciton Layer*
> - $h^T$ï¼šDNNè¾“å‡ºå±‚æƒé‡

###	æ¨¡å‹ç»“æ„

![nfm_structure](imgs/nfm_structure.png)

####	*Embedding Layer*

å…¨è¿æ¥ç½‘ç»œï¼šå°†æ¯ä¸ªç‰¹å¾æ˜ å°„ä¸ºç¨ å¯†å‘é‡è¡¨ç¤º

$$
\varepsilon_x = \{x_1v_1, x_2v_2, \cdots, x_mv_m\}
$$

> - $v_i$ï¼š$k$ç»´embeddingå‘é‡

-	åªéœ€è¦è€ƒè™‘é0ç‰¹å¾ï¼Œå¾—åˆ°ä¸€ç»„ç‰¹å¾å‘é‡
-	ç‰¹å¾å‘é‡ä¼šä¹˜ä»¥ç‰¹å¾å€¼ä»¥åæ˜ çœŸå®å€¼ç‰¹å¾
	ï¼ˆä¸€èˆ¬embeddingç‰¹å¾å–0/1ï¼Œç­‰ä»·äºæŸ¥è¡¨ï¼‰

####	*Bi-Interaction Layer*

BIå±‚ï¼šå°†ä¸€ç»„embeddingå‘é‡è½¬æ¢ä¸ºå•ä¸ªå‘é‡

$$\begin{align*}
f_(BI)(\varepsilon_x) & = \sum_{i=1} \sum_{j=i+1}
	x_i v_i \odot x_j v_j \\
& = \frac 1 2 (\|\sum_{i=1}^m x_i v_i\|_2^2 -
	\sum_{i=1}^m \|x_i v_i\|_2^2)
\end{align*}$$

> - $\odot$ï¼šé€å…ƒç´ ä¹˜ç§¯

-	æ²¡æœ‰å¼•å…¥é¢å¤–å‚æ•°ï¼Œå¯åœ¨çº¿æ€§æ—¶é—´$\in O(kM_x)$å†…è®¡ç®—
-	å¯ä»¥æ•è·åœ¨ä½å±‚æ¬¡äºŒé˜¶äº¤äº’å½±å“ï¼Œè¾ƒæ‹¼æ¥æ“ä½œæ›´
	informativeï¼Œæ–¹ä¾¿å­¦ä¹ æ›´é«˜é˜¶ç‰¹å¾äº¤äº’

> - å°†BIå±‚æ›¿æ¢ä¸ºæ‹¼æ¥ã€åŒæ—¶æ›¿æ¢éšå±‚ä¸ºå¡”å‹MLPï¼ˆæ®‹å·®ç½‘ç»œï¼‰
	åˆ™å¯ä»¥å¾—åˆ°*wide&deep*ã€*DeepCross*
> - æ‹¼æ¥æ“ä½œä¸æ¶‰åŠç‰¹å¾é—´äº¤äº’å½±å“ï¼Œéƒ½äº¤ç”±åç»­æ·±åº¦ç½‘ç»œå­¦ä¹ 
	ï¼Œå®é™…æ“ä½œä¸­æ¯”è¾ƒéš¾è®­ç»ƒ

####	*Hidden Layer* 

éšå±‚ï¼šæ™®é€šå¤šå±‚åµŒå¥—æƒé‡ã€æ¿€æ´»å‡½æ•°

$$
f_{\sigma} = \sigma_l(\beta_l (\cdot 
	\sigma_1(\beta_l f_{BI}(\varepsilon_X) + b_1)) + b_l)
$$

> - $l=0$æ²¡æœ‰éšå±‚æ—¶ï¼Œ$f_{\sigma}$åŸæ ·è¾“å‡ºï¼Œå–$h^T$ä¸º
	å…¨1å‘é‡ï¼Œå³å¯å¾—FMæ¨¡å‹

##	Attentional Factorization Machines

*AFM*ï¼šå¼•å…¥Attentionç½‘ç»œæ›¿æ¢FMä¸­äºŒé˜¶äº¤äº’é¡¹ï¼Œå­¦ä¹ äº¤äº’ç‰¹å¾çš„
é‡è¦æ€§ï¼Œå‰”é™¤æ— æ•ˆçš„ç‰¹å¾ç»„åˆï¼ˆäº¤äº’é¡¹ï¼‰

$$\begin{align*}
\hat y_{AFM} & = w_0 + \sum_{i=1}^m w_i x_i +
	f_{AFM}(\varepsilon) \\
& = w_0 + \sum_{i=1}^m w_i x_i + p^T \sum_{i=1}^m \sum_{j=i+1}^m
	a_{i,j} (v_i \odot v_j) x_i x_j
\end{align*}$$

> - $\varepsilon$ï¼šéšå‘é‡é›†ï¼ŒåŒä¸Š
> - $p^T$ï¼šAttentionç½‘ç»œè¾“å‡ºæƒé‡

###	æ¨¡å‹ç»“æ„

![afm_structure](imgs/afm_structure.png)

####	*Pair-Wise Interaction Layer*

æˆå¯¹äº¤äº’å±‚ï¼šå°†mä¸ªembeddingå‘é‡æ‰©å……ä¸º$m(m-1)/2$ä¸ªäº¤äº’å‘é‡

$$
f_{PI}(\varepsilon) = \{(v_i \odot v_j) x_i x_j\}_{(i,j) \in R_X}
$$

> - $R_X = \{(i,j) | i \in X, j \in X, j > i \}$
> - $v_i$ï¼š$k$ç»´embeddingå‘é‡

####	*Attention-based Pooling*

æ³¨æ„åŠ›æ± åŒ–å±‚ï¼šå‹ç¼©äº¤äº’ä½œç”¨ä¸ºå•ä¸€è¡¨ç¤ºæ—¶ï¼Œç»™äº¤äº’ä½œç”¨èµ‹ä¸åŒæƒé‡

$$\begin{align*}
f_{Att}(f_{PI}(\varepsilon)) = \sum_{(i,j) \in R_X}
	a_{i,j} (v_i \odot v_j) x_i x_j
\end{align*}$$

> - $a_{i,j}$ï¼šäº¤äº’æƒé‡$w_{i,j}$çš„æ³¨æ„åŠ›å¾—åˆ†
> - $\odot$ï¼šé€å…ƒç´ ä¹˜ç§¯

-	è€ƒè™‘åˆ°ç‰¹å¾é«˜ç»´ç¨€ç–ï¼Œæ³¨æ„åŠ›å¾—åˆ†ä¸èƒ½ç›´æ¥è®­ç»ƒï¼Œä½¿ç”¨MLP
	*attention network*å‚æ•°åŒ–æ³¨æ„åŠ›å¾—åˆ†

	$$\begin{align*}
	a_{i,j}^{'} & = h^T ReLU(W((v_i \odot v_j) x_i x_j) + b) \\
	a_{i,j} & = \frac {exp(a_{i,j}^{'})}
		{\sum_{(i,j) \in R_X} exp(a_{i,j}^{'})}
	\end{align*}$$

	> - $W \in R^{t*k}, b \in R^t, h \in R^T$ï¼šæ¨¡å‹å‚æ•°
	> - $t$ï¼šattention networkéšå±‚å¤§å°

##	Deep Interest Network

*DIN*ï¼šèåˆAttentionæœºåˆ¶ä½œç”¨äºDNN

###	æ¨¡å‹ç»“æ„

![din_stucture](imgs/din_structure.png)

####	*activation unit*

æ¿€æ´»å•å…ƒ

$$\begin{align*}
v_U(A) & = f_{au}(v_A, e_1, e_2, \cdots, e_H) \\
& = \sum_{j=1}^H a(e_j, v_A) e_j \\
& = \sum_{j=1}^H w_j e_j
\end{align*}$$

> - ç›¸è¾ƒäºä¸Šä¸ªç»“æ„ä»…å¤šäº†ç›´æ¥æ‹¼æ¥çš„ç”¨æˆ·ã€ä¸Šä¸‹æ–‡ç‰¹å¾
	![din_stucture_comparision](imgs/din_structure_comparision.png)

###	æ¨¡å‹è®­ç»ƒ

####	Mini-batch Aware Regularization

> - ä»¥Batchå†…å‚æ•°å¹³å‡è¿‘ä¼¼$L_2$çº¦æŸ

$$\begin{align*}
L_2(W) & = \sum_{i=1}^M \sum_{j=1}^B \sum_{(x,y) \in B_j}
	\frac {I(x_i \neq 0)} {n_i} \|W_i\|_2^2 \\
& \approx \sum_{i=1}^M \sum_{j=1}^B \frac {\alpha_{j,i}} {n_i}
	\|W_i\|_2^2
\end{align*}$$

> - $W \in R^{K * M}, W_i$ï¼šembeddingå­—å…¸ã€ç¬¬$i$embedding
	å‘é‡
> - $K, M$ï¼šembeddingå‘é‡ç»´æ•°ã€ç‰¹å¾æ•°é‡
> - $B, B_j$ï¼šbatchæ•°é‡ã€ç¬¬$j$ä¸ªbatch

-	åˆ™å‚æ•°è¿­ä»£

	$$
	W_i \leftarrow w_j - \eta[\frac 1 {|B_j|} \sum_{(x,y) \in B_j}
		\frac {\partial L(p(x), y)} {\partial W_j} + \lambda
		\frac {\alpha_{j,i}} {n_i} W_i]
	$$

####	Data Adaptive Activation Function

$$\begin{align*}
f(x) & = \left \{ \begin{array}{l}
		x, & x > 0 \\
		\alpha x, & x \leq 0
	\end{array} \right. \\
& = p(x) * x + (1 - p(x)) * x \\
p(x) & = I(x > 0)
\end{align*}$$

PReLUåœ¨0ç‚¹å¤„ç¡¬ä¿®æ­£ï¼Œè€ƒè™‘ä½¿ç”¨å…¶ä»–å¯¹è¾“å…¥è‡ªé€‚åº”çš„å‡½æ•°æ›¿ä»£ï¼Œä»¥
é€‚åº”ä¸åŒå±‚çš„ä¸åŒè¾“å…¥åˆ†å¸ƒ

$$
p(x)  \frac 1 {1 + exp(-\frac {x - E[x]} {\sqrt{Var[x] + \epsilon}})}
$$

##	Deep Interest Evolution Network

*DIEN*ï¼šå¼•å…¥åºåˆ—æ¨¡å‹AUGRUæ¨¡æ‹Ÿè¡Œä¸ºè¿›åŒ–è¿‡ç¨‹

###	æ¨¡å‹ç»“æ„

![dien_structure](imgs/dien_structure.png)

-	*Interest Extractor Layer*ï¼šä½¿ç”¨GRUå•å…ƒå»ºæ¨¡å†å²è¡Œä¸ºä¾èµ–
	å…³ç³»



–
	å…³ç³»



