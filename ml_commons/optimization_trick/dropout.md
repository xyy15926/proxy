---
title: Dropout
categories:
  - ML Technique
  - Neural Network
tags:
  - Machine Learning
  - Technique
  - ML Model
  - Neural Network
  - Normalization
  - Regularization
  - Dropout
date: 2019-07-29 21:16:01
updated: 2021-08-04 19:35:20
toc: true
mathjax: true
comments: true
description: Dropout
---

##	Dropout

*Dropout*：**训练时**根据随机隐藏部分神经元、对应连接边避免
过拟合


###	固定概率丢弃

Dropout最简单方法：设置固定概率p，对每个神经元以概率p判定
是否需要保留

$$\begin{align*}
y & = f(W * d(x) + b) \\
d(x) & = \left \{ \begin{array}{l}
	m \odot x, & 训练阶段
	px, & 测试阶段
\end{array} \right.
\end{align*}$$

> - $d(x)$：丢弃函数
> - $m \in \{0, 1\}^d$：丢弃掩码，通过概率为p的伯努利
	分布随机生成

-	$p$可以设置为0.5，对大部分网络、任务比较有效
	-	此时随机生成多的网络最具多样性

-	训练时
	-	激活神经元数量为原来的p倍
	-	每个batch分别进行drop，相当于对每个batch都有独特网络

-	测试时
	-	所有神经元都被激活，造成训练、测试时网络输出不一致，
		需将每个神经元输出乘p避免
	-	也相当于把不同网络做平均

-	在预测时，类似bagging技术将多个模型组合
	-	只是类似，各个drop后的子网并不独立，在不同子网中相同
		神经元的权重相同
	-	多个模型组合组合可以一定程度上抵消过拟合
	-	因为在训练时子网中部分神经元被drop，剩余部分权重相较
		完全网络有$\frac 1 {1-p}$，所以在完整网络中，各部分
		权重需要$ * (1-p)$

> - 讲道理应该是隐藏部分神经元而不是连接，否则会使神经元偏向
	某些输入，还不如隐藏部分神经元，这样可以让神经元随机降低
	样本权重，理论上能减弱过拟合

###	丢弃方法

-	输入层神经元丢弃率更接近1，使得输入变化不会太大
	-	输入层神经元丢失时，相当于给数据增加噪声，提高网络
		稳健性

-	循环神经网络丢弃
	-	不能直接丢弃隐状态，会损害循环网络在时间维度上的记忆
		能力
	-	简单方法：可以考虑对非循环连接进行随机丢弃
	-	变分丢弃法：根据贝叶斯对丢弃法是对参数的采样解释，
		采样参数需要每个时刻保持不变
		-	需要对参数矩阵的每个元素随机丢弃
		-	所有时刻使用相同的丢弃掩码

###	解释

-	集成学习解释
	-	每次丢弃，相当于从原网络采样得到子网络
	-	每次迭代，相当于训练不同的子网络，共享原始网络参数
	-	最终网络可以近似看作是集成了指数个不同网络的组合模型

-	贝叶斯学习解释
	-	对需要学习的网络$y = f(x, \theta)$，贝叶斯学习假设
		参数$\theta$为随机向量
	-	设先验分布为$q(\theta)$，贝叶斯方法预测为

		$$\begin{align*}
		E_{q(\theta)}[y] &  = \int_q f(x, \theta) q(\theta)
			d\theta \\
		& \approx \frac 1 M \sum_{m=1}^M f(x, \theta_m)
		\end{align*}$$

	> - $f(x, \theta_m)$：第$m$次应用丢弃方法的网络
	> - $\theta_m$：对全部参数的采样


