---
title: Bagging
categories:
  - ML Model
  - Model Enhencement
tags:
  - Machine Learning
  - ML Model
  - Model Enhencement
  - Bagging
date: 2019-07-21 00:46:35
updated: 2019-07-21 00:46:35
toc: true
mathjax: true
comments: true
description: Bagging
---

##	*Bagging*

*bagging*：*bootstrap aggregating*，每个分类器随机从原样本
中做**有放回的随机抽样**，在抽样结果上训练基模型，最后根据
多个基模型的预测结果产生最终结果

-	核心为bootstrap重抽样自举

###	步骤

-	建模阶段：通过boostrap技术获得k个自举样本
	$S_1, S_2,..., S_K$，以其为基础建立k个相同类型模型
	$T_1, T_2,..., T_K$

-	预测阶段：组合K个预测模型
	-	分类问题：K个预测模型“投票”
	-	回归问题：K个预测模型平均值

###	模型性质

-	相较于单个基学习器，Bagging的优势
	-	分类Bagging几乎是最优的贝叶斯分类器
	-	回归Bagging可以通过降低方差（主要）降低均方误差

####	预测误差

总有部分观测未参与建模，预测误差估计偏乐观

-	*OOB*预测误差：*out of bag*，基于袋外观测的预测误差，
	对每个模型，使用没有参与建立模型的样本进行预测，计算预测
	误差

-	OOB观测比率：样本总量n较大时有
	$$
	r = (1 - \frac 1 n)^n \approx \frac 1 e = 0.367
	$$

	-	每次训练样本比率小于10交叉验证的90%

##	*Random Forest*

随机森林：随机建立多个有较高预测精度、弱相关（甚至不相关）
的决策树（基础学习器），多棵决策树共同对新观测做预测

-	RF是Bagging的扩展变体，在以决策树为基学习器构建Bagging
	集成模型的基础上，在训练过程中引入了**随机特征选择**

-	适合场景
	-	数据维度相对较低、同时对准确率有要求
	-	无需很多参数调整即可达到不错的效果

###	步骤

-	样本随机：Bootstrap自举样本

-	输入属性随机：对第i棵决策树通过随机方式选取K个输入变量
	构成候选变量子集$\Theta_I$

	-	Forest-Random Input：随机选择$k=log_2P+1或k=\sqrt P$
		个变量

	-	Forest-Random Combination
		-	随机选择L个输入变量x
		-	生成L个服从均匀分布的随机数$\alpha$
		-	做线性组合
			$v_j = \sum_{i=1}^L \alpha_i x_i, \alpha_i \in [-1, 1]$
		-	得到k个由新变量v组成的输入变量子集$\Theta_i$

-	在候选变量子集中选择最优变量构建决策树
	-	生成决策树时不需要剪枝

-	重复以上步骤构建k棵决策树，用一定集成策略组合多个决策树
	-	简单平均/随机森林投票

###	优点

-	样本抽样、属性抽样引入随机性
	-	基学习器估计误差较大，但是组合模型偏差被修正
	-	不容易发生过拟合、对随机波动稳健性较好
	-	一定程度上避免贪心算法带来的局部最优局限

-	数据兼容性
	-	能够方便处理高维数据，“不用做特征选择”
	-	能处理分类型、连续型数据

-	训练速度快、容易实现并行

-	其他
	-	可以得到变量重要性排序
	-	启发式操作
	-	优化操作

###	缺点

-	决策树数量过多时，训练需要资源多
-	模型解释能力差，有点黑盒模型




